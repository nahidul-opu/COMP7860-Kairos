{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoding=utf-8\n",
    "import os.path as osp\n",
    "import os\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch.nn import Linear\n",
    "from sklearn.metrics import average_precision_score, roc_auc_score\n",
    "from torch_geometric.data import TemporalData\n",
    "\n",
    "from torch_geometric.nn import TGNMemory, TransformerConv\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn.models.tgn import (LastNeighborLoader, IdentityMessage, MeanAggregator,\n",
    "                                           LastAggregator)\n",
    "from torch_geometric import *\n",
    "from torch_geometric.utils import negative_sampling\n",
    "\n",
    "from tqdm import tqdm\n",
    "# from .autonotebook import tqdm as notebook_tqdm\n",
    "\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import math\n",
    "import copy\n",
    "import re\n",
    "import time\n",
    "import json\n",
    "import pandas as pd\n",
    "from random import choice\n",
    "import gc\n",
    "from graphviz import Digraph\n",
    "import xxhash\n",
    "\n",
    "from datetime import datetime, timezone\n",
    "import time\n",
    "import pytz\n",
    "from time import mktime\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "\n",
    "from rich.progress import Progress\n",
    "from rich.progress import (\n",
    "    BarColumn,\n",
    "    DownloadColumn,\n",
    "    Progress,\n",
    "    SpinnerColumn,\n",
    "    TaskProgressColumn,\n",
    "    TimeElapsedColumn,\n",
    "    TimeRemainingColumn,\n",
    ")\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "def hashgen(l):\n",
    "    \"\"\"Generate a single hash value from a list. @l is a list of\n",
    "    string values, which can be properties of a node/edge. This\n",
    "    function returns a single hashed integer value.\"\"\"\n",
    "    hasher = xxhash.xxh64()\n",
    "    for e in l:\n",
    "        hasher.update(e)\n",
    "    return hasher.intdigest()\n",
    "\n",
    "\n",
    "def datetime_to_ns_time(date):\n",
    "    \"\"\"\n",
    "    :param date: str   format: %Y-%m-%d %H:%M:%S   e.g. 2013-10-10 23:40:00\n",
    "    :return: nano timestamp\n",
    "    \"\"\"\n",
    "    date,ns=date.split('.')\n",
    "\n",
    "    timeArray = time.strptime(date, '%Y-%m-%dT%H:%M:%S')\n",
    "    timeStamp = int(time.mktime(timeArray))\n",
    "    timeStamp = timeStamp * 1000000000\n",
    "    timeStamp += int(ns.split('Z')[0])\n",
    "    return timeStamp\n",
    "\n",
    "\n",
    "def datetime_to_timestamp_US(date):\n",
    "    \"\"\"\n",
    "    :param date: str   format: %Y-%m-%d %H:%M:%S   e.g. 2013-10-10 23:40:00\n",
    "    :return: nano timestamp\n",
    "    \"\"\"\n",
    "    date=date.replace('-04:00','')\n",
    "    if '.' in date:\n",
    "        date,ms=date.split('.')\n",
    "    else:\n",
    "        ms=0\n",
    "    tz = pytz.timezone('Etc/GMT+4')\n",
    "    timeArray = time.strptime(date, \"%Y-%m-%dT%H:%M:%S\")\n",
    "    dt = datetime.fromtimestamp(mktime(timeArray))\n",
    "    timestamp = tz.localize(dt)\n",
    "    timestamp=timestamp.timestamp()\n",
    "    timeStamp = timestamp*1000+int(ms)\n",
    "    return int(timeStamp)\n",
    "\n",
    "\n",
    "def timestamp_to_datetime_US(ns):\n",
    "    \"\"\"\n",
    "    :param date: str   format: %Y-%m-%d %H:%M:%S   e.g. 2013-10-10 23:40:00\n",
    "    :return: nano timestamp\n",
    "    \"\"\"\n",
    "    tz = pytz.timezone('US/Eastern')\n",
    "    ms=ns%1000\n",
    "    ns/=1000\n",
    "    dt = pytz.datetime.datetime.fromtimestamp(int(ns), tz)\n",
    "    s = dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "    s+='.'+str(ms)\n",
    "#     s += '.' + str(int(int(ns) % 1000000000)).zfill(9)\n",
    "    return s\n",
    "\n",
    "pid_split_symble=\"#_\"\n",
    "\n",
    "host_split_symble=\"_@\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Database setting (Make sure the database and tables are created)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "\n",
    "from psycopg2 import extras as ex\n",
    "# Create a postgreSQL DB connection object for storing provenance graph edges into DB\n",
    "# Original '/var/run/postgresql/' has been replaced with 'localhost' since we are using docker and accessing as a service in port 5437\n",
    "connect = psycopg2.connect(database = 'optc_db',\n",
    "                           host = 'localhost',\n",
    "                           user = 'postgres',\n",
    "                           password = 'postgres',\n",
    "                           port = '5437'\n",
    "                          )\n",
    "\n",
    "cur = connect.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear all data in the database. Run it carefully!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "# Delete events/edges inserted in previous run\n",
    "tt=cur.execute(\"\"\"\n",
    "    delete from event_table where 1=1;\n",
    "\"\"\")\n",
    "print(tt)\n",
    "connect.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "# Delete messages inserted in previous run\n",
    "tt=cur.execute(\"\"\"\n",
    "    delete from nodeid2msg where 1=1;\n",
    "\"\"\")\n",
    "print(tt)\n",
    "connect.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parse data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reverse_edge_type=[\n",
    "    \"READ\",\n",
    "]\n",
    "\n",
    "\n",
    "# 3 types of nodes used in kairos\n",
    "node_type_used=[\n",
    "    'FILE',\n",
    " 'FLOW',\n",
    " 'PROCESS',\n",
    "#  'SHELL',\n",
    "]\n",
    "# Parsing source node, destination node, edge, timestamp, hostname etc.\n",
    "def process_raw_dic(raw_dic):\n",
    "    ans_dic={}\n",
    "    \n",
    "    \n",
    "    ans_dic['hostname']=raw_dic['hostname'].split('.')[0]\n",
    "    \n",
    "    ans_dic['edge_type']=raw_dic['action']\n",
    "    ans_dic['src_id']=raw_dic['actorID']\n",
    "    ans_dic['dst_id']=raw_dic['objectID']\n",
    "    \n",
    "    ans_dic['src_type']='PROCESS'\n",
    "    ans_dic['timestamp']=datetime_to_timestamp_US(raw_dic['timestamp'])\n",
    "    ans_dic['dst_type']=raw_dic['object']\n",
    "    \n",
    "    try:\n",
    "        node_uuid2path[ans_dic['src_id']]=ans_dic['hostname']+host_split_symble+raw_dic['properties']['image_path']  \n",
    "        \n",
    "    \n",
    "        if raw_dic['object']=='FLOW':\n",
    "            temp_flow=f\"{raw_dic['properties']['direction']}#{raw_dic['properties']['src_ip']}:{raw_dic['properties']['src_port']}->{raw_dic['properties']['dest_ip']}:{raw_dic['properties']['dest_port']}\"\n",
    "            node_uuid2path[ans_dic['dst_id']]=ans_dic['hostname']+host_split_symble+temp_flow\n",
    "\n",
    "        if raw_dic['object']=='FILE':              \n",
    "            node_uuid2path[ans_dic['dst_id']]=ans_dic['hostname']+host_split_symble+raw_dic['properties']['file_path']\n",
    "\n",
    "\n",
    "    except:\n",
    "        ans_dic={}\n",
    "    \n",
    "    return ans_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_type={'FILE',\n",
    " 'FLOW',\n",
    " 'MODULE',\n",
    " 'PROCESS',\n",
    " 'REGISTRY',\n",
    " 'SHELL',\n",
    " 'TASK',\n",
    " 'THREAD',\n",
    " 'USER_SESSION'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unzip data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import walk\n",
    " \n",
    "# folder path\n",
    "dir_path = '/home/shahidul2k9/data/optc/plain/'\n",
    " \n",
    "# list to store files name\n",
    "res = []\n",
    "for (dir_path, dir_names, file_names) in walk(dir_path):\n",
    "    if dir_path[-1]!='/':\n",
    "        dir_path+='/'\n",
    "#     print(f\"{dir_path=}\")\n",
    "#     print(f\"{file_names=}\")\n",
    "    for f in file_names:\n",
    "        temp_file_path=dir_path+f\n",
    "#         print(f\"{temp_file_path=}\")\n",
    "     \n",
    "        res.append(temp_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4481/4481 [04:32<00:00, 16.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " /home/shahidul2k9/data/optc/plain/ecar/benign/17-18Sep19/AIA-51-75/AIA-51-75.ecar-last.json.gz Finished！\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Unzip compressed json files\n",
    "for r in tqdm(res):\n",
    "    if (\"201-225\" in r or \"401-425\" in r or \"651-675\" in r or \"501-525\" in r or \"51-75\" in r) and \".gz\" in r:\n",
    "        os.system(f\"gzip -d {r}\")\n",
    "        print(f\" {r} Finished！\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process the features of nodes and edges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Edge features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10 types of edges\n",
    "edge_set=['OPEN',\n",
    "'READ',\n",
    "'CREATE',\n",
    "'MESSAGE',\n",
    "'MODIFY',\n",
    "'START',\n",
    "'RENAME',\n",
    "'DELETE',\n",
    "'TERMINATE',\n",
    "'WRITE',]\n",
    "\n",
    "# Generate edge type one-hot\n",
    "edgevec=torch.nn.functional.one_hot(torch.arange(0, len(edge_set)), num_classes=len(edge_set))\n",
    "\n",
    "\n",
    "# Allocating One-hot encoding for each edge type\n",
    "edge2vec={}\n",
    "for e in range(len(edge_set)):\n",
    "    edge2vec[edge_set[e]]=edgevec[e]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'OPEN': tensor([1, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " 'READ': tensor([0, 1, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " 'CREATE': tensor([0, 0, 1, 0, 0, 0, 0, 0, 0, 0]),\n",
       " 'MESSAGE': tensor([0, 0, 0, 1, 0, 0, 0, 0, 0, 0]),\n",
       " 'MODIFY': tensor([0, 0, 0, 0, 1, 0, 0, 0, 0, 0]),\n",
       " 'START': tensor([0, 0, 0, 0, 0, 1, 0, 0, 0, 0]),\n",
       " 'RENAME': tensor([0, 0, 0, 0, 0, 0, 1, 0, 0, 0]),\n",
       " 'DELETE': tensor([0, 0, 0, 0, 0, 0, 0, 1, 0, 0]),\n",
       " 'TERMINATE': tensor([0, 0, 0, 0, 0, 0, 0, 0, 1, 0]),\n",
       " 'WRITE': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 1])}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edge2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Edge label to real number mapping for indexing operation\n",
    "rel2id={}\n",
    "index=1\n",
    "for i in edge_set:\n",
    "    rel2id[index]=i\n",
    "    rel2id[i]=index\n",
    "    index+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 'OPEN',\n",
       " 'OPEN': 1,\n",
       " 2: 'READ',\n",
       " 'READ': 2,\n",
       " 3: 'CREATE',\n",
       " 'CREATE': 3,\n",
       " 4: 'MESSAGE',\n",
       " 'MESSAGE': 4,\n",
       " 5: 'MODIFY',\n",
       " 'MODIFY': 5,\n",
       " 6: 'START',\n",
       " 'START': 6,\n",
       " 7: 'RENAME',\n",
       " 'RENAME': 7,\n",
       " 8: 'DELETE',\n",
       " 'DELETE': 8,\n",
       " 9: 'TERMINATE',\n",
       " 'TERMINATE': 9,\n",
       " 10: 'WRITE',\n",
       " 'WRITE': 10}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rel2id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Node features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import FeatureHasher\n",
    "from torch_geometric.transforms import NormalizeFeatures\n",
    "\n",
    "from sklearn import preprocessing\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "encode_len=16\n",
    "\n",
    "FH_string=FeatureHasher(n_features=encode_len,input_type=\"string\")\n",
    "FH_dict=FeatureHasher(n_features=encode_len,input_type=\"dict\")\n",
    "\n",
    "\n",
    "def path2higlist(p):\n",
    "    l=[]\n",
    "    spl=p.strip().split('/')\n",
    "    for i in spl:\n",
    "        if len(l)!=0:\n",
    "            l.append(l[-1]+'/'+i)\n",
    "        else:\n",
    "            l.append(i)\n",
    "#     print(l)\n",
    "    return l\n",
    "\n",
    "def ip2higlist(p):\n",
    "    l=[]\n",
    "    if \"::\" not in p:\n",
    "        spl=p.strip().split('.')\n",
    "        for i in spl:\n",
    "            if len(l)!=0:\n",
    "                l.append(l[-1]+'.'+i)\n",
    "            else:\n",
    "                l.append(i)\n",
    "    #     print(l)\n",
    "        return l\n",
    "    else:\n",
    "        spl=p.strip().split(':')\n",
    "        for i in spl:\n",
    "            if len(l)!=0:\n",
    "                l.append(l[-1]+':'+i)\n",
    "            else:\n",
    "                l.append(i)\n",
    "    #     print(l)\n",
    "        return l\n",
    "def list2str(l):\n",
    "    s=''\n",
    "    for i in l:\n",
    "        s+=i\n",
    "    return s\n",
    "\n",
    "def str2tensor(msg_type,msg):\n",
    "    if msg_type == 'FLOW':\n",
    "        h_msg=list2str(ip2higlist(msg))\n",
    "    else:\n",
    "        h_msg=list2str(path2higlist(msg))\n",
    "    vec=FH_string.transform([msg_type+h_msg]).toarray()\n",
    "    vec=torch.tensor(vec).reshape(encode_len).float()\n",
    "#     print(h_msg)\n",
    "    return vec\n",
    "\n",
    "\n",
    "class TimeEncoder(torch.nn.Module):\n",
    "    def __init__(self, out_channels):\n",
    "        super().__init__()\n",
    "        self.out_channels = out_channels\n",
    "        self.lin = Linear(1, out_channels)\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.lin.reset_parameters()\n",
    "\n",
    "    def forward(self, t):\n",
    "        return self.lin(t.view(-1, 1)).cos()\n",
    "    \n",
    "time_enc=TimeEncoder(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Store the benign data to database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hashmap for storing unique nodes with associated metadata\n",
    "node_uuid2path={}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import walk\n",
    " \n",
    "# folder path\n",
    "dir_path = '/home/shahidul2k9/data/optc/plain/ecar/benign/'\n",
    "\n",
    "res = []\n",
    "for (dir_path, dir_names, file_names) in walk(dir_path):\n",
    "    if dir_path[-1]!='/':\n",
    "        dir_path+='/'\n",
    "#     print(f\"{dir_path=}\")\n",
    "#     print(f\"{file_names=}\")\n",
    "    for f in file_names:\n",
    "        temp_file_path=dir_path+f\n",
    "#         print(f\"{temp_file_path=}\")\n",
    "        if \"201-225\" in temp_file_path or (\"20-23Sep19\" in temp_file_path and (\"401-425\" in temp_file_path or \"651-675\" in temp_file_path or \"501-525\" in temp_file_path or \"51-75\" in temp_file_path)):\n",
    "            res.append(temp_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 36/36 [00:00<00:00, 406994.46it/s]\n"
     ]
    }
   ],
   "source": [
    "# Unzip ecar benign compressed data file\n",
    "for r in tqdm(res):\n",
    "    if  \".gz\" in r:\n",
    "        #os.system(f\"gzip -d {r}\")\n",
    "        print(f\" {r} Finished！\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# White  listed hosts names\n",
    "def is_selected_hosts(line):\n",
    "    hosts=[\n",
    "        'SysClient0201',\n",
    "        'SysClient0402',\n",
    "        'SysClient0660',\n",
    "        'SysClient0501',\n",
    "        'SysClient0051',        \n",
    "        'SysClient0209',\n",
    "    ]\n",
    "    flag=False\n",
    "    for h in hosts:\n",
    "        if h in line:\n",
    "            flag=True\n",
    "            break\n",
    "    return flag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6190907it [00:56, 110516.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(edge_list)=403686\n",
      "/home/shahidul2k9/data/optc/plain/ecar/benign/19Sep19/AIA-201-225/AIA-201-225.ecar-2019-12-07T16-16-05.667.json Finished! \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23189497it [03:30, 110353.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(edge_list)=1744136\n",
      "/home/shahidul2k9/data/optc/plain/ecar/benign/19Sep19/AIA-201-225/AIA-201-225.ecar-last.json Finished! \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "76699983it [10:59, 116299.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(edge_list)=2957067\n",
      "/home/shahidul2k9/data/optc/plain/ecar/benign/20-23Sep19/AIA-501-525/AIA-501-525.ecar-2019-11-15T09-43-35.856.json Finished! \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "76706216it [10:47, 118536.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(edge_list)=2965137\n",
      "/home/shahidul2k9/data/optc/plain/ecar/benign/20-23Sep19/AIA-501-525/AIA-501-525.ecar-2019-11-15T17-22-42.923.json Finished! \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20541672it [03:16, 104274.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(edge_list)=785228\n",
      "/home/shahidul2k9/data/optc/plain/ecar/benign/20-23Sep19/AIA-501-525/AIA-501-525.ecar-last.json Finished! \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "78048775it [11:08, 116744.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(edge_list)=4039788\n",
      "/home/shahidul2k9/data/optc/plain/ecar/benign/20-23Sep19/AIA-501-525/AIA-501-525.ecar-2019-11-15T05-59-37.208.json Finished! \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "76719146it [10:39, 119990.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(edge_list)=2876893\n",
      "/home/shahidul2k9/data/optc/plain/ecar/benign/20-23Sep19/AIA-501-525/AIA-501-525.ecar-2019-11-15T13-29-59.064.json Finished! \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "36440311it [04:33, 133396.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(edge_list)=843401\n",
      "/home/shahidul2k9/data/optc/plain/ecar/benign/20-23Sep19/AIA-501-525/AIA-501-525.ecar-2019-11-15T03-10-00.546.json Finished! \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "36333460it [05:06, 118559.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(edge_list)=1022401\n",
      "/home/shahidul2k9/data/optc/plain/ecar/benign/20-23Sep19/AIA-651-675/AIA-651-675.ecar-2019-11-15T03-09-38.187.json Finished! \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "76458599it [10:36, 120164.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(edge_list)=2610952\n",
      "/home/shahidul2k9/data/optc/plain/ecar/benign/20-23Sep19/AIA-651-675/AIA-651-675.ecar-2019-11-15T13-28-16.876.json Finished! \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "32029474it [04:15, 125258.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(edge_list)=1099540\n",
      "/home/shahidul2k9/data/optc/plain/ecar/benign/20-23Sep19/AIA-651-675/AIA-651-675.ecar-last.json Finished! \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "77786670it [10:47, 120131.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(edge_list)=3133116\n",
      "/home/shahidul2k9/data/optc/plain/ecar/benign/20-23Sep19/AIA-651-675/AIA-651-675.ecar-2019-11-15T05-48-17.579.json Finished! \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "76457220it [10:25, 122299.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(edge_list)=2644933\n",
      "/home/shahidul2k9/data/optc/plain/ecar/benign/20-23Sep19/AIA-651-675/AIA-651-675.ecar-2019-11-15T09-37-46.741.json Finished! \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "76425447it [10:30, 121194.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(edge_list)=2617847\n",
      "/home/shahidul2k9/data/optc/plain/ecar/benign/20-23Sep19/AIA-651-675/AIA-651-675.ecar-2019-11-15T17-26-42.298.json Finished! \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "76383303it [10:59, 115853.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(edge_list)=3024043\n",
      "/home/shahidul2k9/data/optc/plain/ecar/benign/20-23Sep19/AIA-401-425/AIA-401-425.ecar-2019-12-07T20-18-48.097.json Finished! \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "76389825it [10:41, 119053.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(edge_list)=2990117\n",
      "/home/shahidul2k9/data/optc/plain/ecar/benign/20-23Sep19/AIA-401-425/AIA-401-425.ecar-2019-12-07T12-19-23.521.json Finished! \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "76407480it [10:32, 120726.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(edge_list)=3002918\n",
      "/home/shahidul2k9/data/optc/plain/ecar/benign/20-23Sep19/AIA-401-425/AIA-401-425.ecar-2019-12-07T16-09-39.085.json Finished! \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "53285475it [07:04, 125617.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(edge_list)=2101814\n",
      "/home/shahidul2k9/data/optc/plain/ecar/benign/20-23Sep19/AIA-401-425/AIA-401-425.ecar-last.json Finished! \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "77020972it [11:03, 116146.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(edge_list)=3421514\n",
      "/home/shahidul2k9/data/optc/plain/ecar/benign/20-23Sep19/AIA-401-425/AIA-401-425.ecar-2019-12-07T08-33-35.028.json Finished! \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21076091it [02:49, 124576.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(edge_list)=480749\n",
      "/home/shahidul2k9/data/optc/plain/ecar/benign/20-23Sep19/AIA-401-425/AIA-401-425.ecar-2019-12-07T06-28-53.370.json Finished! \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25224989it [03:08, 133931.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(edge_list)=432115\n",
      "/home/shahidul2k9/data/optc/plain/ecar/benign/20-23Sep19/AIA-51-75/AIA-51-75.ecar-2019-12-07T16-15-43.163.json Finished! \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "76427896it [10:44, 118605.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(edge_list)=2892230\n",
      "/home/shahidul2k9/data/optc/plain/ecar/benign/20-23Sep19/AIA-51-75/AIA-51-75.ecar-2019-12-07T21-31-30.259.json Finished! \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "76404717it [10:27, 121694.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(edge_list)=2919561\n",
      "/home/shahidul2k9/data/optc/plain/ecar/benign/20-23Sep19/AIA-51-75/AIA-51-75.ecar-2019-12-08T00-56-58.175.json Finished! \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "42840935it [06:13, 114785.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(edge_list)=1616861\n",
      "/home/shahidul2k9/data/optc/plain/ecar/benign/20-23Sep19/AIA-51-75/AIA-51-75.ecar-last.json Finished! \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "76421197it [10:28, 121583.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(edge_list)=2824549\n",
      "/home/shahidul2k9/data/optc/plain/ecar/benign/20-23Sep19/AIA-51-75/AIA-51-75.ecar-2019-12-08T04-30-36.852.json Finished! \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "76638913it [10:52, 117444.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(edge_list)=3417179\n",
      "/home/shahidul2k9/data/optc/plain/ecar/benign/20-23Sep19/AIA-51-75/AIA-51-75.ecar-2019-12-07T18-18-31.331.json Finished! \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "53321795it [08:47, 101079.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(edge_list)=4793249\n",
      "/home/shahidul2k9/data/optc/plain/ecar/benign/20-23Sep19/AIA-201-225/AIA-201-225.ecar-2019-12-07T19-16-05.788.json Finished! \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "76648515it [11:22, 112239.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(edge_list)=5027745\n",
      "/home/shahidul2k9/data/optc/plain/ecar/benign/20-23Sep19/AIA-201-225/AIA-201-225.ecar-2019-12-07T22-06-33.589.json Finished! \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "74706481it [11:24, 109147.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(edge_list)=5727218\n",
      "/home/shahidul2k9/data/optc/plain/ecar/benign/20-23Sep19/AIA-201-225/AIA-201-225.ecar-last.json Finished! \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "76495083it [11:51, 107515.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(edge_list)=5879664\n",
      "/home/shahidul2k9/data/optc/plain/ecar/benign/20-23Sep19/AIA-201-225/AIA-201-225.ecar-2019-12-08T01-57-30.012.json Finished! \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "76469610it [11:33, 110258.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(edge_list)=5944960\n",
      "/home/shahidul2k9/data/optc/plain/ecar/benign/20-23Sep19/AIA-201-225/AIA-201-225.ecar-2019-12-08T05-46-21.658.json Finished! \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "32326513it [04:42, 114486.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(edge_list)=2449767\n",
      "/home/shahidul2k9/data/optc/plain/ecar/benign/18-19Sep19/AIA-201-225/AIA-201-225.ecar-2019-12-07T10-37-17.942.json Finished! \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "69051137it [10:31, 109378.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(edge_list)=4978166\n",
      "/home/shahidul2k9/data/optc/plain/ecar/benign/18-19Sep19/AIA-201-225/AIA-201-225.ecar-last.json Finished! \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "75743706it [11:38, 108369.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(edge_list)=5575885\n",
      "/home/shahidul2k9/data/optc/plain/ecar/benign/17-18Sep19/AIA-201-225/AIA-201-225.ecar-2019-12-07T01-57-49.366.json Finished! \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "43174191it [06:58, 103262.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(edge_list)=3393220\n",
      "/home/shahidul2k9/data/optc/plain/ecar/benign/17-18Sep19/AIA-201-225/AIA-201-225.ecar-last.json Finished! \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "75798918it [11:53, 106214.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(edge_list)=6295186\n",
      "/home/shahidul2k9/data/optc/plain/ecar/benign/17-18Sep19/AIA-201-225/AIA-201-225.ecar-2019-12-07T06-00-00.251.json Finished! \n"
     ]
    }
   ],
   "source": [
    "# Iterate though all ecar benign logs, parse it and insert into DB\n",
    "for file_path in res:\n",
    "    \n",
    "    edge_list=[]\n",
    "\n",
    "    with open(file_path) as f:\n",
    "        for line in tqdm(f):\n",
    "            line=line.replace('\\\\\\\\','/')\n",
    "            temp_dic=json.loads(line.strip())\n",
    "            hostname=temp_dic['hostname'].split('.')[0]\n",
    "            if temp_dic['object'] in node_type_used and is_selected_hosts(hostname):\n",
    "                edge_list.append(process_raw_dic(temp_dic))\n",
    "    \n",
    "        print(f'{len(edge_list)=}')\n",
    "        data_list=[]\n",
    "        for e in edge_list:\n",
    "            try:\n",
    "                data_list.append([\n",
    "                    e['src_id'],\n",
    "                    e['src_type'],\n",
    "                    e['edge_type'],\n",
    "                    e['dst_id'],\n",
    "                    e['dst_type'],\n",
    "                    e['hostname'],\n",
    "                    e['timestamp'],\n",
    "                    \"benign\",\n",
    "                ])\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "        # write to database\n",
    "        sql = '''insert into event_table\n",
    "                             values %s\n",
    "                '''\n",
    "        ex.execute_values(cur,sql, data_list,page_size=10000)\n",
    "        connect.commit()\n",
    "        \n",
    "        print(f\"{file_path} Finished! \")\n",
    "        # Clear the tmp variables to release the memory.\n",
    "        del edge_list\n",
    "        del data_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Store the evaluation data to database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import walk\n",
    " # Evaluation dataset subfolder listing\n",
    "# folder path\n",
    "dir_path = '/home/shahidul2k9/data/optc/plain/ecar/evaluation/'\n",
    "\n",
    "res = []\n",
    "for (dir_path, dir_names, file_names) in walk(dir_path):\n",
    "    if dir_path[-1]!='/':\n",
    "        dir_path+='/'\n",
    "    for f in file_names:\n",
    "        temp_file_path=dir_path+f\n",
    "#         print(f\"{temp_file_path=}\")\n",
    "        if (\"201-225\" in temp_file_path or \"401-425\" in temp_file_path or \"651-675\" in temp_file_path or \"501-525\" in temp_file_path or \"51-75\" in temp_file_path):\n",
    "            res.append(temp_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 35/35 [00:00<00:00, 304565.64it/s]\n"
     ]
    }
   ],
   "source": [
    "# Decompress evaluation log files\n",
    "for r in tqdm(res):\n",
    "    if  \".gz\" in r:\n",
    "        os.system(f\"gzip -d {r}\")\n",
    "        print(f\" {r} Finished！\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# White listed hostnames for evaluation\n",
    "def is_selected_hosts(line):\n",
    "    hosts=[\n",
    "        'SysClient0201',\n",
    "        'SysClient0402',\n",
    "        'SysClient0660',\n",
    "        'SysClient0501',\n",
    "        'SysClient0051',        \n",
    "        'SysClient0207',\n",
    "    ]\n",
    "    flag=False\n",
    "    for h in hosts:\n",
    "        if h in line:\n",
    "            flag=True\n",
    "            break\n",
    "    return flag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "32570231it [04:12, 129201.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(edge_list)=1194622\n",
      "/home/shahidul2k9/data/optc/plain/ecar/evaluation/23Sep19-red/AIA-501-525/AIA-501-525.ecar-last.json Finished! \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "32614330it [04:44, 114534.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(edge_list)=1131200\n",
      "/home/shahidul2k9/data/optc/plain/ecar/evaluation/23Sep19-red/AIA-651-675/AIA-651-675.ecar-last.json Finished! \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23066470it [02:57, 129809.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(edge_list)=947751\n",
      "/home/shahidul2k9/data/optc/plain/ecar/evaluation/23Sep19-red/AIA-401-425/AIA-401-425.ecar-2019-12-08T01-29-39.403.json Finished! \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9855840it [01:16, 129404.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(edge_list)=391973\n",
      "/home/shahidul2k9/data/optc/plain/ecar/evaluation/23Sep19-red/AIA-401-425/AIA-401-425.ecar-last.json Finished! \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "32602194it [04:35, 118389.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(edge_list)=1179498\n",
      "/home/shahidul2k9/data/optc/plain/ecar/evaluation/23Sep19-red/AIA-51-75/AIA-51-75.ecar-last.json Finished! \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "34146068it [04:50, 117508.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(edge_list)=2151310\n",
      "/home/shahidul2k9/data/optc/plain/ecar/evaluation/23Sep19-red/AIA-201-225/AIA-201-225.ecar-last.json Finished! \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1759566it [00:15, 109979.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(edge_list)=139907\n",
      "/home/shahidul2k9/data/optc/plain/ecar/evaluation/23Sep19-red/AIA-201-225/AIA-201-225.ecar-2019-12-08T11-05-10.046.json Finished! \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30125538it [04:28, 112385.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(edge_list)=1274143\n",
      "/home/shahidul2k9/data/optc/plain/ecar/evaluation/24Sep19/AIA-501-525/AIA-501-525.ecar-2019-11-17T04-01-58.625.json Finished! \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "65758362it [09:04, 120713.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(edge_list)=2554194\n",
      "/home/shahidul2k9/data/optc/plain/ecar/evaluation/24Sep19/AIA-501-525/AIA-501-525.ecar-last.json Finished! \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "18306997it [02:21, 128999.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(edge_list)=568693\n",
      "/home/shahidul2k9/data/optc/plain/ecar/evaluation/24Sep19/AIA-651-675/AIA-651-675.ecar-2019-11-17T03-25-23.290.json Finished! \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "76274003it [10:13, 124375.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(edge_list)=2829980\n",
      "/home/shahidul2k9/data/optc/plain/ecar/evaluation/24Sep19/AIA-651-675/AIA-651-675.ecar-last.json Finished! \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "73190980it [10:05, 120953.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(edge_list)=2825428\n",
      "/home/shahidul2k9/data/optc/plain/ecar/evaluation/24Sep19/AIA-401-425/AIA-401-425.ecar-2019-12-08T07-35-11.579.json Finished! \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23196562it [03:00, 128314.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(edge_list)=878620\n",
      "/home/shahidul2k9/data/optc/plain/ecar/evaluation/24Sep19/AIA-401-425/AIA-401-425.ecar-last.json Finished! \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "76327011it [10:17, 123697.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(edge_list)=2839067\n",
      "/home/shahidul2k9/data/optc/plain/ecar/evaluation/24Sep19/AIA-51-75/AIA-51-75.ecar-2019-12-08T15-24-26.681.json Finished! \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7422309it [01:05, 113218.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(edge_list)=275431\n",
      "/home/shahidul2k9/data/optc/plain/ecar/evaluation/24Sep19/AIA-51-75/AIA-51-75.ecar-2019-12-08T12-56-31.374.json Finished! \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12133760it [01:56, 104511.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(edge_list)=457651\n",
      "/home/shahidul2k9/data/optc/plain/ecar/evaluation/24Sep19/AIA-51-75/AIA-51-75.ecar-last.json Finished! \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "49333875it [07:05, 115981.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(edge_list)=3544607\n",
      "/home/shahidul2k9/data/optc/plain/ecar/evaluation/24Sep19/AIA-201-225/AIA-201-225.ecar-2019-12-08T17-41-18.327.json Finished! \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "46514809it [07:15, 106872.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(edge_list)=3497340\n",
      "/home/shahidul2k9/data/optc/plain/ecar/evaluation/24Sep19/AIA-201-225/AIA-201-225.ecar-last.json Finished! \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "13921814it [01:48, 128881.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(edge_list)=586663\n",
      "/home/shahidul2k9/data/optc/plain/ecar/evaluation/25Sept/AIA-501-525/AIA-501-525.ecar-last.json Finished! \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10781277it [01:29, 120326.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(edge_list)=493144\n",
      "/home/shahidul2k9/data/optc/plain/ecar/evaluation/25Sept/AIA-501-525/AIA-501-525.ecar-2019-11-17T15-04-02.073.json Finished! \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "333939it [00:07, 47156.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(edge_list)=11430\n",
      "/home/shahidul2k9/data/optc/plain/ecar/evaluation/25Sept/AIA-651-675/AIA-651-675.ecar-2019-11-17T14-50-25.754.json Finished! \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24895771it [03:24, 121958.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(edge_list)=1035074\n",
      "/home/shahidul2k9/data/optc/plain/ecar/evaluation/25Sept/AIA-651-675/AIA-651-675.ecar-last.json Finished! \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "27461034it [03:33, 128625.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(edge_list)=1049439\n",
      "/home/shahidul2k9/data/optc/plain/ecar/evaluation/25Sept/AIA-401-425/AIA-401-425.ecar-last.json Finished! \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25186287it [03:41, 113871.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(edge_list)=1053016\n",
      "/home/shahidul2k9/data/optc/plain/ecar/evaluation/25Sept/AIA-51-75/AIA-51-75.ecar-last.json Finished! \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "27031942it [03:51, 116579.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(edge_list)=1839580\n",
      "/home/shahidul2k9/data/optc/plain/ecar/evaluation/25Sept/AIA-201-225/AIA-201-225.ecar-last.json Finished! \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "46406787it [06:14, 123758.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(edge_list)=1768209\n",
      "/home/shahidul2k9/data/optc/plain/ecar/evaluation/23Sep-night/AIA-501-525/AIA-501-525.ecar-last.json Finished! \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23561343it [03:06, 126386.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(edge_list)=897205\n",
      "/home/shahidul2k9/data/optc/plain/ecar/evaluation/23Sep-night/AIA-501-525/AIA-501-525.ecar-2019-11-16T23-22-29.234.json Finished! \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "58296574it [08:11, 118513.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(edge_list)=2195435\n",
      "/home/shahidul2k9/data/optc/plain/ecar/evaluation/23Sep-night/AIA-651-675/AIA-651-675.ecar-last.json Finished! \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11713601it [01:32, 125974.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(edge_list)=441649\n",
      "/home/shahidul2k9/data/optc/plain/ecar/evaluation/23Sep-night/AIA-651-675/AIA-651-675.ecar-2019-11-16T23-07-40.716.json Finished! \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "66585106it [09:06, 121837.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(edge_list)=2522019\n",
      "/home/shahidul2k9/data/optc/plain/ecar/evaluation/23Sep-Night/AIA-401-425/AIA-401-425.ecar-2019-12-08T04-06-31.326.json Finished! \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3341987it [00:25, 128664.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(edge_list)=125517\n",
      "/home/shahidul2k9/data/optc/plain/ecar/evaluation/23Sep-Night/AIA-401-425/AIA-401-425.ecar-last.json Finished! \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "69126143it [09:30, 121237.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(edge_list)=2654927\n",
      "/home/shahidul2k9/data/optc/plain/ecar/evaluation/23Sep-Night/AIA-51-75/AIA-51-75.ecar-last.json Finished! \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "943962it [00:06, 138007.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(edge_list)=14520\n",
      "/home/shahidul2k9/data/optc/plain/ecar/evaluation/23Sep-Night/AIA-51-75/AIA-51-75.ecar-2019-12-08T10-19-52.584.json Finished! \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "27304413it [03:54, 116648.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(edge_list)=2064339\n",
      "/home/shahidul2k9/data/optc/plain/ecar/evaluation/23Sep-Night/AIA-201-225/AIA-201-225.ecar-last.json Finished! \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "42593860it [06:27, 109872.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(edge_list)=3222906\n",
      "/home/shahidul2k9/data/optc/plain/ecar/evaluation/23Sep-Night/AIA-201-225/AIA-201-225.ecar-2019-12-08T14-19-51.427.json Finished! \n"
     ]
    }
   ],
   "source": [
    "# Iterating through uncompressed evaluation dataset files, extracting log and finally inserting into DB\n",
    "for file_path in res:\n",
    "    \n",
    "    edge_list=[]\n",
    "\n",
    "    with open(file_path) as f:\n",
    "        for line in tqdm(f):\n",
    "            line=line.replace('\\\\\\\\','/')\n",
    "            temp_dic=json.loads(line.strip())\n",
    "            hostname=temp_dic['hostname'].split('.')[0]\n",
    "            if temp_dic['object'] in node_type_used and is_selected_hosts(hostname):\n",
    "                edge_list.append(process_raw_dic(temp_dic))\n",
    "    \n",
    "        print(f'{len(edge_list)=}')\n",
    "        data_list=[]\n",
    "        for e in edge_list:\n",
    "            try:\n",
    "                data_list.append([\n",
    "                    e['src_id'],\n",
    "                    e['src_type'],\n",
    "                    e['edge_type'],\n",
    "                    e['dst_id'],\n",
    "                    e['dst_type'],\n",
    "                    e['hostname'],\n",
    "                    e['timestamp'],\n",
    "                    \"evaluation\",\n",
    "                ])\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "        sql = '''insert into event_table\n",
    "                             values %s\n",
    "                '''\n",
    "        ex.execute_values(cur,sql, data_list,page_size=10000)\n",
    "        connect.commit()\n",
    "        \n",
    "        print(f\"{file_path} Finished! \")\n",
    "        # Clear the tmp variables to release the memory.\n",
    "        del edge_list\n",
    "        del data_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Store the node data into database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert temporarily stored nodes and associated metadata into DB\n",
    "data_list=[]\n",
    "for n in node_uuid2path:\n",
    "    try:\n",
    "        data_list.append([\n",
    "            n,\n",
    "             node_uuid2path[n]\n",
    "        ])\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "\n",
    "sql = '''insert into nodeid2msg\n",
    "                     values %s\n",
    "        '''\n",
    "ex.execute_values(cur,sql, data_list,page_size=10000)\n",
    "connect.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18965643"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Log total number of nodes ~19 millions\n",
    "len(node_uuid2path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load node data from database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18965643/18965643 [00:11<00:00, 1622800.85it/s]\n"
     ]
    }
   ],
   "source": [
    "# Construct the map between nodeid and msg\n",
    "sql=\"select * from nodeid2msg;\"\n",
    "cur.execute(sql)\n",
    "rows = cur.fetchall()\n",
    "\n",
    "node_uuid2path={}  # nodeid => msg      node hash => nodeid\n",
    "for i in tqdm(rows):\n",
    "    # Map node UUID to metadata\n",
    "    node_uuid2path[i[0]]=i[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate the benign datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## h402  22"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(events)=4410529\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [25:57<00:00, 1557.68s/it]\n"
     ]
    }
   ],
   "source": [
    "# Read benign dataset generated by host SysClient0402 on 22nd September 2019 and create temporal graph\n",
    "for day in tqdm(range(22,23)):\n",
    "    start_timestamp=datetime_to_timestamp_US('2019-09-'+str(day)+'T00:00:00')\n",
    "    end_timestamp=datetime_to_timestamp_US('2019-09-'+str(day+1)+'T00:00:00')\n",
    "    hostname='SysClient0402'\n",
    "    datalabel='benign'\n",
    "\n",
    "    # Create SQL query command\n",
    "    sql=f\"\"\"\n",
    "    select * from event_table\n",
    "    where\n",
    "          timestamp>{start_timestamp} and timestamp<{end_timestamp}\n",
    "          and hostname='{hostname}' and data_label='{datalabel}' ORDER BY timestamp;\n",
    "    \"\"\"\n",
    "\n",
    "    # Execute SQL query\n",
    "    cur.execute(sql)\n",
    "    # Fetch Edges\n",
    "    events = cur.fetchall()\n",
    "    print(f\"{len(events)=}\")\n",
    "    \n",
    "    \n",
    "    # Generate local temporal graph node indexes mapping\n",
    "    node_set=set()\n",
    "    node_uuid2index={}\n",
    "    temp_index=0\n",
    "    for e in events:\n",
    "        if e[3] not in node_uuid2path or e[0]  not in node_uuid2path:\n",
    "            continue\n",
    "\n",
    "        if e[0] in node_uuid2index:\n",
    "            pass\n",
    "        else:\n",
    "            node_uuid2index[e[0]]=temp_index\n",
    "            node_uuid2index[temp_index]=node_uuid2path[e[0]]\n",
    "            temp_index+=1\n",
    "\n",
    "        if e[3] in node_uuid2index:\n",
    "            pass\n",
    "        else:\n",
    "            node_uuid2index[e[3]]=temp_index\n",
    "            node_uuid2index[temp_index]=node_uuid2path[e[3]]\n",
    "            temp_index+=1 \n",
    "\n",
    "    torch.save(node_uuid2index,f'node_uuid2index_9_{day}_host={hostname}_datalabel={datalabel}')\n",
    "       \n",
    "\n",
    "    dataset = TemporalData()\n",
    "    src = []\n",
    "    dst = []\n",
    "    msg = []\n",
    "    t = []\n",
    "    \n",
    "    # Create temporal graph with nodes, edges, messages and times\n",
    "    for e in (events):\n",
    "        if e[3] in node_uuid2index and e[0] in node_uuid2index:\n",
    "            # If the image path of the node is not recorded, then skip this edge\n",
    "            src.append(node_uuid2index[e[0]])\n",
    "            dst.append(node_uuid2index[e[3]])\n",
    "        #     msg.append(torch.cat([torch.from_numpy(node2higvec_bn[i[0]]), rel2vec[i[2]], torch.from_numpy(node2higvec_bn[i[1]])] ))\n",
    "\n",
    "            msg.append(torch.cat([str2tensor(e[1],node_uuid2path[e[0]]), \n",
    "                                  edge2vec[e[2]], \n",
    "                                  str2tensor(e[4],node_uuid2path[e[3]])\n",
    "                                 ]))\n",
    "            t.append(int(e[6]))\n",
    "\n",
    "    dataset.src = torch.tensor(src)\n",
    "    dataset.dst = torch.tensor(dst)\n",
    "    dataset.t = torch.tensor(t)\n",
    "    dataset.msg = torch.vstack(msg)\n",
    "    dataset.src = dataset.src.to(torch.long)\n",
    "    dataset.dst = dataset.dst.to(torch.long)\n",
    "    dataset.msg = dataset.msg.to(torch.float)\n",
    "    dataset.t = dataset.t.to(torch.long)\n",
    "    # Store temporal graph in disk\n",
    "    torch.save(dataset, f\"/home/shahidul2k9/data/optc/out/evaluation/9_{day}_host={hostname}_datalabel={datalabel}.TemporalData\")  \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## h660 22"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(events)=3889699\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [22:04<00:00, 1324.87s/it]\n"
     ]
    }
   ],
   "source": [
    "# Read benign dataset generated by host SysClient0660 on 22nd September 2019 and create temporal graph\n",
    "for day in tqdm(range(22,23)):\n",
    "    start_timestamp=datetime_to_timestamp_US('2019-09-'+str(day)+'T00:00:00')\n",
    "    end_timestamp=datetime_to_timestamp_US('2019-09-'+str(day+1)+'T00:00:00')\n",
    "    hostname='SysClient0660'\n",
    "    datalabel='benign'\n",
    "    # Create SQL query command\n",
    "    sql=f\"\"\"\n",
    "    select * from event_table\n",
    "    where\n",
    "          timestamp>{start_timestamp} and timestamp<{end_timestamp}\n",
    "          and hostname='{hostname}' and data_label='{datalabel}' ORDER BY timestamp;\n",
    "    \"\"\"\n",
    "    \n",
    "    # Execute SQL query\n",
    "    cur.execute(sql)\n",
    "    events = cur.fetchall()\n",
    "    print(f\"{len(events)=}\")\n",
    "    \n",
    "    \n",
    "    # Generate local temporal graph node indexes mapping\n",
    "    node_set=set()\n",
    "    node_uuid2index={}\n",
    "    temp_index=0\n",
    "    for e in events:\n",
    "        if e[3] not in node_uuid2path or e[0]  not in node_uuid2path:\n",
    "            continue\n",
    "\n",
    "        if e[0] in node_uuid2index:\n",
    "            pass\n",
    "        else:\n",
    "            node_uuid2index[e[0]]=temp_index\n",
    "            node_uuid2index[temp_index]=node_uuid2path[e[0]]\n",
    "            temp_index+=1\n",
    "\n",
    "        if e[3] in node_uuid2index:\n",
    "            pass\n",
    "        else:\n",
    "            node_uuid2index[e[3]]=temp_index\n",
    "            node_uuid2index[temp_index]=node_uuid2path[e[3]]\n",
    "            temp_index+=1 \n",
    "\n",
    "    torch.save(node_uuid2index,f'node_uuid2index_9_{day}_host={hostname}_datalabel={datalabel}')\n",
    "       \n",
    "\n",
    "    dataset = TemporalData()\n",
    "    src = []\n",
    "    dst = []\n",
    "    msg = []\n",
    "    t = []\n",
    "    # Create temporal graph with nodes, edges, messages and times\n",
    "    for e in (events):\n",
    "        if e[3] in node_uuid2index and e[0] in node_uuid2index:\n",
    "            # If the image path of the node is not recorded, then skip this edge\n",
    "            src.append(node_uuid2index[e[0]])\n",
    "            dst.append(node_uuid2index[e[3]])\n",
    "        #     msg.append(torch.cat([torch.from_numpy(node2higvec_bn[i[0]]), rel2vec[i[2]], torch.from_numpy(node2higvec_bn[i[1]])] ))\n",
    "\n",
    "            msg.append(torch.cat([str2tensor(e[1],node_uuid2path[e[0]]), \n",
    "                                  edge2vec[e[2]], \n",
    "                                  str2tensor(e[4],node_uuid2path[e[3]])\n",
    "                                 ]))\n",
    "            t.append(int(e[6]))\n",
    "\n",
    "    dataset.src = torch.tensor(src)\n",
    "    dataset.dst = torch.tensor(dst)\n",
    "    dataset.t = torch.tensor(t)\n",
    "    dataset.msg = torch.vstack(msg)\n",
    "    dataset.src = dataset.src.to(torch.long)\n",
    "    dataset.dst = dataset.dst.to(torch.long)\n",
    "    dataset.msg = dataset.msg.to(torch.float)\n",
    "    dataset.t = dataset.t.to(torch.long)\n",
    "    # Store temporal graph in disk\n",
    "    torch.save(dataset, f\"/home/shahidul2k9/data/optc/out/evaluation/9_{day}_host={hostname}_datalabel={datalabel}.TemporalData\")  \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## h501 21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(events)=4337416\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [23:52<00:00, 1432.56s/it]\n"
     ]
    }
   ],
   "source": [
    "# Read benign dataset generated by host SysClient0501 on 21st September 2019 and create temporal graph\n",
    "for day in tqdm(range(21,22)):\n",
    "    start_timestamp=datetime_to_timestamp_US('2019-09-'+str(day)+'T00:00:00')\n",
    "    end_timestamp=datetime_to_timestamp_US('2019-09-'+str(day+1)+'T00:00:00')\n",
    "    hostname='SysClient0501'\n",
    "    datalabel='benign'\n",
    "    # Create SQL query command\n",
    "    sql=f\"\"\"\n",
    "    select * from event_table\n",
    "    where\n",
    "          timestamp>{start_timestamp} and timestamp<{end_timestamp}\n",
    "          and hostname='{hostname}' and data_label='{datalabel}' ORDER BY timestamp;\n",
    "    \"\"\"\n",
    "    # Execute SQL query\n",
    "    cur.execute(sql)\n",
    "    events = cur.fetchall()\n",
    "    print(f\"{len(events)=}\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    node_set=set()\n",
    "    node_uuid2index={}\n",
    "    temp_index=0\n",
    "    for e in events:\n",
    "        if e[3] not in node_uuid2path or e[0]  not in node_uuid2path:\n",
    "            continue\n",
    "\n",
    "        if e[0] in node_uuid2index:\n",
    "            pass\n",
    "        else:\n",
    "            node_uuid2index[e[0]]=temp_index\n",
    "            node_uuid2index[temp_index]=node_uuid2path[e[0]]\n",
    "            temp_index+=1\n",
    "\n",
    "        if e[3] in node_uuid2index:\n",
    "            pass\n",
    "        else:\n",
    "            node_uuid2index[e[3]]=temp_index\n",
    "            node_uuid2index[temp_index]=node_uuid2path[e[3]]\n",
    "            temp_index+=1 \n",
    "\n",
    "    torch.save(node_uuid2index,f'node_uuid2index_9_{day}_host={hostname}_datalabel={datalabel}')\n",
    "       \n",
    "\n",
    "    dataset = TemporalData()\n",
    "    src = []\n",
    "    dst = []\n",
    "    msg = []\n",
    "    t = []\n",
    "    # Create temporal graph with nodes, edges, messages and times\n",
    "    for e in (events):\n",
    "        if e[3] in node_uuid2index and e[0] in node_uuid2index:\n",
    "            # If the image path of the node is not recorded, then skip this edge\n",
    "            src.append(node_uuid2index[e[0]])\n",
    "            dst.append(node_uuid2index[e[3]])\n",
    "        #     msg.append(torch.cat([torch.from_numpy(node2higvec_bn[i[0]]), rel2vec[i[2]], torch.from_numpy(node2higvec_bn[i[1]])] ))\n",
    "\n",
    "            msg.append(torch.cat([str2tensor(e[1],node_uuid2path[e[0]]), \n",
    "                                  edge2vec[e[2]], \n",
    "                                  str2tensor(e[4],node_uuid2path[e[3]])\n",
    "                                 ]))\n",
    "            t.append(int(e[6]))\n",
    "\n",
    "    dataset.src = torch.tensor(src)\n",
    "    dataset.dst = torch.tensor(dst)\n",
    "    dataset.t = torch.tensor(t)\n",
    "    dataset.msg = torch.vstack(msg)\n",
    "    dataset.src = dataset.src.to(torch.long)\n",
    "    dataset.dst = dataset.dst.to(torch.long)\n",
    "    dataset.msg = dataset.msg.to(torch.float)\n",
    "    dataset.t = dataset.t.to(torch.long)\n",
    "    # Store temporal graph in disk\n",
    "    torch.save(dataset, f\"/home/shahidul2k9/data/optc/out/evaluation/9_{day}_host={hostname}_datalabel={datalabel}.TemporalData\")  \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## h501 22"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(events)=4263136\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [24:09<00:00, 1449.94s/it]\n"
     ]
    }
   ],
   "source": [
    "# Read benign dataset generated by host SysClient0501 on 22nd September 2019 and create temporal graph\n",
    "for day in tqdm(range(22,23)):\n",
    "    start_timestamp=datetime_to_timestamp_US('2019-09-'+str(day)+'T00:00:00')\n",
    "    end_timestamp=datetime_to_timestamp_US('2019-09-'+str(day+1)+'T00:00:00')\n",
    "    hostname='SysClient0501'\n",
    "    datalabel='benign'\n",
    "    # Create SQL query command\n",
    "    sql=f\"\"\"\n",
    "    select * from event_table\n",
    "    where\n",
    "          timestamp>{start_timestamp} and timestamp<{end_timestamp}\n",
    "          and hostname='{hostname}' and data_label='{datalabel}' ORDER BY timestamp;\n",
    "    \"\"\"\n",
    "    # Execute SQL query\n",
    "    cur.execute(sql)\n",
    "    events = cur.fetchall()\n",
    "    print(f\"{len(events)=}\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    node_set=set()\n",
    "    node_uuid2index={}\n",
    "    temp_index=0\n",
    "    # Generate local temporal graph node indexes mapping\n",
    "    for e in events:\n",
    "        if e[3] not in node_uuid2path or e[0]  not in node_uuid2path:\n",
    "            continue\n",
    "\n",
    "        if e[0] in node_uuid2index:\n",
    "            pass\n",
    "        else:\n",
    "            node_uuid2index[e[0]]=temp_index\n",
    "            node_uuid2index[temp_index]=node_uuid2path[e[0]]\n",
    "            temp_index+=1\n",
    "\n",
    "        if e[3] in node_uuid2index:\n",
    "            pass\n",
    "        else:\n",
    "            node_uuid2index[e[3]]=temp_index\n",
    "            node_uuid2index[temp_index]=node_uuid2path[e[3]]\n",
    "            temp_index+=1 \n",
    "\n",
    "    torch.save(node_uuid2index,f'node_uuid2index_9_{day}_host={hostname}_datalabel={datalabel}')\n",
    "\n",
    "    dataset = TemporalData()\n",
    "    src = []\n",
    "    dst = []\n",
    "    msg = []\n",
    "    t = []\n",
    "    # Create temporal graph with nodes, edges, messages and times\n",
    "    for e in (events):\n",
    "        if e[3] in node_uuid2index and e[0] in node_uuid2index:\n",
    "            # If the image path of the node is not recorded, then skip this edge\n",
    "            src.append(node_uuid2index[e[0]])\n",
    "            dst.append(node_uuid2index[e[3]])\n",
    "        #     msg.append(torch.cat([torch.from_numpy(node2higvec_bn[i[0]]), rel2vec[i[2]], torch.from_numpy(node2higvec_bn[i[1]])] ))\n",
    "\n",
    "            msg.append(torch.cat([str2tensor(e[1],node_uuid2path[e[0]]), \n",
    "                                  edge2vec[e[2]], \n",
    "                                  str2tensor(e[4],node_uuid2path[e[3]])\n",
    "                                 ]))\n",
    "            t.append(int(e[6]))\n",
    "\n",
    "    dataset.src = torch.tensor(src)\n",
    "    dataset.dst = torch.tensor(dst)\n",
    "    dataset.t = torch.tensor(t)\n",
    "    dataset.msg = torch.vstack(msg)\n",
    "    dataset.src = dataset.src.to(torch.long)\n",
    "    dataset.dst = dataset.dst.to(torch.long)\n",
    "    dataset.msg = dataset.msg.to(torch.float)\n",
    "    dataset.t = dataset.t.to(torch.long)\n",
    "    # Store temporal graph in disk\n",
    "    torch.save(dataset, f\"/home/shahidul2k9/data/optc/out/evaluation/9_{day}_host={hostname}_datalabel={datalabel}.TemporalData\")  \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## h051 22"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(events)=4074941\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [22:32<00:00, 1352.34s/it]\n"
     ]
    }
   ],
   "source": [
    "# Read benign dataset generated by host SysClient0051 on 22nd September 2019 and create temporal graph\n",
    "for day in tqdm(range(22,23)):\n",
    "    start_timestamp=datetime_to_timestamp_US('2019-09-'+str(day)+'T00:00:00')\n",
    "    end_timestamp=datetime_to_timestamp_US('2019-09-'+str(day+1)+'T00:00:00')\n",
    "    hostname='SysClient0051'\n",
    "    datalabel='benign'\n",
    "    # Create SQL query command\n",
    "    sql=f\"\"\"\n",
    "    select * from event_table\n",
    "    where\n",
    "          timestamp>{start_timestamp} and timestamp<{end_timestamp}\n",
    "          and hostname='{hostname}' and data_label='{datalabel}' ORDER BY timestamp;\n",
    "    \"\"\"\n",
    "    # Execute SQL query\n",
    "    cur.execute(sql)\n",
    "    events = cur.fetchall()\n",
    "    print(f\"{len(events)=}\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    node_set=set()\n",
    "    node_uuid2index={}\n",
    "    temp_index=0\n",
    "    # Generate local temporal graph node indexes mapping\n",
    "    for e in events:\n",
    "        if e[3] not in node_uuid2path or e[0]  not in node_uuid2path:\n",
    "            continue\n",
    "\n",
    "        if e[0] in node_uuid2index:\n",
    "            pass\n",
    "        else:\n",
    "            node_uuid2index[e[0]]=temp_index\n",
    "            node_uuid2index[temp_index]=node_uuid2path[e[0]]\n",
    "            temp_index+=1\n",
    "\n",
    "        if e[3] in node_uuid2index:\n",
    "            pass\n",
    "        else:\n",
    "            node_uuid2index[e[3]]=temp_index\n",
    "            node_uuid2index[temp_index]=node_uuid2path[e[3]]\n",
    "            temp_index+=1 \n",
    "\n",
    "    torch.save(node_uuid2index,f'node_uuid2index_9_{day}_host={hostname}_datalabel={datalabel}')\n",
    "       \n",
    "\n",
    "    dataset = TemporalData()\n",
    "    src = []\n",
    "    dst = []\n",
    "    msg = []\n",
    "    t = []\n",
    "    # Create temporal graph with nodes, edges, messages and times\n",
    "    for e in (events):\n",
    "        if e[3] in node_uuid2index and e[0] in node_uuid2index:\n",
    "            # If the image path of the node is not recorded, then skip this edge\n",
    "            src.append(node_uuid2index[e[0]])\n",
    "            dst.append(node_uuid2index[e[3]])\n",
    "        #     msg.append(torch.cat([torch.from_numpy(node2higvec_bn[i[0]]), rel2vec[i[2]], torch.from_numpy(node2higvec_bn[i[1]])] ))\n",
    "\n",
    "            msg.append(torch.cat([str2tensor(e[1],node_uuid2path[e[0]]), \n",
    "                                  edge2vec[e[2]], \n",
    "                                  str2tensor(e[4],node_uuid2path[e[3]])\n",
    "                                 ]))\n",
    "            t.append(int(e[6]))\n",
    "\n",
    "    dataset.src = torch.tensor(src)\n",
    "    dataset.dst = torch.tensor(dst)\n",
    "    dataset.t = torch.tensor(t)\n",
    "    dataset.msg = torch.vstack(msg)\n",
    "    dataset.src = dataset.src.to(torch.long)\n",
    "    dataset.dst = dataset.dst.to(torch.long)\n",
    "    dataset.msg = dataset.msg.to(torch.float)\n",
    "    dataset.t = dataset.t.to(torch.long)\n",
    "    # Store temporal graph in disk\n",
    "    torch.save(dataset, f\"/home/shahidul2k9/data/optc/out/evaluation/9_{day}_host={hostname}_datalabel={datalabel}.TemporalData\")  \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## h209 22"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(events)=3853947\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [21:45<00:00, 1305.96s/it]\n"
     ]
    }
   ],
   "source": [
    "# Read benign dataset generated by host SysClient0209 on 22nd September 2019 and create temporal graph\n",
    "for day in tqdm(range(22,23)):\n",
    "    start_timestamp=datetime_to_timestamp_US('2019-09-'+str(day)+'T00:00:00')\n",
    "    end_timestamp=datetime_to_timestamp_US('2019-09-'+str(day+1)+'T00:00:00')\n",
    "    hostname='SysClient0209'\n",
    "    datalabel='benign'\n",
    "    # Create SQL query command\n",
    "    sql=f\"\"\"\n",
    "    select * from event_table\n",
    "    where\n",
    "          timestamp>{start_timestamp} and timestamp<{end_timestamp}\n",
    "          and hostname='{hostname}' and data_label='{datalabel}' ORDER BY timestamp;\n",
    "    \"\"\"\n",
    "    cur.execute(sql)\n",
    "    events = cur.fetchall()\n",
    "    print(f\"{len(events)=}\")\n",
    "    \n",
    "    # Generate local temporal graph node indexes mapping\n",
    "    node_set=set()\n",
    "    node_uuid2index={}\n",
    "    temp_index=0\n",
    "    for e in events:\n",
    "        if e[3] not in node_uuid2path or e[0]  not in node_uuid2path:\n",
    "            continue\n",
    "\n",
    "        if e[0] in node_uuid2index:\n",
    "            pass\n",
    "        else:\n",
    "            node_uuid2index[e[0]]=temp_index\n",
    "            node_uuid2index[temp_index]=node_uuid2path[e[0]]\n",
    "            temp_index+=1\n",
    "\n",
    "        if e[3] in node_uuid2index:\n",
    "            pass\n",
    "        else:\n",
    "            node_uuid2index[e[3]]=temp_index\n",
    "            node_uuid2index[temp_index]=node_uuid2path[e[3]]\n",
    "            temp_index+=1 \n",
    "\n",
    "    torch.save(node_uuid2index,f'node_uuid2index_9_{day}_host={hostname}_datalabel={datalabel}')\n",
    "       \n",
    "\n",
    "    dataset = TemporalData()\n",
    "    src = []\n",
    "    dst = []\n",
    "    msg = []\n",
    "    t = []\n",
    "    # Create temporal graph with nodes, edges, messages and times\n",
    "    for e in (events):\n",
    "        if e[3] in node_uuid2index and e[0] in node_uuid2index:\n",
    "            # If the image path of the node is not recorded, then skip this edge\n",
    "            src.append(node_uuid2index[e[0]])\n",
    "            dst.append(node_uuid2index[e[3]])\n",
    "        #     msg.append(torch.cat([torch.from_numpy(node2higvec_bn[i[0]]), rel2vec[i[2]], torch.from_numpy(node2higvec_bn[i[1]])] ))\n",
    "\n",
    "            msg.append(torch.cat([str2tensor(e[1],node_uuid2path[e[0]]), \n",
    "                                  edge2vec[e[2]], \n",
    "                                  str2tensor(e[4],node_uuid2path[e[3]])\n",
    "                                 ]))\n",
    "            t.append(int(e[6]))\n",
    "\n",
    "    dataset.src = torch.tensor(src)\n",
    "    dataset.dst = torch.tensor(dst)\n",
    "    dataset.t = torch.tensor(t)\n",
    "    dataset.msg = torch.vstack(msg)\n",
    "    dataset.src = dataset.src.to(torch.long)\n",
    "    dataset.dst = dataset.dst.to(torch.long)\n",
    "    dataset.msg = dataset.msg.to(torch.float)\n",
    "    dataset.t = dataset.t.to(torch.long)\n",
    "    # Store temporal graph in disk\n",
    "    torch.save(dataset, f\"/home/shahidul2k9/data/optc/out/evaluation/9_{day}_host={hostname}_datalabel={datalabel}.TemporalData\")  \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate the validation set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## h209 23"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(events)=1462775\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [09:01<00:00, 541.35s/it]\n"
     ]
    }
   ],
   "source": [
    "# Read benign dataset generated by host SysClient0209 on 23rd September 2019 and create temporal graph\n",
    "for day in tqdm(range(23,24)):\n",
    "    start_timestamp=datetime_to_timestamp_US('2019-09-'+str(day)+'T00:00:00')\n",
    "    end_timestamp=datetime_to_timestamp_US('2019-09-'+str(day+1)+'T00:00:00')\n",
    "    hostname='SysClient0209'\n",
    "    datalabel='benign'\n",
    "    # Create SQL query command\n",
    "    sql=f\"\"\"\n",
    "    select * from event_table\n",
    "    where\n",
    "          timestamp>{start_timestamp} and timestamp<{end_timestamp}\n",
    "          and hostname='{hostname}' and data_label='{datalabel}' ORDER BY timestamp;\n",
    "    \"\"\"\n",
    "    # Execute SQL query\n",
    "    cur.execute(sql)\n",
    "    events = cur.fetchall()\n",
    "    print(f\"{len(events)=}\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    node_set=set()\n",
    "    node_uuid2index={}\n",
    "    temp_index=0\n",
    "    # Generate local temporal graph node indexes mapping\n",
    "    for e in events:\n",
    "        if e[3] not in node_uuid2path or e[0]  not in node_uuid2path:\n",
    "            continue\n",
    "\n",
    "        if e[0] in node_uuid2index:\n",
    "            pass\n",
    "        else:\n",
    "            node_uuid2index[e[0]]=temp_index\n",
    "            node_uuid2index[temp_index]=node_uuid2path[e[0]]\n",
    "            temp_index+=1\n",
    "\n",
    "        if e[3] in node_uuid2index:\n",
    "            pass\n",
    "        else:\n",
    "            node_uuid2index[e[3]]=temp_index\n",
    "            node_uuid2index[temp_index]=node_uuid2path[e[3]]\n",
    "            temp_index+=1 \n",
    "\n",
    "    torch.save(node_uuid2index,f'node_uuid2index_9_{day}_host={hostname}_datalabel={datalabel}')\n",
    "       \n",
    "\n",
    "    dataset = TemporalData()\n",
    "    src = []\n",
    "    dst = []\n",
    "    msg = []\n",
    "    t = []\n",
    "    # Create temporal graph with nodes, edges, messages and times\n",
    "    for e in (events):\n",
    "        if e[3] in node_uuid2index and e[0] in node_uuid2index:\n",
    "            # If the image path of the node is not recorded, then skip this edge\n",
    "            src.append(node_uuid2index[e[0]])\n",
    "            dst.append(node_uuid2index[e[3]])\n",
    "        #     msg.append(torch.cat([torch.from_numpy(node2higvec_bn[i[0]]), rel2vec[i[2]], torch.from_numpy(node2higvec_bn[i[1]])] ))\n",
    "\n",
    "            msg.append(torch.cat([str2tensor(e[1],node_uuid2path[e[0]]), \n",
    "                                  edge2vec[e[2]], \n",
    "                                  str2tensor(e[4],node_uuid2path[e[3]])\n",
    "                                 ]))\n",
    "            t.append(int(e[6]))\n",
    "\n",
    "    dataset.src = torch.tensor(src)\n",
    "    dataset.dst = torch.tensor(dst)\n",
    "    dataset.t = torch.tensor(t)\n",
    "    dataset.msg = torch.vstack(msg)\n",
    "    dataset.src = dataset.src.to(torch.long)\n",
    "    dataset.dst = dataset.dst.to(torch.long)\n",
    "    dataset.msg = dataset.msg.to(torch.float)\n",
    "    dataset.t = dataset.t.to(torch.long)\n",
    "    # Store temporal graph in disk\n",
    "    torch.save(dataset, f\"/home/shahidul2k9/data/optc/out/evaluation/9_{day}_host={hostname}_datalabel={datalabel}.TemporalData\")  \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate the evaluation set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## h201 23-25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Day 22 len(events)=0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/4 [01:02<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "vstack expects a non-empty TensorList",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[74], line 67\u001b[0m\n\u001b[1;32m     65\u001b[0m dataset\u001b[38;5;241m.\u001b[39mdst \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(dst)\n\u001b[1;32m     66\u001b[0m dataset\u001b[38;5;241m.\u001b[39mt \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(t)\n\u001b[0;32m---> 67\u001b[0m dataset\u001b[38;5;241m.\u001b[39mmsg \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     68\u001b[0m dataset\u001b[38;5;241m.\u001b[39msrc \u001b[38;5;241m=\u001b[39m dataset\u001b[38;5;241m.\u001b[39msrc\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39mlong)\n\u001b[1;32m     69\u001b[0m dataset\u001b[38;5;241m.\u001b[39mdst \u001b[38;5;241m=\u001b[39m dataset\u001b[38;5;241m.\u001b[39mdst\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39mlong)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: vstack expects a non-empty TensorList"
     ]
    }
   ],
   "source": [
    "# Read evaluation dataset generated by host SysClient0201 on 23rd-25th September 2019 and create temporal graph\n",
    "for day in tqdm(range(23,26)):\n",
    "    start_timestamp=datetime_to_timestamp_US('2019-09-'+str(day)+'T00:00:00')\n",
    "    end_timestamp=datetime_to_timestamp_US('2019-09-'+str(day+1)+'T00:00:00')\n",
    "    hostname='SysClient0201'\n",
    "    datalabel='evaluation'\n",
    "    # Create SQL query command\n",
    "    sql=f\"\"\"\n",
    "    select * from event_table\n",
    "    where\n",
    "          timestamp>{start_timestamp} and timestamp<{end_timestamp}\n",
    "          and hostname='{hostname}' and data_label='{datalabel}' ORDER BY timestamp;\n",
    "    \"\"\"\n",
    "    # Execute SQL query\n",
    "    cur.execute(sql)\n",
    "    events = cur.fetchall()\n",
    "    print(f\"{len(events)=}\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    node_set=set()\n",
    "    node_uuid2index={}\n",
    "    temp_index=0\n",
    "    # Generate local temporal graph node indexes mapping\n",
    "    for e in events:\n",
    "        if e[3] not in node_uuid2path or e[0]  not in node_uuid2path:\n",
    "            continue\n",
    "\n",
    "        if e[0] in node_uuid2index:\n",
    "            pass\n",
    "        else:\n",
    "            node_uuid2index[e[0]]=temp_index\n",
    "            node_uuid2index[temp_index]=node_uuid2path[e[0]]\n",
    "            temp_index+=1\n",
    "\n",
    "        if e[3] in node_uuid2index:\n",
    "            pass\n",
    "        else:\n",
    "            node_uuid2index[e[3]]=temp_index\n",
    "            node_uuid2index[temp_index]=node_uuid2path[e[3]]\n",
    "            temp_index+=1 \n",
    "\n",
    "    torch.save(node_uuid2index,f'node_uuid2index_9_{day}_host={hostname}_datalabel={datalabel}')\n",
    "\n",
    "    dataset = TemporalData()\n",
    "    src = []\n",
    "    dst = []\n",
    "    msg = []\n",
    "    t = []\n",
    "    # Create temporal graph with nodes, edges, messages and times\n",
    "    for e in (events):\n",
    "        if e[3] in node_uuid2index and e[0] in node_uuid2index:\n",
    "            # If the image path of the node is not recorded, then skip this edge\n",
    "            src.append(node_uuid2index[e[0]])\n",
    "            dst.append(node_uuid2index[e[3]])\n",
    "        #     msg.append(torch.cat([torch.from_numpy(node2higvec_bn[i[0]]), rel2vec[i[2]], torch.from_numpy(node2higvec_bn[i[1]])] ))\n",
    "\n",
    "            msg.append(torch.cat([str2tensor(e[1],node_uuid2path[e[0]]), \n",
    "                                  edge2vec[e[2]], \n",
    "                                  str2tensor(e[4],node_uuid2path[e[3]])\n",
    "                                 ]))\n",
    "            t.append(int(e[6]))\n",
    "\n",
    "    dataset.src = torch.tensor(src)\n",
    "    dataset.dst = torch.tensor(dst)\n",
    "    dataset.t = torch.tensor(t)\n",
    "    dataset.msg = torch.vstack(msg)\n",
    "    dataset.src = dataset.src.to(torch.long)\n",
    "    dataset.dst = dataset.dst.to(torch.long)\n",
    "    dataset.msg = dataset.msg.to(torch.float)\n",
    "    dataset.t = dataset.t.to(torch.long)\n",
    "    # Store temporal graph in disk\n",
    "    torch.save(dataset, f\"/home/shahidul2k9/data/optc/out/evaluation/9_{day}_host={hostname}_datalabel={datalabel}.TemporalData\")  \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## h402 23-25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(events)=2513800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 1/3 [14:04<28:08, 844.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(events)=3844461\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 2/3 [35:05<18:09, 1089.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(events)=2317807\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [48:59<00:00, 979.86s/it] \n"
     ]
    }
   ],
   "source": [
    "# Read evaluation dataset generated by host SysClient0402 on 23rd-25th September 2019 and create temporal graph\n",
    "for day in tqdm(range(23,26)):\n",
    "    start_timestamp=datetime_to_timestamp_US('2019-09-'+str(day)+'T00:00:00')\n",
    "    end_timestamp=datetime_to_timestamp_US('2019-09-'+str(day+1)+'T00:00:00')\n",
    "    hostname='SysClient0402'\n",
    "    datalabel='evaluation'\n",
    "    # Create SQL query command\n",
    "    sql=f\"\"\"\n",
    "    select * from event_table\n",
    "    where\n",
    "          timestamp>{start_timestamp} and timestamp<{end_timestamp}\n",
    "          and hostname='{hostname}' and data_label='{datalabel}' ORDER BY timestamp;\n",
    "    \"\"\"\n",
    "    # Execute SQL query\n",
    "    cur.execute(sql)\n",
    "    events = cur.fetchall()\n",
    "    print(f\"{len(events)=}\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    node_set=set()\n",
    "    node_uuid2index={}\n",
    "    temp_index=0\n",
    "    # Generate local temporal graph node indexes mapping\n",
    "    for e in events:\n",
    "        if e[3] not in node_uuid2path or e[0]  not in node_uuid2path:\n",
    "            continue\n",
    "\n",
    "        if e[0] in node_uuid2index:\n",
    "            pass\n",
    "        else:\n",
    "            node_uuid2index[e[0]]=temp_index\n",
    "            node_uuid2index[temp_index]=node_uuid2path[e[0]]\n",
    "            temp_index+=1\n",
    "\n",
    "        if e[3] in node_uuid2index:\n",
    "            pass\n",
    "        else:\n",
    "            node_uuid2index[e[3]]=temp_index\n",
    "            node_uuid2index[temp_index]=node_uuid2path[e[3]]\n",
    "            temp_index+=1 \n",
    "\n",
    "    torch.save(node_uuid2index,f'node_uuid2index_9_{day}_host={hostname}_datalabel={datalabel}')\n",
    "       \n",
    "\n",
    "    dataset = TemporalData()\n",
    "    src = []\n",
    "    dst = []\n",
    "    msg = []\n",
    "    t = []\n",
    "    # Create temporal graph with nodes, edges, messages and times\n",
    "    for e in (events):\n",
    "        if e[3] in node_uuid2index and e[0] in node_uuid2index:\n",
    "            # If the image path of the node is not recorded, then skip this edge\n",
    "            src.append(node_uuid2index[e[0]])\n",
    "            dst.append(node_uuid2index[e[3]])\n",
    "        #     msg.append(torch.cat([torch.from_numpy(node2higvec_bn[i[0]]), rel2vec[i[2]], torch.from_numpy(node2higvec_bn[i[1]])] ))\n",
    "\n",
    "            msg.append(torch.cat([str2tensor(e[1],node_uuid2path[e[0]]), \n",
    "                                  edge2vec[e[2]], \n",
    "                                  str2tensor(e[4],node_uuid2path[e[3]])\n",
    "                                 ]))\n",
    "            t.append(int(e[6]))\n",
    "\n",
    "    dataset.src = torch.tensor(src)\n",
    "    dataset.dst = torch.tensor(dst)\n",
    "    dataset.t = torch.tensor(t)\n",
    "    dataset.msg = torch.vstack(msg)\n",
    "    dataset.src = dataset.src.to(torch.long)\n",
    "    dataset.dst = dataset.dst.to(torch.long)\n",
    "    dataset.msg = dataset.msg.to(torch.float)\n",
    "    dataset.t = dataset.t.to(torch.long)\n",
    "    # Store temporal graph in disk\n",
    "    torch.save(dataset, f\"/home/shahidul2k9/data/optc/out/evaluation/9_{day}_host={hostname}_datalabel={datalabel}.TemporalData\")  \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## h660 23-25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(events)=2317440\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 1/3 [13:32<27:04, 812.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(events)=3558940\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 2/3 [34:00<17:37, 1057.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(events)=2314759\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [47:16<00:00, 945.38s/it] \n"
     ]
    }
   ],
   "source": [
    "# Read evaluation dataset generated by host SysClient0660 on 23rd-25th September 2019 and create temporal graph\n",
    "for day in tqdm(range(23,26)):\n",
    "    start_timestamp=datetime_to_timestamp_US('2019-09-'+str(day)+'T00:00:00')\n",
    "    end_timestamp=datetime_to_timestamp_US('2019-09-'+str(day+1)+'T00:00:00')\n",
    "    hostname='SysClient0660'\n",
    "    datalabel='evaluation'\n",
    "    # Create SQL query command\n",
    "    sql=f\"\"\"\n",
    "    select * from event_table\n",
    "    where\n",
    "          timestamp>{start_timestamp} and timestamp<{end_timestamp}\n",
    "          and hostname='{hostname}' and data_label='{datalabel}' ORDER BY timestamp;\n",
    "    \"\"\"\n",
    "    # Execute SQL query\n",
    "    cur.execute(sql)\n",
    "    events = cur.fetchall()\n",
    "    print(f\"{len(events)=}\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    node_set=set()\n",
    "    node_uuid2index={}\n",
    "    temp_index=0\n",
    "    # Generate local temporal graph node indexes mapping\n",
    "    for e in events:\n",
    "        if e[3] not in node_uuid2path or e[0]  not in node_uuid2path:\n",
    "            continue\n",
    "\n",
    "        if e[0] in node_uuid2index:\n",
    "            pass\n",
    "        else:\n",
    "            node_uuid2index[e[0]]=temp_index\n",
    "            node_uuid2index[temp_index]=node_uuid2path[e[0]]\n",
    "            temp_index+=1\n",
    "\n",
    "        if e[3] in node_uuid2index:\n",
    "            pass\n",
    "        else:\n",
    "            node_uuid2index[e[3]]=temp_index\n",
    "            node_uuid2index[temp_index]=node_uuid2path[e[3]]\n",
    "            temp_index+=1 \n",
    "\n",
    "    torch.save(node_uuid2index,f'node_uuid2index_9_{day}_host={hostname}_datalabel={datalabel}')\n",
    "       \n",
    "\n",
    "    dataset = TemporalData()\n",
    "    src = []\n",
    "    dst = []\n",
    "    msg = []\n",
    "    t = []\n",
    "    # Create temporal graph with nodes, edges, messages and times\n",
    "    for e in (events):\n",
    "        if e[3] in node_uuid2index and e[0] in node_uuid2index:\n",
    "            # If the image path of the node is not recorded, then skip this edge\n",
    "            src.append(node_uuid2index[e[0]])\n",
    "            dst.append(node_uuid2index[e[3]])\n",
    "        #     msg.append(torch.cat([torch.from_numpy(node2higvec_bn[i[0]]), rel2vec[i[2]], torch.from_numpy(node2higvec_bn[i[1]])] ))\n",
    "\n",
    "            msg.append(torch.cat([str2tensor(e[1],node_uuid2path[e[0]]), \n",
    "                                  edge2vec[e[2]], \n",
    "                                  str2tensor(e[4],node_uuid2path[e[3]])\n",
    "                                 ]))\n",
    "            t.append(int(e[6]))\n",
    "\n",
    "    dataset.src = torch.tensor(src)\n",
    "    dataset.dst = torch.tensor(dst)\n",
    "    dataset.t = torch.tensor(t)\n",
    "    dataset.msg = torch.vstack(msg)\n",
    "    dataset.src = dataset.src.to(torch.long)\n",
    "    dataset.dst = dataset.dst.to(torch.long)\n",
    "    dataset.msg = dataset.msg.to(torch.float)\n",
    "    dataset.t = dataset.t.to(torch.long)\n",
    "    # Store temporal graph in disk\n",
    "    torch.save(dataset, f\"/home/shahidul2k9/data/optc/out/evaluation/9_{day}_host={hostname}_datalabel={datalabel}.TemporalData\")  \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## h501 23-25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(events)=2394555\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 1/3 [13:24<26:48, 804.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(events)=3954364\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 2/3 [35:27<18:29, 1109.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(events)=2386523\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [49:07<00:00, 982.36s/it] \n"
     ]
    }
   ],
   "source": [
    "# Read evaluation dataset generated by host SysClient0501 on 23rd-25th September 2019 and create temporal graph\n",
    "for day in tqdm(range(23,26)):\n",
    "    start_timestamp=datetime_to_timestamp_US('2019-09-'+str(day)+'T00:00:00')\n",
    "    end_timestamp=datetime_to_timestamp_US('2019-09-'+str(day+1)+'T00:00:00')\n",
    "    hostname='SysClient0501'\n",
    "    datalabel='evaluation'\n",
    "    # Create SQL query command\n",
    "    sql=f\"\"\"\n",
    "    select * from event_table\n",
    "    where\n",
    "          timestamp>{start_timestamp} and timestamp<{end_timestamp}\n",
    "          and hostname='{hostname}' and data_label='{datalabel}' ORDER BY timestamp;\n",
    "    \"\"\"\n",
    "    # Execute SQL query\n",
    "    cur.execute(sql)\n",
    "    events = cur.fetchall()\n",
    "    print(f\"{len(events)=}\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    node_set=set()\n",
    "    node_uuid2index={}\n",
    "    temp_index=0\n",
    "    for e in events:\n",
    "        if e[3] not in node_uuid2path or e[0]  not in node_uuid2path:\n",
    "            continue\n",
    "\n",
    "        if e[0] in node_uuid2index:\n",
    "            pass\n",
    "        else:\n",
    "            node_uuid2index[e[0]]=temp_index\n",
    "            node_uuid2index[temp_index]=node_uuid2path[e[0]]\n",
    "            temp_index+=1\n",
    "\n",
    "        if e[3] in node_uuid2index:\n",
    "            pass\n",
    "        else:\n",
    "            node_uuid2index[e[3]]=temp_index\n",
    "            node_uuid2index[temp_index]=node_uuid2path[e[3]]\n",
    "            temp_index+=1 \n",
    "\n",
    "    torch.save(node_uuid2index,f'node_uuid2index_9_{day}_host={hostname}_datalabel={datalabel}')\n",
    "       \n",
    "\n",
    "    dataset = TemporalData()\n",
    "    src = []\n",
    "    dst = []\n",
    "    msg = []\n",
    "    t = []\n",
    "    # Generate local temporal graph node indexes mapping\n",
    "    for e in (events):\n",
    "        if e[3] in node_uuid2index and e[0] in node_uuid2index:\n",
    "            # If the image path of the node is not recorded, then skip this edge\n",
    "            src.append(node_uuid2index[e[0]])\n",
    "            dst.append(node_uuid2index[e[3]])\n",
    "        #     msg.append(torch.cat([torch.from_numpy(node2higvec_bn[i[0]]), rel2vec[i[2]], torch.from_numpy(node2higvec_bn[i[1]])] ))\n",
    "\n",
    "            msg.append(torch.cat([str2tensor(e[1],node_uuid2path[e[0]]), \n",
    "                                  edge2vec[e[2]], \n",
    "                                  str2tensor(e[4],node_uuid2path[e[3]])\n",
    "                                 ]))\n",
    "            t.append(int(e[6]))\n",
    "\n",
    "    dataset.src = torch.tensor(src)\n",
    "    dataset.dst = torch.tensor(dst)\n",
    "    dataset.t = torch.tensor(t)\n",
    "    dataset.msg = torch.vstack(msg)\n",
    "    dataset.src = dataset.src.to(torch.long)\n",
    "    dataset.dst = dataset.dst.to(torch.long)\n",
    "    dataset.msg = dataset.msg.to(torch.float)\n",
    "    dataset.t = dataset.t.to(torch.long)\n",
    "    # Store temporal graph in disk\n",
    "    torch.save(dataset, f\"/home/shahidul2k9/data/optc/out/evaluation/9_{day}_host={hostname}_datalabel={datalabel}.TemporalData\")  \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## h051 23-25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(events)=2381599\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 1/3 [13:17<26:34, 797.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(events)=3741872\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 2/3 [33:46<17:31, 1051.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(events)=2322399\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [47:24<00:00, 948.05s/it] \n"
     ]
    }
   ],
   "source": [
    "# Read evaluation dataset generated by host SysClient0051 on 23rd-25th September 2019 and create temporal graph\n",
    "for day in tqdm(range(23,26)):\n",
    "    start_timestamp=datetime_to_timestamp_US('2019-09-'+str(day)+'T00:00:00')\n",
    "    end_timestamp=datetime_to_timestamp_US('2019-09-'+str(day+1)+'T00:00:00')\n",
    "    hostname='SysClient0051'\n",
    "    datalabel='evaluation'\n",
    "    # Create SQL query command\n",
    "    sql=f\"\"\"\n",
    "    select * from event_table\n",
    "    where\n",
    "          timestamp>{start_timestamp} and timestamp<{end_timestamp}\n",
    "          and hostname='{hostname}' and data_label='{datalabel}' ORDER BY timestamp;\n",
    "    \"\"\"\n",
    "    # Execute SQL query\n",
    "    cur.execute(sql)\n",
    "    events = cur.fetchall()\n",
    "    print(f\"{len(events)=}\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    node_set=set()\n",
    "    node_uuid2index={}\n",
    "    temp_index=0\n",
    "    # Generate local temporal graph node indexes mapping\n",
    "    for e in events:\n",
    "        if e[3] not in node_uuid2path or e[0]  not in node_uuid2path:\n",
    "            continue\n",
    "\n",
    "        if e[0] in node_uuid2index:\n",
    "            pass\n",
    "        else:\n",
    "            node_uuid2index[e[0]]=temp_index\n",
    "            node_uuid2index[temp_index]=node_uuid2path[e[0]]\n",
    "            temp_index+=1\n",
    "\n",
    "        if e[3] in node_uuid2index:\n",
    "            pass\n",
    "        else:\n",
    "            node_uuid2index[e[3]]=temp_index\n",
    "            node_uuid2index[temp_index]=node_uuid2path[e[3]]\n",
    "            temp_index+=1 \n",
    "\n",
    "    torch.save(node_uuid2index,f'node_uuid2index_9_{day}_host={hostname}_datalabel={datalabel}')\n",
    "       \n",
    "\n",
    "    dataset = TemporalData()\n",
    "    src = []\n",
    "    dst = []\n",
    "    msg = []\n",
    "    t = []\n",
    "    # Create temporal graph with nodes, edges, messages and times\n",
    "    for e in (events):\n",
    "        if e[3] in node_uuid2index and e[0] in node_uuid2index:\n",
    "            # If the image path of the node is not recorded, then skip this edge\n",
    "            src.append(node_uuid2index[e[0]])\n",
    "            dst.append(node_uuid2index[e[3]])\n",
    "        #     msg.append(torch.cat([torch.from_numpy(node2higvec_bn[i[0]]), rel2vec[i[2]], torch.from_numpy(node2higvec_bn[i[1]])] ))\n",
    "\n",
    "            msg.append(torch.cat([str2tensor(e[1],node_uuid2path[e[0]]), \n",
    "                                  edge2vec[e[2]], \n",
    "                                  str2tensor(e[4],node_uuid2path[e[3]])\n",
    "                                 ]))\n",
    "            t.append(int(e[6]))\n",
    "\n",
    "    dataset.src = torch.tensor(src)\n",
    "    dataset.dst = torch.tensor(dst)\n",
    "    dataset.t = torch.tensor(t)\n",
    "    dataset.msg = torch.vstack(msg)\n",
    "    dataset.src = dataset.src.to(torch.long)\n",
    "    dataset.dst = dataset.dst.to(torch.long)\n",
    "    dataset.msg = dataset.msg.to(torch.float)\n",
    "    dataset.t = dataset.t.to(torch.long)\n",
    "    # Store temporal graph in disk\n",
    "    torch.save(dataset, f\"/home/shahidul2k9/data/optc/out/evaluation/9_{day}_host={hostname}_datalabel={datalabel}.TemporalData\")  \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## h207 23-25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(events)=2303987\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 1/3 [14:05<28:10, 845.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(events)=3642215\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 2/3 [35:19<18:17, 1097.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(events)=2171961\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [49:06<00:00, 982.22s/it] \n"
     ]
    }
   ],
   "source": [
    "# Read evaluation dataset generated by host SysClient0207 on 23rd-25th September 2019 and create temporal graph\n",
    "for day in tqdm(range(23,26)):\n",
    "    start_timestamp=datetime_to_timestamp_US('2019-09-'+str(day)+'T00:00:00')\n",
    "    end_timestamp=datetime_to_timestamp_US('2019-09-'+str(day+1)+'T00:00:00')\n",
    "    hostname='SysClient0207'\n",
    "    datalabel='evaluation'\n",
    "    # Create SQL query command\n",
    "    sql=f\"\"\"\n",
    "    select * from event_table\n",
    "    where\n",
    "          timestamp>{start_timestamp} and timestamp<{end_timestamp}\n",
    "          and hostname='{hostname}' and data_label='{datalabel}' ORDER BY timestamp;\n",
    "    \"\"\"\n",
    "    # Execute SQL query\n",
    "    cur.execute(sql)\n",
    "    events = cur.fetchall()\n",
    "    print(f\"{len(events)=}\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    node_set=set()\n",
    "    node_uuid2index={}\n",
    "    temp_index=0\n",
    "    # Generate local temporal graph node indexes mapping\n",
    "    for e in events:\n",
    "        if e[3] not in node_uuid2path or e[0]  not in node_uuid2path:\n",
    "            continue\n",
    "\n",
    "        if e[0] in node_uuid2index:\n",
    "            pass\n",
    "        else:\n",
    "            node_uuid2index[e[0]]=temp_index\n",
    "            node_uuid2index[temp_index]=node_uuid2path[e[0]]\n",
    "            temp_index+=1\n",
    "\n",
    "        if e[3] in node_uuid2index:\n",
    "            pass\n",
    "        else:\n",
    "            node_uuid2index[e[3]]=temp_index\n",
    "            node_uuid2index[temp_index]=node_uuid2path[e[3]]\n",
    "            temp_index+=1 \n",
    "\n",
    "    torch.save(node_uuid2index,f'node_uuid2index_9_{day}_host={hostname}_datalabel={datalabel}')\n",
    "       \n",
    "\n",
    "    dataset = TemporalData()\n",
    "    src = []\n",
    "    dst = []\n",
    "    msg = []\n",
    "    t = []\n",
    "    # Create temporal graph with nodes, edges, messages and times\n",
    "    for e in (events):\n",
    "        if e[3] in node_uuid2index and e[0] in node_uuid2index:\n",
    "            # If the image path of the node is not recorded, then skip this edge\n",
    "            src.append(node_uuid2index[e[0]])\n",
    "            dst.append(node_uuid2index[e[3]])\n",
    "        #     msg.append(torch.cat([torch.from_numpy(node2higvec_bn[i[0]]), rel2vec[i[2]], torch.from_numpy(node2higvec_bn[i[1]])] ))\n",
    "\n",
    "            msg.append(torch.cat([str2tensor(e[1],node_uuid2path[e[0]]), \n",
    "                                  edge2vec[e[2]], \n",
    "                                  str2tensor(e[4],node_uuid2path[e[3]])\n",
    "                                 ]))\n",
    "            t.append(int(e[6]))\n",
    "\n",
    "    dataset.src = torch.tensor(src)\n",
    "    dataset.dst = torch.tensor(dst)\n",
    "    dataset.t = torch.tensor(t)\n",
    "    dataset.msg = torch.vstack(msg)\n",
    "    dataset.src = dataset.src.to(torch.long)\n",
    "    dataset.dst = dataset.dst.to(torch.long)\n",
    "    dataset.msg = dataset.msg.to(torch.float)\n",
    "    dataset.t = dataset.t.to(torch.long)\n",
    "    # Store temporal graph in disk\n",
    "    torch.save(dataset, f\"/home/shahidul2k9/data/optc/out/evaluation/9_{day}_host={hostname}_datalabel={datalabel}.TemporalData\")  \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A CSV file containing the ground truth nodes&edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './labels.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[67], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#Load ground truth labels\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m label_df\u001b[38;5;241m=\u001b[39m\u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./labels.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/kairos/lib/python3.9/site-packages/pandas/io/parsers/readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m   1014\u001b[0m     dialect,\n\u001b[1;32m   1015\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m   1023\u001b[0m )\n\u001b[1;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/kairos/lib/python3.9/site-packages/pandas/io/parsers/readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/anaconda3/envs/kairos/lib/python3.9/site-packages/pandas/io/parsers/readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/kairos/lib/python3.9/site-packages/pandas/io/parsers/readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1887\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1889\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/anaconda3/envs/kairos/lib/python3.9/site-packages/pandas/io/common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    874\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    875\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    876\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './labels.csv'"
     ]
    }
   ],
   "source": [
    "#Load ground truth labels\n",
    "label_df=pd.read_csv(\"./labels.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'label_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[68], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#Log ground truth labels\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mlabel_df\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'label_df' is not defined"
     ]
    }
   ],
   "source": [
    "#Log ground truth labels\n",
    "label_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'label_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[69], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m edges_attack_list\u001b[38;5;241m=\u001b[39m[]\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Attack edge listing\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx,row \u001b[38;5;129;01min\u001b[39;00m \u001b[43mlabel_df\u001b[49m\u001b[38;5;241m.\u001b[39miterrows():\n\u001b[1;32m      5\u001b[0m     flag\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mobjectID\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;129;01min\u001b[39;00m node_uuid2path:\n",
      "\u001b[0;31mNameError\u001b[0m: name 'label_df' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "nodes_attack={}\n",
    "edges_attack_list=[]\n",
    "# Attack edge listing\n",
    "for idx,row in label_df.iterrows():\n",
    "    flag=False\n",
    "    if row['objectID'] in node_uuid2path:\n",
    "        nodes_attack[row['objectID']]=node_uuid2path[row['objectID']]\n",
    "        flag=True\n",
    "    if row['actorID'] in node_uuid2path:\n",
    "        nodes_attack[row['actorID']]=node_uuid2path[row['actorID']]\n",
    "        flag=True\n",
    "    if flag and row['action'] in edge2vec:    \n",
    "#         and row['action'] in edge2vec\n",
    "        temp_dic={}\n",
    "        temp_dic['src_uuid']=row['actorID']\n",
    "        temp_dic['dst_uuid']=row['objectID']\n",
    "        temp_dic['edge_type']=row['action']\n",
    "        temp_dic['timestamp']=datetime_to_timestamp_US(row['timestamp'])\n",
    "\n",
    "        edges_attack_list.append(temp_dic)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Log number of attack edges\n",
    "len(edges_attack_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Log number of attack nodes\n",
    "len(nodes_attack)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistics (Num of nodes and edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load evaluation temporal graph\n",
    "# TODO : not found while preprocessing\n",
    "#graph_9_22_h201=torch.load(\"/home/shahidul2k9/data/optc/out/evaluation/9_22_host=SysClient0201_datalabel=benign.TemporalData\")\n",
    "graph_9_22_h402=torch.load(\"/home/shahidul2k9/data/optc/out/evaluation/9_22_host=SysClient0402_datalabel=benign.TemporalData\")\n",
    "graph_9_22_h660=torch.load(\"/home/shahidul2k9/data/optc/out/evaluation/9_22_host=SysClient0660_datalabel=benign.TemporalData\")\n",
    "graph_9_22_h501=torch.load(\"/home/shahidul2k9/data/optc/out/evaluation/9_22_host=SysClient0501_datalabel=benign.TemporalData\")\n",
    "graph_9_22_h051=torch.load(\"/home/shahidul2k9/data/optc/out/evaluation/9_22_host=SysClient0051_datalabel=benign.TemporalData\")\n",
    "graph_9_22_h209=torch.load(\"/home/shahidul2k9/data/optc/out/evaluation/9_22_host=SysClient0209_datalabel=benign.TemporalData\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load evaluation temporal graph\n",
    "graph_9_23_h201=torch.load(\"/home/shahidul2k9/data/optc/out/evaluation/9_23_host=SysClient0201_datalabel=evaluation.TemporalData\")\n",
    "graph_9_24_h201=torch.load(\"/home/shahidul2k9/data/optc/out/evaluation/9_24_host=SysClient0201_datalabel=evaluation.TemporalData\")\n",
    "graph_9_25_h201=torch.load(\"/home/shahidul2k9/data/optc/out/evaluation/9_25_host=SysClient0201_datalabel=evaluation.TemporalData\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load evaluation temporal graph\n",
    "graph_9_23_h402=torch.load(\"/home/shahidul2k9/data/optc/out/evaluation/9_23_host=SysClient0402_datalabel=evaluation.TemporalData\")\n",
    "graph_9_24_h402=torch.load(\"/home/shahidul2k9/data/optc/out/evaluation/9_24_host=SysClient0402_datalabel=evaluation.TemporalData\")\n",
    "graph_9_25_h402=torch.load(\"/home/shahidul2k9/data/optc/out/evaluation/9_25_host=SysClient0402_datalabel=evaluation.TemporalData\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load evaluation temporal graph\n",
    "graph_9_23_h660=torch.load(\"/home/shahidul2k9/data/optc/out/evaluation/9_23_host=SysClient0660_datalabel=evaluation.TemporalData\")\n",
    "graph_9_24_h660=torch.load(\"/home/shahidul2k9/data/optc/out/evaluation/9_24_host=SysClient0660_datalabel=evaluation.TemporalData\")\n",
    "graph_9_25_h660=torch.load(\"/home/shahidul2k9/data/optc/out/evaluation/9_25_host=SysClient0660_datalabel=evaluation.TemporalData\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load evaluation temporal graph\n",
    "graph_9_23_h501=torch.load(\"/home/shahidul2k9/data/optc/out/evaluation/9_23_host=SysClient0501_datalabel=evaluation.TemporalData\")\n",
    "graph_9_24_h501=torch.load(\"/home/shahidul2k9/data/optc/out/evaluation/9_24_host=SysClient0501_datalabel=evaluation.TemporalData\")\n",
    "graph_9_25_h501=torch.load(\"/home/shahidul2k9/data/optc/out/evaluation/9_25_host=SysClient0501_datalabel=evaluation.TemporalData\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load evaluation temporal graph\n",
    "graph_9_23_h051=torch.load(\"/home/shahidul2k9/data/optc/out/evaluation/9_23_host=SysClient0051_datalabel=evaluation.TemporalData\")\n",
    "graph_9_24_h051=torch.load(\"/home/shahidul2k9/data/optc/out/evaluation/9_24_host=SysClient0051_datalabel=evaluation.TemporalData\")\n",
    "graph_9_25_h051=torch.load(\"/home/shahidul2k9/data/optc/out/evaluation/9_25_host=SysClient0051_datalabel=evaluation.TemporalData\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load evaluation temporal graph\n",
    "graph_9_23_h207=torch.load(\"/home/shahidul2k9/data/optc/out/evaluation/9_23_host=SysClient0207_datalabel=evaluation.TemporalData\")\n",
    "graph_9_24_h207=torch.load(\"/home/shahidul2k9/data/optc/out/evaluation/9_24_host=SysClient0207_datalabel=evaluation.TemporalData\")\n",
    "graph_9_25_h207=torch.load(\"/home/shahidul2k9/data/optc/out/evaluation/9_25_host=SysClient0207_datalabel=evaluation.TemporalData\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'graph_9_23_h051' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 27\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Create an array of loaded temporal graphs\u001b[39;00m\n\u001b[1;32m      2\u001b[0m graphs\u001b[38;5;241m=\u001b[39m[\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;66;03m# TODO : Not found in preprocessing\u001b[39;00m\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;66;03m#graph_9_22_h201,\u001b[39;00m\n\u001b[1;32m      5\u001b[0m     graph_9_22_h402,\n\u001b[1;32m      6\u001b[0m     graph_9_22_h660,\n\u001b[1;32m      7\u001b[0m     graph_9_22_h501,\n\u001b[1;32m      8\u001b[0m     graph_9_22_h051,\n\u001b[1;32m      9\u001b[0m     graph_9_22_h209,\n\u001b[1;32m     10\u001b[0m     \n\u001b[1;32m     11\u001b[0m     graph_9_23_h201,\n\u001b[1;32m     12\u001b[0m     graph_9_24_h201,\n\u001b[1;32m     13\u001b[0m     graph_9_25_h201,\n\u001b[1;32m     14\u001b[0m     \n\u001b[1;32m     15\u001b[0m     graph_9_23_h402,\n\u001b[1;32m     16\u001b[0m     graph_9_24_h402,\n\u001b[1;32m     17\u001b[0m     graph_9_25_h402,\n\u001b[1;32m     18\u001b[0m     \n\u001b[1;32m     19\u001b[0m     graph_9_23_h660,\n\u001b[1;32m     20\u001b[0m     graph_9_24_h660,\n\u001b[1;32m     21\u001b[0m     graph_9_25_h660,\n\u001b[1;32m     22\u001b[0m     \n\u001b[1;32m     23\u001b[0m     graph_9_23_h501,\n\u001b[1;32m     24\u001b[0m     graph_9_24_h501,\n\u001b[1;32m     25\u001b[0m     graph_9_25_h501,\n\u001b[1;32m     26\u001b[0m     \n\u001b[0;32m---> 27\u001b[0m     \u001b[43mgraph_9_23_h051\u001b[49m,\n\u001b[1;32m     28\u001b[0m     graph_9_24_h051,\n\u001b[1;32m     29\u001b[0m     graph_9_25_h051,\n\u001b[1;32m     30\u001b[0m     \n\u001b[1;32m     31\u001b[0m     graph_9_23_h207,\n\u001b[1;32m     32\u001b[0m     graph_9_24_h207,\n\u001b[1;32m     33\u001b[0m     graph_9_25_h207,\n\u001b[1;32m     34\u001b[0m ]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'graph_9_23_h051' is not defined"
     ]
    }
   ],
   "source": [
    "# Create an array of loaded temporal graphs\n",
    "graphs=[\n",
    "    # TODO : Not found in preprocessing\n",
    "    #graph_9_22_h201,\n",
    "    graph_9_22_h402,\n",
    "    graph_9_22_h660,\n",
    "    graph_9_22_h501,\n",
    "    graph_9_22_h051,\n",
    "    graph_9_22_h209,\n",
    "    \n",
    "    graph_9_23_h201,\n",
    "    graph_9_24_h201,\n",
    "    graph_9_25_h201,\n",
    "    \n",
    "    graph_9_23_h402,\n",
    "    graph_9_24_h402,\n",
    "    graph_9_25_h402,\n",
    "    \n",
    "    graph_9_23_h660,\n",
    "    graph_9_24_h660,\n",
    "    graph_9_25_h660,\n",
    "    \n",
    "    graph_9_23_h501,\n",
    "    graph_9_24_h501,\n",
    "    graph_9_25_h501,\n",
    "    \n",
    "    graph_9_23_h051,\n",
    "    graph_9_24_h051,\n",
    "    graph_9_25_h051,\n",
    "    \n",
    "    graph_9_23_h207,\n",
    "    graph_9_24_h207,\n",
    "    graph_9_25_h207,\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count total number of edges over all evaluation temporal graphs\n",
    "edges_count=0\n",
    "for g in graphs:\n",
    "     edges_count+=len(g.t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "70760485"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Log temporal graphs edge count\n",
    "edges_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load unique node ID(UUID) to integer indexing\n",
    "#TODO : not found in preprocessing\n",
    "#node_uuid2index_9_22_h201=torch.load(\"node_uuid2index_9_22_host=SysClient0201_datalabel=benign\")\n",
    "node_uuid2index_9_22_h402=torch.load(\"node_uuid2index_9_22_host=SysClient0402_datalabel=benign\")\n",
    "node_uuid2index_9_22_h660=torch.load(\"node_uuid2index_9_22_host=SysClient0660_datalabel=benign\")\n",
    "node_uuid2index_9_22_h501=torch.load(\"node_uuid2index_9_22_host=SysClient0501_datalabel=benign\")\n",
    "node_uuid2index_9_22_h051=torch.load(\"node_uuid2index_9_22_host=SysClient0051_datalabel=benign\")\n",
    "node_uuid2index_9_22_h209=torch.load(\"node_uuid2index_9_22_host=SysClient0209_datalabel=benign\")\n",
    "\n",
    "\n",
    "node_uuid2index_9_23_h201=torch.load(\"node_uuid2index_9_23_host=SysClient0201_datalabel=evaluation\")\n",
    "node_uuid2index_9_24_h201=torch.load(\"node_uuid2index_9_24_host=SysClient0201_datalabel=evaluation\")\n",
    "node_uuid2index_9_25_h201=torch.load(\"node_uuid2index_9_25_host=SysClient0201_datalabel=evaluation\")\n",
    "\n",
    "node_uuid2index_9_23_h402=torch.load(\"node_uuid2index_9_23_host=SysClient0402_datalabel=evaluation\")\n",
    "node_uuid2index_9_24_h402=torch.load(\"node_uuid2index_9_24_host=SysClient0402_datalabel=evaluation\")\n",
    "node_uuid2index_9_25_h402=torch.load(\"node_uuid2index_9_25_host=SysClient0402_datalabel=evaluation\")\n",
    "\n",
    "node_uuid2index_9_23_h660=torch.load(\"node_uuid2index_9_23_host=SysClient0660_datalabel=evaluation\")\n",
    "node_uuid2index_9_24_h660=torch.load(\"node_uuid2index_9_24_host=SysClient0660_datalabel=evaluation\")\n",
    "node_uuid2index_9_25_h660=torch.load(\"node_uuid2index_9_25_host=SysClient0660_datalabel=evaluation\")\n",
    "\n",
    "node_uuid2index_9_23_h501=torch.load(\"node_uuid2index_9_23_host=SysClient0501_datalabel=evaluation\")\n",
    "node_uuid2index_9_24_h501=torch.load(\"node_uuid2index_9_24_host=SysClient0501_datalabel=evaluation\")\n",
    "node_uuid2index_9_25_h501=torch.load(\"node_uuid2index_9_25_host=SysClient0501_datalabel=evaluation\")\n",
    "\n",
    "node_uuid2index_9_23_h051=torch.load(\"node_uuid2index_9_23_host=SysClient0051_datalabel=evaluation\")\n",
    "node_uuid2index_9_24_h051=torch.load(\"node_uuid2index_9_24_host=SysClient0051_datalabel=evaluation\")\n",
    "node_uuid2index_9_25_h051=torch.load(\"node_uuid2index_9_25_host=SysClient0051_datalabel=evaluation\")\n",
    "\n",
    "node_uuid2index_9_23_h207=torch.load(\"node_uuid2index_9_23_host=SysClient0207_datalabel=evaluation\")\n",
    "node_uuid2index_9_24_h207=torch.load(\"node_uuid2index_9_24_host=SysClient0207_datalabel=evaluation\")\n",
    "node_uuid2index_9_25_h207=torch.load(\"node_uuid2index_9_25_host=SysClient0207_datalabel=evaluation\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of node indexing files\n",
    "node_dics=[\n",
    "    #node_uuid2index_9_22_h201,\n",
    "    node_uuid2index_9_22_h402,\n",
    "    node_uuid2index_9_22_h660,\n",
    "    node_uuid2index_9_22_h501,\n",
    "    node_uuid2index_9_22_h051,\n",
    "    node_uuid2index_9_22_h209,\n",
    "    node_uuid2index_9_23_h201,\n",
    "    node_uuid2index_9_24_h201,\n",
    "    node_uuid2index_9_25_h201,\n",
    "    node_uuid2index_9_23_h402,\n",
    "    node_uuid2index_9_24_h402,\n",
    "    node_uuid2index_9_25_h402,\n",
    "    node_uuid2index_9_23_h660,\n",
    "    node_uuid2index_9_24_h660,\n",
    "    node_uuid2index_9_25_h660,\n",
    "    node_uuid2index_9_23_h501,\n",
    "    node_uuid2index_9_24_h501,\n",
    "    node_uuid2index_9_25_h501,\n",
    "    node_uuid2index_9_23_h051,\n",
    "    node_uuid2index_9_24_h051,\n",
    "    node_uuid2index_9_25_h051,\n",
    "    node_uuid2index_9_23_h207,\n",
    "    node_uuid2index_9_24_h207,\n",
    "    node_uuid2index_9_25_h207,\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a set of unique nodes\n",
    "nodes=set()\n",
    "for dic in node_dics:\n",
    "    for n in dic:\n",
    "        if type(n)==str:\n",
    "            nodes.add(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9306260"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Log nodes\n",
    "len(nodes)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kairos",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "47.7109px",
    "left": "21px",
    "top": "204.141px",
    "width": "199.344px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

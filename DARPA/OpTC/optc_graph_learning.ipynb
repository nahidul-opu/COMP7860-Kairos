{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoding=utf-8\n",
    "import os.path as osp\n",
    "import os\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch.nn import Linear\n",
    "from sklearn.metrics import average_precision_score, roc_auc_score\n",
    "from torch_geometric.data import TemporalData\n",
    "\n",
    "from torch_geometric.nn import TGNMemory, TransformerConv\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn.models.tgn import (LastNeighborLoader, IdentityMessage, MeanAggregator,\n",
    "                                           LastAggregator)\n",
    "from torch_geometric import *\n",
    "from torch_geometric.utils import negative_sampling\n",
    "\n",
    "from tqdm import tqdm\n",
    "# from .autonotebook import tqdm as notebook_tqdm\n",
    "\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import math\n",
    "import copy\n",
    "import re\n",
    "import time\n",
    "import json\n",
    "import pandas as pd\n",
    "from random import choice\n",
    "import gc\n",
    "from graphviz import Digraph\n",
    "import xxhash\n",
    "\n",
    "from datetime import datetime, timezone\n",
    "import time\n",
    "import pytz\n",
    "from time import mktime\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Load utility methods\n",
    "def hashgen(l):\n",
    "    \"\"\"Generate a single hash value from a list. @l is a list of\n",
    "    string values, which can be properties of a node/edge. This\n",
    "    function returns a single hashed integer value.\"\"\"\n",
    "    hasher = xxhash.xxh64()\n",
    "    for e in l:\n",
    "        hasher.update(e)\n",
    "    return hasher.intdigest()\n",
    "\n",
    "\n",
    "def datetime_to_ns_time(date):\n",
    "    \"\"\"\n",
    "    :param date: str   format: %Y-%m-%d %H:%M:%S   e.g. 2013-10-10 23:40:00\n",
    "    :return: nano timestamp\n",
    "    \"\"\"\n",
    "    date,ns=date.split('.')\n",
    "\n",
    "    timeArray = time.strptime(date, '%Y-%m-%dT%H:%M:%S')\n",
    "    timeStamp = int(time.mktime(timeArray))\n",
    "    timeStamp = timeStamp * 1000000000\n",
    "    timeStamp += int(ns.split('Z')[0])\n",
    "    return timeStamp\n",
    "\n",
    "\n",
    "def datetime_to_timestamp_US(date):\n",
    "    \"\"\"\n",
    "    :param date: str   format: %Y-%m-%d %H:%M:%S   e.g. 2013-10-10 23:40:00\n",
    "    :return: nano timestamp\n",
    "    \"\"\"\n",
    "    date=date.replace('-04:00','')\n",
    "    if '.' in date:\n",
    "        date,ms=date.split('.')\n",
    "    else:\n",
    "        ms=0\n",
    "    tz = pytz.timezone('Etc/GMT+4')\n",
    "    timeArray = time.strptime(date, \"%Y-%m-%dT%H:%M:%S\")\n",
    "    dt = datetime.fromtimestamp(mktime(timeArray))\n",
    "    timestamp = tz.localize(dt)\n",
    "    timestamp=timestamp.timestamp()\n",
    "    timeStamp = timestamp*1000+int(ms)\n",
    "    return int(timeStamp)\n",
    "\n",
    "\n",
    "def timestamp_to_datetime_US(ns):\n",
    "    \"\"\"\n",
    "    :param date: str   format: %Y-%m-%d %H:%M:%S   e.g. 2013-10-10 23:40:00\n",
    "    :return: nano timestamp\n",
    "    \"\"\"\n",
    "    tz = pytz.timezone('US/Eastern')\n",
    "    ms=ns%1000\n",
    "    ns//=1000\n",
    "    dt = pytz.datetime.datetime.fromtimestamp(int(ns), tz)\n",
    "    s = dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "    s+='.'+str(ms)\n",
    "#     s += '.' + str(int(int(ns) % 1000000000)).zfill(9)\n",
    "    return s\n",
    "\n",
    "pid_split_symble=\"#_\"\n",
    "\n",
    "host_split_symble=\"_@\"\n",
    "\n",
    "\n",
    "\n",
    "import xxhash\n",
    "\n",
    "def tensor_find(t,x):\n",
    "    t_np=t.numpy()\n",
    "    idx=np.argwhere(t_np==x)\n",
    "    return idx[0][0]+1\n",
    "\n",
    "\n",
    "def std(t):\n",
    "    t = np.array(t)\n",
    "    return np.std(t)\n",
    "\n",
    "\n",
    "def var(t):\n",
    "    t = np.array(t)\n",
    "    return np.var(t)\n",
    "\n",
    "\n",
    "def mean(t):\n",
    "    t = np.array(t)\n",
    "    return np.mean(t)\n",
    "\n",
    "def hashgen(l):\n",
    "    \"\"\"Generate a single hash value from a list. @l is a list of\n",
    "    string values, which can be properties of a node/edge. This\n",
    "    function returns a single hashed integer value.\"\"\"\n",
    "    hasher = xxhash.xxh64()\n",
    "    for e in l:\n",
    "        hasher.update(e)\n",
    "    return hasher.intdigest()\n",
    "\n",
    "\n",
    "def cal_pos_edges_loss(link_pred_ratio):\n",
    "    loss=[]\n",
    "    for i in link_pred_ratio:\n",
    "        loss.append(criterion(i,torch.ones(1)))\n",
    "    return torch.tensor(loss)\n",
    "\n",
    "def cal_pos_edges_loss_multiclass(link_pred_ratio,labels):\n",
    "    loss=[] \n",
    "    for i in range(len(link_pred_ratio)):\n",
    "        loss.append(criterion(link_pred_ratio[i].reshape(1,-1),labels[i].reshape(-1)))\n",
    "    return torch.tensor(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Connect to database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "\n",
    "from psycopg2 import extras as ex\n",
    "# Create a postgreSQL DB connection object for storing provenance graph edges into DB\n",
    "# Original '/var/run/postgresql/' has been replaced with 'localhost' since we are using docker and accessing as a service in port 5437\n",
    "connect = psycopg2.connect(database = 'optc_db',\n",
    "                           host = 'localhost',\n",
    "                           user = 'postgres',\n",
    "                           password = 'postgres',\n",
    "                           port = '5437'\n",
    "                          )\n",
    "\n",
    "cur = connect.cursor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading benign dataset/temporal graphs\n",
    "# This graph not preprocessed, likely pre-processing bug\n",
    "#graph_9_22_h201=torch.load(\"/home/shahidul2k9/data/optc/out/evaluation/9_22_host=SysClient0201_datalabel=benign.TemporalData\")\n",
    "graph_9_22_h402=torch.load(\"/home/shahidul2k9/data/optc/out/evaluation/9_22_host=SysClient0402_datalabel=benign.TemporalData\")\n",
    "graph_9_22_h660=torch.load(\"/home/shahidul2k9/data/optc/out/evaluation/9_22_host=SysClient0660_datalabel=benign.TemporalData\")\n",
    "graph_9_22_h501=torch.load(\"/home/shahidul2k9/data/optc/out/evaluation/9_22_host=SysClient0501_datalabel=benign.TemporalData\")\n",
    "graph_9_22_h051=torch.load(\"/home/shahidul2k9/data/optc/out/evaluation/9_22_host=SysClient0051_datalabel=benign.TemporalData\")\n",
    "graph_9_22_h209=torch.load(\"/home/shahidul2k9/data/optc/out/evaluation/9_22_host=SysClient0209_datalabel=benign.TemporalData\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data=graph_9_22_h660"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load testing dataset/temporal graphs\n",
    "\n",
    "graph_9_23_h201=torch.load(\"/home/shahidul2k9/data/optc/out/evaluation/9_23_host=SysClient0201_datalabel=evaluation.TemporalData\")\n",
    "graph_9_23_h402=torch.load(\"/home/shahidul2k9/data/optc/out/evaluation/9_23_host=SysClient0402_datalabel=evaluation.TemporalData\")\n",
    "graph_9_23_h660=torch.load(\"/home/shahidul2k9/data/optc/out/evaluation/9_23_host=SysClient0660_datalabel=evaluation.TemporalData\")\n",
    "graph_9_23_h501=torch.load(\"/home/shahidul2k9/data/optc/out/evaluation/9_23_host=SysClient0501_datalabel=evaluation.TemporalData\")\n",
    "graph_9_23_h051=torch.load(\"/home/shahidul2k9/data/optc/out/evaluation/9_23_host=SysClient0051_datalabel=evaluation.TemporalData\")\n",
    "graph_9_23_h207=torch.load(\"/home/shahidul2k9/data/optc/out/evaluation/9_23_host=SysClient0207_datalabel=evaluation.TemporalData\")\n",
    "\n",
    "\n",
    "graph_9_24_h201=torch.load(\"/home/shahidul2k9/data/optc/out/evaluation/9_24_host=SysClient0201_datalabel=evaluation.TemporalData\")\n",
    "graph_9_24_h402=torch.load(\"/home/shahidul2k9/data/optc/out/evaluation/9_24_host=SysClient0402_datalabel=evaluation.TemporalData\")\n",
    "graph_9_24_h660=torch.load(\"/home/shahidul2k9/data/optc/out/evaluation/9_24_host=SysClient0660_datalabel=evaluation.TemporalData\")\n",
    "graph_9_24_h501=torch.load(\"/home/shahidul2k9/data/optc/out/evaluation/9_24_host=SysClient0501_datalabel=evaluation.TemporalData\")\n",
    "graph_9_24_h051=torch.load(\"/home/shahidul2k9/data/optc/out/evaluation/9_24_host=SysClient0051_datalabel=evaluation.TemporalData\")\n",
    "graph_9_24_h207=torch.load(\"/home/shahidul2k9/data/optc/out/evaluation/9_24_host=SysClient0207_datalabel=evaluation.TemporalData\")\n",
    "\n",
    "\n",
    "graph_9_25_h201=torch.load(\"/home/shahidul2k9/data/optc/out/evaluation/9_25_host=SysClient0201_datalabel=evaluation.TemporalData\")\n",
    "graph_9_25_h402=torch.load(\"/home/shahidul2k9/data/optc/out/evaluation/9_25_host=SysClient0402_datalabel=evaluation.TemporalData\")\n",
    "graph_9_25_h660=torch.load(\"/home/shahidul2k9/data/optc/out/evaluation/9_25_host=SysClient0660_datalabel=evaluation.TemporalData\")\n",
    "graph_9_25_h501=torch.load(\"/home/shahidul2k9/data/optc/out/evaluation/9_25_host=SysClient0501_datalabel=evaluation.TemporalData\")\n",
    "graph_9_25_h051=torch.load(\"/home/shahidul2k9/data/optc/out/evaluation/9_25_host=SysClient0051_datalabel=evaluation.TemporalData\")\n",
    "graph_9_25_h207=torch.load(\"/home/shahidul2k9/data/optc/out/evaluation/9_25_host=SysClient0207_datalabel=evaluation.TemporalData\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data=graph_9_25_h207"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO : investigate, authors didn't preprocessed this files why?\n",
    "#  graph_9_19=torch.load(\"/home/shahidul2k9/data/optc/out/benign/9-19-h201.TemporalData\")\n",
    "# graph_9_20=torch.load(\"/home/shahidul2k9/data/optc/out/benign/9-20-h201.TemporalData\")\n",
    "# graph_9_21=torch.load(\"/home/shahidul2k9/data/optc/out/benign/9-21-h201.TemporalData\")\n",
    "# graph_9_22=torch.load(\"/home/shahidul2k9/data/optc/out/benign/9-22-h201.TemporalData\")\n",
    "# graph_9_23=torch.load(\"/home/shahidul2k9/data/optc/out/evaluation/9-23-h201.TemporalData\")\n",
    "\n",
    "#train_data=graph_9_22"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate node2msg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18965643/18965643 [00:09<00:00, 1942777.42it/s]\n"
     ]
    }
   ],
   "source": [
    "# Constructing the map for nodeid to msg\n",
    "sql=\"select * from nodeid2msg;\"\n",
    "cur.execute(sql)\n",
    "rows = cur.fetchall()\n",
    "\n",
    "node_uuid2path={}  # nodeid => msg and node hash => nodeid\n",
    "for i in tqdm(rows):\n",
    "    node_uuid2path[i[0]]=i[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The node2index of training data\n",
    "# TODO : investigate, authors didn't preprocessed this files why?\n",
    "#node_uuid2index_9_17_h201=torch.load(\"node_uuid2index_9_17_host=SysClient0201_datalabel=benign\")\n",
    "#node_uuid2index_9_18_h201=torch.load(\"node_uuid2index_9_18_host=SysClient0201_datalabel=benign\")\n",
    "#node_uuid2index_9_19_h201=torch.load(\"node_uuid2index_9_19_host=SysClient0201_datalabel=benign\")\n",
    "#node_uuid2index_9_20_h201=torch.load(\"node_uuid2index_9_20_host=SysClient0201_datalabel=benign\")\n",
    "#node_uuid2index_9_21_h201=torch.load(\"node_uuid2index_9_21_host=SysClient0201_datalabel=benign\")\n",
    "#node_uuid2index_9_22_h201=torch.load(\"node_uuid2index_9_22_host=SysClient0201_datalabel=benign\")\n",
    "#node_uuid2index_9_23_h201=torch.load(\"node_uuid2index_9_23_host=SysClient0201_datalabel=benign\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO : investigate, authors didn't preprocessed this files why?\n",
    "#node_uuid2index_9_22_h201=torch.load(\"node_uuid2index_9_22_host=SysClient0201_datalabel=benign\")\n",
    "node_uuid2index_9_22_h402=torch.load(\"node_uuid2index_9_22_host=SysClient0402_datalabel=benign\")\n",
    "node_uuid2index_9_22_h660=torch.load(\"node_uuid2index_9_22_host=SysClient0660_datalabel=benign\")\n",
    "node_uuid2index_9_22_h501=torch.load(\"node_uuid2index_9_22_host=SysClient0501_datalabel=benign\")\n",
    "node_uuid2index_9_22_h051=torch.load(\"node_uuid2index_9_22_host=SysClient0051_datalabel=benign\")\n",
    "node_uuid2index_9_22_h209=torch.load(\"node_uuid2index_9_22_host=SysClient0209_datalabel=benign\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The node2index of testing data\n",
    "node_uuid2index_9_23_h201=torch.load(\"node_uuid2index_9_23_host=SysClient0201_datalabel=evaluation\")\n",
    "node_uuid2index_9_24_h201=torch.load(\"node_uuid2index_9_24_host=SysClient0201_datalabel=evaluation\")\n",
    "node_uuid2index_9_25_h201=torch.load(\"node_uuid2index_9_25_host=SysClient0201_datalabel=evaluation\")\n",
    "\n",
    "node_uuid2index_9_23_h402=torch.load(\"node_uuid2index_9_23_host=SysClient0402_datalabel=evaluation\")\n",
    "node_uuid2index_9_24_h402=torch.load(\"node_uuid2index_9_24_host=SysClient0402_datalabel=evaluation\")\n",
    "node_uuid2index_9_25_h402=torch.load(\"node_uuid2index_9_25_host=SysClient0402_datalabel=evaluation\")\n",
    "\n",
    "node_uuid2index_9_23_h660=torch.load(\"node_uuid2index_9_23_host=SysClient0660_datalabel=evaluation\")\n",
    "node_uuid2index_9_24_h660=torch.load(\"node_uuid2index_9_24_host=SysClient0660_datalabel=evaluation\")\n",
    "node_uuid2index_9_25_h660=torch.load(\"node_uuid2index_9_25_host=SysClient0660_datalabel=evaluation\")\n",
    "\n",
    "node_uuid2index_9_23_h501=torch.load(\"node_uuid2index_9_23_host=SysClient0501_datalabel=evaluation\")\n",
    "node_uuid2index_9_24_h501=torch.load(\"node_uuid2index_9_24_host=SysClient0501_datalabel=evaluation\")\n",
    "node_uuid2index_9_25_h501=torch.load(\"node_uuid2index_9_25_host=SysClient0501_datalabel=evaluation\")\n",
    "\n",
    "node_uuid2index_9_23_h051=torch.load(\"node_uuid2index_9_23_host=SysClient0051_datalabel=evaluation\")\n",
    "node_uuid2index_9_24_h051=torch.load(\"node_uuid2index_9_24_host=SysClient0051_datalabel=evaluation\")\n",
    "node_uuid2index_9_25_h051=torch.load(\"node_uuid2index_9_25_host=SysClient0051_datalabel=evaluation\")\n",
    "\n",
    "node_uuid2index_9_23_h207=torch.load(\"node_uuid2index_9_23_host=SysClient0207_datalabel=evaluation\")\n",
    "node_uuid2index_9_24_h207=torch.load(\"node_uuid2index_9_24_host=SysClient0207_datalabel=evaluation\")\n",
    "node_uuid2index_9_25_h207=torch.load(\"node_uuid2index_9_25_host=SysClient0207_datalabel=evaluation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO : investigate, authors didn't preprocessed this files why?\n",
    "# Calculate maximum possible node index over all the temporal graphs\n",
    "maxnode_num=max(\n",
    "    #len(node_uuid2index_9_17_h201)//2+1,\n",
    "    # len(node_uuid2index_9_18_h201)//2+1,\n",
    "    # len(node_uuid2index_9_19_h201)//2+1,\n",
    "    # len(node_uuid2index_9_20_h201)//2+1,\n",
    "    # len(node_uuid2index_9_21_h201)//2+1,\n",
    "    # len(node_uuid2index_9_22_h201)//2+1,\n",
    "    # len(node_uuid2index_9_23_h201)//2+1,\n",
    "    \n",
    "    # len(node_uuid2index_9_22_h201)//2+1,\n",
    "    len(node_uuid2index_9_22_h402)//2+1,\n",
    "    len(node_uuid2index_9_22_h660)//2+1,\n",
    "    len(node_uuid2index_9_22_h501)//2+1,\n",
    "    len(node_uuid2index_9_22_h051)//2+1,\n",
    "    len(node_uuid2index_9_22_h209)//2+1,\n",
    "    \n",
    "    len(node_uuid2index_9_23_h201)//2+1,\n",
    "    len(node_uuid2index_9_24_h201)//2+1,\n",
    "    len(node_uuid2index_9_25_h201)//2+1,\n",
    "    len(node_uuid2index_9_23_h402)//2+1,\n",
    "    len(node_uuid2index_9_24_h402)//2+1,\n",
    "    len(node_uuid2index_9_25_h402)//2+1,    \n",
    "    \n",
    "    len(node_uuid2index_9_23_h660)//2+1,\n",
    "    len(node_uuid2index_9_24_h660)//2+1,\n",
    "    len(node_uuid2index_9_25_h660)//2+1,\n",
    "    \n",
    "    len(node_uuid2index_9_23_h501)//2+1,\n",
    "    len(node_uuid2index_9_24_h501)//2+1,\n",
    "    len(node_uuid2index_9_25_h501)//2+1,\n",
    "    \n",
    "    len(node_uuid2index_9_23_h051)//2+1,\n",
    "    len(node_uuid2index_9_24_h051)//2+1,\n",
    "    len(node_uuid2index_9_25_h051)//2+1,\n",
    "    \n",
    "    len(node_uuid2index_9_23_h207)//2+1,\n",
    "    len(node_uuid2index_9_24_h207)//2+1,\n",
    "    len(node_uuid2index_9_25_h207)//2+1,    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "691859"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Log max node index over all graphs\n",
    "maxnode_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Edge <==> Index mapping\n",
    "rel2id={1: 'OPEN',\n",
    " 'OPEN': 1,\n",
    " 2: 'READ',\n",
    " 'READ': 2,\n",
    " 3: 'CREATE',\n",
    " 'CREATE': 3,\n",
    " 4: 'MESSAGE',\n",
    " 'MESSAGE': 4,\n",
    " 5: 'MODIFY',\n",
    " 'MODIFY': 5,\n",
    " 6: 'START',\n",
    " 'START': 6,\n",
    " 7: 'RENAME',\n",
    " 'RENAME': 7,\n",
    " 8: 'DELETE',\n",
    " 'DELETE': 8,\n",
    " 9: 'TERMINATE',\n",
    " 'TERMINATE': 9,\n",
    " 10: 'WRITE',\n",
    " 'WRITE': 10}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting the parameters and Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set embedding dimensions and other parameters\n",
    "time_dim = 100\n",
    "edge_embedding_dimension=100\n",
    "embedding_dim = edge_embedding_dimension\n",
    "neighbor_size=20\n",
    "memory_dim = 100\n",
    "\n",
    "max_node_num = maxnode_num+2\n",
    "min_dst_idx, max_dst_idx = 0, max_node_num\n",
    "# Set neighbour loader\n",
    "neighbor_loader = LastNeighborLoader(max_node_num, size=neighbor_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Set batch size\n",
    "BATCH=1024\n",
    "# Configure graph attention layer\n",
    "class GraphAttentionEmbedding(torch.nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, msg_dim, time_enc):\n",
    "        super(GraphAttentionEmbedding, self).__init__()\n",
    "        self.time_enc = time_enc\n",
    "        edge_dim = msg_dim + time_enc.out_channels\n",
    "        # Set two succesive convolution filters\n",
    "        self.conv = TransformerConv(in_channels, out_channels, heads=8,\n",
    "                                    dropout=0.0, edge_dim=edge_dim)\n",
    "        self.conv2 = TransformerConv(out_channels*8, out_channels,heads=1, concat=False,\n",
    "                             dropout=0.0, edge_dim=edge_dim)\n",
    "\n",
    "    def forward(self, x, last_update, edge_index, t, msg):\n",
    "        last_update.to(device)\n",
    "        x = x.to(device)\n",
    "        t = t.to(device)\n",
    "        rel_t = last_update[edge_index[0]] - t\n",
    "        rel_t_enc = self.time_enc(rel_t.to(x.dtype))\n",
    "        edge_attr = torch.cat([rel_t_enc, msg], dim=-1)\n",
    "        x = F.relu(self.conv(x, edge_index, edge_attr))\n",
    "        x = F.relu(self.conv2(x, edge_index, edge_attr))\n",
    "        return x\n",
    "\n",
    "# Configure Link Predictor\n",
    "class LinkPredictor(torch.nn.Module):\n",
    "    def __init__(self, in_channels):\n",
    "        super(LinkPredictor, self).__init__()\n",
    "        self.lin_src = Linear(in_channels, in_channels*2)\n",
    "        self.lin_dst = Linear(in_channels, in_channels*2)\n",
    "        self.lin_seq = nn.Sequential(\n",
    "            Linear(in_channels * 4, in_channels * 8),\n",
    "            torch.nn.BatchNorm1d(in_channels * 8),\n",
    "            torch.nn.Dropout(0.5),\n",
    "            nn.Tanh(),\n",
    "            Linear(in_channels * 8, in_channels * 2),\n",
    "            torch.nn.BatchNorm1d(in_channels * 2),\n",
    "            torch.nn.Dropout(0.5),\n",
    "            nn.Tanh(),\n",
    "            Linear(in_channels * 2, int(in_channels // 2)),\n",
    "            torch.nn.BatchNorm1d(int(in_channels // 2)),\n",
    "            torch.nn.Dropout(0.5),\n",
    "            nn.Tanh(),\n",
    "            Linear(int(in_channels // 2), train_data.msg.shape[1] - 32)\n",
    "        )\n",
    "\n",
    "    def forward(self, z_src, z_dst):\n",
    "        h = torch.cat([self.lin_src(z_src) , self.lin_dst(z_dst)],dim=-1)      \n",
    "        h = self.lin_seq (h)        \n",
    "        return h\n",
    "# Configure memory layer\n",
    "memory = TGNMemory(\n",
    "    max_node_num,\n",
    "    train_data.msg.size(-1),\n",
    "    memory_dim,\n",
    "    time_dim,\n",
    "    message_module=IdentityMessage(train_data.msg.size(-1), memory_dim, time_dim),\n",
    "    aggregator_module=LastAggregator(),\n",
    ").to(device)\n",
    "# Create graph attention embedding layer\n",
    "gnn = GraphAttentionEmbedding(\n",
    "    in_channels=memory_dim,\n",
    "    out_channels=embedding_dim,\n",
    "    msg_dim=train_data.msg.size(-1),\n",
    "    time_enc=memory.time_enc,\n",
    ").to(device)\n",
    "\n",
    "# Create link predictor\n",
    "link_pred = LinkPredictor(in_channels=embedding_dim).to(device)\n",
    "\n",
    "# Set Adam optimizer\n",
    "optimizer = torch.optim.Adam(\n",
    "    set(memory.parameters()) | set(gnn.parameters())\n",
    "    | set(link_pred.parameters()), lr=0.00005, eps=1e-08,weight_decay=0.01)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "assoc = torch.empty(max_node_num, dtype=torch.long, device=device)\n",
    "\n",
    "\n",
    "saved_nodes=set()\n",
    "\n",
    "BATCH=1024\n",
    "# Training method\n",
    "def train(train_data):\n",
    "\n",
    "    # Signal each layer to prepares for training modes\n",
    "    memory.train()\n",
    "    gnn.train()\n",
    "    link_pred.train()\n",
    "\n",
    "    memory.reset_state()  # Start with a fresh memory.\n",
    "    neighbor_loader.reset_state()  # Start with an empty graph.\n",
    "    saved_nodes=set()\n",
    "\n",
    "    total_loss = 0\n",
    "    # Iterate over the batches and train the models\n",
    "    for batch in train_data.seq_batches(batch_size=BATCH):\n",
    "        # Reset the gradiant of all parameters to zeroes\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        src, pos_dst, t, msg = batch.src, batch.dst, batch.t, batch.msg        \n",
    "\n",
    "        n_id = torch.cat([src, pos_dst]).unique()\n",
    "\n",
    "        n_id, edge_index, e_id = neighbor_loader(n_id)\n",
    "        assoc[n_id] = torch.arange(n_id.size(0), device=device)\n",
    "\n",
    "        # Get updated memory of all nodes involved in the computation.\n",
    "        z, last_update = memory(n_id)\n",
    "\n",
    "        z = gnn(z, last_update, edge_index, train_data.t[e_id], train_data.msg[e_id])\n",
    "\n",
    "        # Predict likelihood of positive edge\n",
    "        pos_out = link_pred(z[assoc[src]], z[assoc[pos_dst]])       \n",
    "\n",
    "        y_pred = torch.cat([pos_out], dim=0)\n",
    "\n",
    "        y_true=[]\n",
    "        for m in msg:\n",
    "            l=tensor_find(m[16:-16],1)-1\n",
    "            y_true.append(l)           \n",
    "\n",
    "        y_true = torch.tensor(y_true)\n",
    "        y_true=y_true.reshape(-1).to(torch.long)\n",
    "\n",
    "        loss = criterion(y_pred, y_true)     \n",
    "\n",
    "        # Update memory and neighbor loader with ground-truth state.\n",
    "        memory.update_state(src, pos_dst, t, msg)\n",
    "        neighbor_loader.insert(src, pos_dst)       \n",
    "        # Computes the gradients of the loss with respect to the model's parameters\n",
    "        loss.backward()\n",
    "        # Updates the model parameters using the computed gradients\n",
    "        optimizer.step()\n",
    "        # Detaches the memory tensor from the computation graph\n",
    "        memory.detach()\n",
    "        # sum losses at this epoch\n",
    "        total_loss += float(loss) * batch.num_events\n",
    "    return total_loss / train_data.num_events\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedding_dim=100\n",
      "gnn=GraphAttentionEmbedding(\n",
      "  (time_enc): TimeEncoder(\n",
      "    (lin): Linear(in_features=1, out_features=100, bias=True)\n",
      "  )\n",
      "  (conv): TransformerConv(100, 100, heads=8)\n",
      "  (conv2): TransformerConv(800, 100, heads=1)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch: 01, Loss: 1.4748\n",
      "  Epoch: 01, Loss: 0.9420\n",
      "  Epoch: 01, Loss: 0.8856\n",
      "  Epoch: 01, Loss: 0.8082\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 1/10 [59:15<8:53:19, 3555.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch: 01, Loss: 0.7344\n",
      "  Epoch: 02, Loss: 0.8230\n",
      "  Epoch: 02, Loss: 0.6913\n",
      "  Epoch: 02, Loss: 0.7675\n",
      "  Epoch: 02, Loss: 0.7262\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 2/10 [1:57:32<7:49:29, 3521.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch: 02, Loss: 0.6808\n",
      "  Epoch: 03, Loss: 0.7673\n",
      "  Epoch: 03, Loss: 0.6629\n",
      "  Epoch: 03, Loss: 0.7371\n",
      "  Epoch: 03, Loss: 0.7038\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 3/10 [2:55:11<6:47:28, 3492.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch: 03, Loss: 0.6667\n",
      "  Epoch: 04, Loss: 0.7465\n",
      "  Epoch: 04, Loss: 0.6527\n",
      "  Epoch: 04, Loss: 0.7245\n",
      "  Epoch: 04, Loss: 0.6950\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 4/10 [3:52:56<5:48:10, 3481.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch: 04, Loss: 0.6599\n",
      "  Epoch: 05, Loss: 0.7366\n",
      "  Epoch: 05, Loss: 0.6470\n",
      "  Epoch: 05, Loss: 0.7184\n",
      "  Epoch: 05, Loss: 0.6909\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 5/10 [4:50:06<4:48:36, 3463.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch: 05, Loss: 0.6559\n",
      "  Epoch: 06, Loss: 0.7317\n",
      "  Epoch: 06, Loss: 0.6450\n",
      "  Epoch: 06, Loss: 0.7150\n",
      "  Epoch: 06, Loss: 0.6878\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 6/10 [5:47:03<3:49:50, 3447.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch: 06, Loss: 0.6534\n",
      "  Epoch: 07, Loss: 0.7281\n",
      "  Epoch: 07, Loss: 0.6423\n",
      "  Epoch: 07, Loss: 0.7130\n",
      "  Epoch: 07, Loss: 0.6856\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 7/10 [6:44:11<2:52:03, 3441.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch: 07, Loss: 0.6522\n",
      "  Epoch: 08, Loss: 0.7259\n",
      "  Epoch: 08, Loss: 0.6410\n",
      "  Epoch: 08, Loss: 0.7110\n",
      "  Epoch: 08, Loss: 0.6842\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 8/10 [7:41:12<1:54:29, 3434.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch: 08, Loss: 0.6508\n",
      "  Epoch: 09, Loss: 0.7235\n",
      "  Epoch: 09, Loss: 0.6405\n",
      "  Epoch: 09, Loss: 0.7095\n",
      "  Epoch: 09, Loss: 0.6827\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 9/10 [8:37:56<57:05, 3425.19s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch: 09, Loss: 0.6496\n",
      "  Epoch: 10, Loss: 0.7215\n",
      "  Epoch: 10, Loss: 0.6392\n",
      "  Epoch: 10, Loss: 0.7086\n",
      "  Epoch: 10, Loss: 0.6817\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [9:34:00<00:00, 3444.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch: 10, Loss: 0.6487\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# list out training graphs\n",
    "train_graphs=[\n",
    "    # graph_9_22_h201,\n",
    "    graph_9_22_h402,\n",
    "    graph_9_22_h660,\n",
    "    graph_9_22_h501,\n",
    "    graph_9_22_h051,\n",
    "    graph_9_22_h209,\n",
    "]\n",
    "print(f\"{embedding_dim=}\")\n",
    "print(f\"{gnn=}\")\n",
    "# Train the model\n",
    "for epoch in tqdm(range(1, 11)):\n",
    "    for g in train_graphs:\n",
    "        loss = train(g)\n",
    "        print(f'  Epoch: {epoch:02d}, Loss: {loss:.4f}')\n",
    "\n",
    "\n",
    "memory.reset_state()  # Start with a fresxh memory.\n",
    "neighbor_loader.reset_state() \n",
    "model=[memory,gnn, link_pred,neighbor_loader]\n",
    "# Save the model\n",
    "torch.save(model,f\"/home/shahidul2k9/data/optc/model/model_saved_traindata=hosts_9_22.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the function for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time \n",
    "\n",
    "@torch.no_grad()\n",
    "def test_day_new(inference_data,path,nodeuuid2index):\n",
    "    if os.path.exists(path):\n",
    "        pass\n",
    "    else:\n",
    "        os.mkdir(path)\n",
    "    # Switch each layer from training to test modes\n",
    "    memory.eval()\n",
    "    gnn.eval()\n",
    "    link_pred.eval()\n",
    "    \n",
    "    memory.reset_state()  \n",
    "    neighbor_loader.reset_state()  \n",
    "    \n",
    "    time_with_loss={}\n",
    "    total_loss = 0    \n",
    "    edge_list=[]\n",
    "    \n",
    "    unique_nodes=torch.tensor([])\n",
    "    total_edges=0\n",
    "    \n",
    "\n",
    "\n",
    "    start_time=int(inference_data.t[0])\n",
    "    event_count=0\n",
    "    \n",
    "    pos_o=[]\n",
    "    \n",
    "    loss_list=[]\n",
    "\n",
    "\n",
    "    print(\"after merge:\",inference_data)\n",
    "    \n",
    "\n",
    "    start = time.perf_counter()\n",
    "    # Iterate over batches of validation dataset\n",
    "    for batch in inference_data.seq_batches(batch_size=BATCH):\n",
    "        \n",
    "        src, pos_dst, t, msg = batch.src, batch.dst, batch.t, batch.msg\n",
    "        unique_nodes=torch.cat([unique_nodes,src,pos_dst]).unique()\n",
    "        total_edges+=BATCH\n",
    "        \n",
    "       \n",
    "        n_id = torch.cat([src, pos_dst]).unique()       \n",
    "        n_id, edge_index, e_id = neighbor_loader(n_id)\n",
    "        assoc[n_id] = torch.arange(n_id.size(0), device=device)\n",
    "\n",
    "        z, last_update = memory(n_id)\n",
    "\n",
    "        # Pass the node representations and graph information through the Graph Neural Network (GNN)\n",
    "        z = gnn(z, last_update, edge_index, inference_data.t[e_id], inference_data.msg[e_id])\n",
    "\n",
    "        # Predict the likelihood of an edge existing between the source and positive destination nodes\n",
    "        pos_out = link_pred(z[assoc[src]], z[assoc[pos_dst]])\n",
    "        \n",
    "        pos_o.append(pos_out)\n",
    "        y_pred = torch.cat([pos_out], dim=0)\n",
    "\n",
    "        y_true=[]\n",
    "        for m in msg:\n",
    "            l=tensor_find(m[16:-16],1)-1\n",
    "            y_true.append(l) \n",
    "        y_true = torch.tensor(y_true)\n",
    "        y_true=y_true.reshape(-1).to(torch.long)\n",
    "        \n",
    "        # Compute the loss for positive link predictions\n",
    "        loss = criterion(y_pred, y_true)\n",
    "\n",
    "        # Accumulate the loss, weighted by the number of events in the batch\n",
    "        total_loss += float(loss) * batch.num_events\n",
    "     \n",
    "        # Update memory state\n",
    "        memory.update_state(src, pos_dst, t, msg)\n",
    "        # Insert the edge into the neighbor loader for future neighborhood sampling\n",
    "        neighbor_loader.insert(src, pos_dst)\n",
    "        \n",
    "\n",
    "        each_edge_loss= cal_pos_edges_loss_multiclass(pos_out,y_true)\n",
    "        \n",
    "        for i in range(len(pos_out)):\n",
    "            srcnode= int(src[i])\n",
    "            dstnode=  int(pos_dst[i])\n",
    "            \n",
    "            srcmsg=str(nodeuuid2index[srcnode]) \n",
    "            dstmsg=str(nodeuuid2index[dstnode])\n",
    "            t_var=int(t[i])\n",
    "            edgeindex=tensor_find(msg[i][16:-16],1) \n",
    "            edge_type=rel2id[edgeindex]\n",
    "            loss=each_edge_loss[i]    \n",
    "\n",
    "            temp_dic={}\n",
    "            temp_dic['loss']=float(loss)\n",
    "            temp_dic['srcnode']=srcnode\n",
    "            temp_dic['dstnode']=dstnode\n",
    "            temp_dic['srcmsg']=srcmsg\n",
    "            temp_dic['dstmsg']=dstmsg\n",
    "            temp_dic['edge_type']=edge_type\n",
    "            temp_dic['time']=t_var\n",
    "\n",
    "            edge_list.append(temp_dic)\n",
    "        \n",
    "        event_count+=len(batch.src)\n",
    "        if t[-1]>start_time+60000*15:\n",
    "\n",
    "            time_interval=timestamp_to_datetime_US(start_time)+\"~\"+timestamp_to_datetime_US(int(t[-1]))\n",
    "\n",
    "            end = time.perf_counter()\n",
    "            time_with_loss[time_interval]={'loss':loss,\n",
    "                                \n",
    "                                          'nodes_count':len(unique_nodes),\n",
    "                                          'total_edges':total_edges,\n",
    "                                          'costed_time':(end-start)}\n",
    "            \n",
    "            \n",
    "            log=open(path+\"/\"+time_interval+\".txt\",'w')\n",
    "\n",
    "            \n",
    "            for e in edge_list: \n",
    "                loss+=e['loss']\n",
    "\n",
    "            loss=loss/event_count   \n",
    "            print(f'Time: {time_interval}, Loss: {loss:.4f}, Nodes_count: {len(unique_nodes)}, Cost Time: {(end-start):.2f}s')\n",
    "            edge_list = sorted(edge_list, key=lambda x:x['loss'],reverse=True)  \n",
    "            for e in edge_list: \n",
    "                log.write(str(e))\n",
    "                log.write(\"\\n\") \n",
    "            event_count=0\n",
    "            total_loss=0\n",
    "            loss=0\n",
    "            start_time=t[-1]\n",
    "            log.close()\n",
    "            edge_list.clear()\n",
    "            \n",
    " \n",
    "    return time_with_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9-22 hosts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the the trained model\n",
    "model=torch.load(\"/home/shahidul2k9/data/optc/model/model_saved_traindata=hosts_9_22.pt\")\n",
    "memory,gnn, link_pred,neighbor_loader=model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO : investigate, authors didn't preprocessed this files why?\n",
    "# ans_9_22_h201=test_day_new(graph_9_22_h201,\"graph_9_22_h201\",node_uuid2index_9_22_h201)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after merge: TemporalData(dst=[4407209], msg=[4407209, 42], src=[4407209], t=[4407209])\n",
      "Time: 2019-09-22 00:00:00.8~2019-09-22 00:15:00.418, Loss: 0.8266, Nodes_count: 9470, Cost Time: 4.32s\n",
      "Time: 2019-09-22 00:15:00.tensor(418)~2019-09-22 00:30:42.861, Loss: 0.5816, Nodes_count: 15043, Cost Time: 6.76s\n",
      "Time: 2019-09-22 00:30:42.tensor(861)~2019-09-22 00:45:44.758, Loss: 0.9821, Nodes_count: 28193, Cost Time: 11.60s\n",
      "Time: 2019-09-22 00:45:44.tensor(758)~2019-09-22 01:01:20.483, Loss: 0.5469, Nodes_count: 33751, Cost Time: 17.36s\n",
      "Time: 2019-09-22 01:01:20.tensor(483)~2019-09-22 01:16:20.612, Loss: 0.7375, Nodes_count: 42300, Cost Time: 24.51s\n",
      "Time: 2019-09-22 01:16:20.tensor(612)~2019-09-22 01:32:01.859, Loss: 0.6598, Nodes_count: 47591, Cost Time: 27.52s\n",
      "Time: 2019-09-22 01:32:01.tensor(859)~2019-09-22 01:47:09.362, Loss: 0.6077, Nodes_count: 53575, Cost Time: 31.74s\n",
      "Time: 2019-09-22 01:47:09.tensor(362)~2019-09-22 02:02:34.719, Loss: 0.6372, Nodes_count: 60981, Cost Time: 38.22s\n",
      "Time: 2019-09-22 02:02:34.tensor(719)~2019-09-22 02:17:40.622, Loss: 0.5980, Nodes_count: 67203, Cost Time: 44.90s\n",
      "Time: 2019-09-22 02:17:40.tensor(622)~2019-09-22 02:32:56.239, Loss: 0.6410, Nodes_count: 72674, Cost Time: 49.09s\n",
      "Time: 2019-09-22 02:32:56.tensor(239)~2019-09-22 02:48:16.396, Loss: 0.6917, Nodes_count: 79947, Cost Time: 55.67s\n",
      "Time: 2019-09-22 02:48:16.tensor(396)~2019-09-22 03:03:33.266, Loss: 0.5557, Nodes_count: 84879, Cost Time: 60.49s\n",
      "Time: 2019-09-22 03:03:33.tensor(266)~2019-09-22 03:18:57.78, Loss: 0.6861, Nodes_count: 94006, Cost Time: 69.62s\n",
      "Time: 2019-09-22 03:18:57.tensor(78)~2019-09-22 03:34:04.444, Loss: 0.6055, Nodes_count: 98499, Cost Time: 73.48s\n",
      "Time: 2019-09-22 03:34:04.tensor(444)~2019-09-22 03:49:17.79, Loss: 0.7301, Nodes_count: 106178, Cost Time: 80.59s\n",
      "Time: 2019-09-22 03:49:17.tensor(79)~2019-09-22 04:04:28.575, Loss: 0.7146, Nodes_count: 114548, Cost Time: 88.25s\n",
      "Time: 2019-09-22 04:04:28.tensor(575)~2019-09-22 04:19:57.444, Loss: 0.5743, Nodes_count: 121255, Cost Time: 96.17s\n",
      "Time: 2019-09-22 04:19:57.tensor(444)~2019-09-22 04:35:06.964, Loss: 0.6425, Nodes_count: 127884, Cost Time: 100.92s\n",
      "Time: 2019-09-22 04:35:06.tensor(964)~2019-09-22 04:50:13.236, Loss: 0.7531, Nodes_count: 140117, Cost Time: 111.86s\n",
      "Time: 2019-09-22 04:50:13.tensor(236)~2019-09-22 05:05:23.88, Loss: 0.5688, Nodes_count: 145089, Cost Time: 119.12s\n",
      "Time: 2019-09-22 05:05:23.tensor(88)~2019-09-22 05:20:29.64, Loss: 0.7832, Nodes_count: 155085, Cost Time: 130.06s\n",
      "Time: 2019-09-22 05:20:29.tensor(64)~2019-09-22 05:36:01.913, Loss: 0.7349, Nodes_count: 160540, Cost Time: 135.32s\n",
      "Time: 2019-09-22 05:36:01.tensor(913)~2019-09-22 05:51:09.86, Loss: 0.5555, Nodes_count: 166801, Cost Time: 143.31s\n",
      "Time: 2019-09-22 05:51:09.tensor(86)~2019-09-22 06:06:15.217, Loss: 0.6847, Nodes_count: 174529, Cost Time: 155.01s\n",
      "Time: 2019-09-22 06:06:15.tensor(217)~2019-09-22 06:21:16.855, Loss: 0.6261, Nodes_count: 180807, Cost Time: 164.22s\n",
      "Time: 2019-09-22 06:21:16.tensor(855)~2019-09-22 06:36:20.577, Loss: 0.6008, Nodes_count: 185840, Cost Time: 169.04s\n",
      "Time: 2019-09-22 06:36:20.tensor(577)~2019-09-22 06:51:43.486, Loss: 0.7195, Nodes_count: 195138, Cost Time: 180.24s\n",
      "Time: 2019-09-22 06:51:43.tensor(486)~2019-09-22 07:06:52.865, Loss: 0.8208, Nodes_count: 204234, Cost Time: 193.60s\n",
      "Time: 2019-09-22 07:06:52.tensor(865)~2019-09-22 07:23:00.897, Loss: 0.5437, Nodes_count: 210931, Cost Time: 203.14s\n",
      "Time: 2019-09-22 07:23:00.tensor(897)~2019-09-22 07:38:03.494, Loss: 0.7885, Nodes_count: 219557, Cost Time: 212.86s\n",
      "Time: 2019-09-22 07:38:03.tensor(494)~2019-09-22 07:53:09.845, Loss: 0.5975, Nodes_count: 225200, Cost Time: 222.83s\n",
      "Time: 2019-09-22 07:53:09.tensor(845)~2019-09-22 08:08:17.972, Loss: 0.6504, Nodes_count: 232376, Cost Time: 238.59s\n",
      "Time: 2019-09-22 08:08:17.tensor(972)~2019-09-22 08:23:33.632, Loss: 0.6752, Nodes_count: 239134, Cost Time: 249.58s\n",
      "Time: 2019-09-22 08:23:33.tensor(632)~2019-09-22 08:39:04.848, Loss: 0.6079, Nodes_count: 245084, Cost Time: 256.22s\n",
      "Time: 2019-09-22 08:39:04.tensor(848)~2019-09-22 08:54:16.919, Loss: 0.7282, Nodes_count: 252659, Cost Time: 270.21s\n",
      "Time: 2019-09-22 08:54:16.tensor(919)~2019-09-22 09:09:33.706, Loss: 0.6403, Nodes_count: 258540, Cost Time: 285.05s\n",
      "Time: 2019-09-22 09:09:33.tensor(706)~2019-09-22 09:24:47.805, Loss: 0.7517, Nodes_count: 268173, Cost Time: 298.58s\n",
      "Time: 2019-09-22 09:24:47.tensor(805)~2019-09-22 09:39:59.894, Loss: 0.9096, Nodes_count: 277066, Cost Time: 309.38s\n",
      "Time: 2019-09-22 09:39:59.tensor(894)~2019-09-22 09:55:03.821, Loss: 0.5692, Nodes_count: 282218, Cost Time: 321.45s\n",
      "Time: 2019-09-22 09:55:03.tensor(821)~2019-09-22 10:10:27.192, Loss: 0.7130, Nodes_count: 291096, Cost Time: 340.73s\n",
      "Time: 2019-09-22 10:10:27.tensor(192)~2019-09-22 10:25:44.243, Loss: 0.6220, Nodes_count: 299943, Cost Time: 354.94s\n",
      "Time: 2019-09-22 10:25:44.tensor(243)~2019-09-22 10:40:46.89, Loss: 0.8539, Nodes_count: 310233, Cost Time: 369.01s\n",
      "Time: 2019-09-22 10:40:46.tensor(89)~2019-09-22 10:55:49.38, Loss: 0.6067, Nodes_count: 315218, Cost Time: 383.72s\n",
      "Time: 2019-09-22 10:55:49.tensor(38)~2019-09-22 11:11:05.292, Loss: 0.6336, Nodes_count: 322052, Cost Time: 397.93s\n",
      "Time: 2019-09-22 11:11:05.tensor(292)~2019-09-22 11:26:13.934, Loss: 0.6664, Nodes_count: 328950, Cost Time: 412.69s\n",
      "Time: 2019-09-22 11:26:13.tensor(934)~2019-09-22 11:42:01.35, Loss: 1.1123, Nodes_count: 340105, Cost Time: 429.37s\n",
      "Time: 2019-09-22 11:42:01.tensor(35)~2019-09-22 11:57:17.383, Loss: 0.7086, Nodes_count: 348597, Cost Time: 451.39s\n",
      "Time: 2019-09-22 11:57:17.tensor(383)~2019-09-22 12:12:22.301, Loss: 0.6656, Nodes_count: 355941, Cost Time: 470.57s\n",
      "Time: 2019-09-22 12:12:22.tensor(301)~2019-09-22 12:27:30.479, Loss: 0.5972, Nodes_count: 360539, Cost Time: 480.74s\n",
      "Time: 2019-09-22 12:27:30.tensor(479)~2019-09-22 12:42:50.204, Loss: 0.7947, Nodes_count: 369611, Cost Time: 496.90s\n",
      "Time: 2019-09-22 12:42:50.tensor(204)~2019-09-22 12:57:51.903, Loss: 0.6764, Nodes_count: 376187, Cost Time: 517.19s\n",
      "Time: 2019-09-22 12:57:51.tensor(903)~2019-09-22 13:12:56.22, Loss: 0.6350, Nodes_count: 383206, Cost Time: 539.14s\n",
      "Time: 2019-09-22 13:12:56.tensor(22)~2019-09-22 13:27:56.109, Loss: 0.9339, Nodes_count: 391547, Cost Time: 553.30s\n",
      "Time: 2019-09-22 13:27:56.tensor(109)~2019-09-22 13:43:02.641, Loss: 0.7912, Nodes_count: 400598, Cost Time: 566.86s\n",
      "Time: 2019-09-22 13:43:02.tensor(641)~2019-09-22 13:58:40.823, Loss: 0.5775, Nodes_count: 405899, Cost Time: 587.67s\n",
      "Time: 2019-09-22 13:58:40.tensor(823)~2019-09-22 14:14:18.386, Loss: 0.7029, Nodes_count: 416592, Cost Time: 616.39s\n",
      "Time: 2019-09-22 14:14:18.tensor(386)~2019-09-22 14:29:47.639, Loss: 0.5846, Nodes_count: 421433, Cost Time: 627.25s\n",
      "Time: 2019-09-22 14:29:47.tensor(639)~2019-09-22 14:44:52.207, Loss: 0.8014, Nodes_count: 430228, Cost Time: 645.29s\n",
      "Time: 2019-09-22 14:44:52.tensor(207)~2019-09-22 14:59:52.974, Loss: 0.6136, Nodes_count: 435485, Cost Time: 666.85s\n",
      "Time: 2019-09-22 14:59:52.tensor(974)~2019-09-22 15:14:58.173, Loss: 0.5505, Nodes_count: 441207, Cost Time: 686.98s\n",
      "Time: 2019-09-22 15:14:58.tensor(173)~2019-09-22 15:30:17.765, Loss: 0.7910, Nodes_count: 449274, Cost Time: 705.85s\n",
      "Time: 2019-09-22 15:30:17.tensor(765)~2019-09-22 15:45:19.162, Loss: 0.6094, Nodes_count: 455702, Cost Time: 719.69s\n",
      "Time: 2019-09-22 15:45:19.tensor(162)~2019-09-22 16:00:43.633, Loss: 0.6892, Nodes_count: 463048, Cost Time: 746.03s\n",
      "Time: 2019-09-22 16:00:43.tensor(633)~2019-09-22 16:16:09.718, Loss: 0.5945, Nodes_count: 469342, Cost Time: 770.29s\n",
      "Time: 2019-09-22 16:16:09.tensor(718)~2019-09-22 16:31:26.721, Loss: 0.7856, Nodes_count: 476737, Cost Time: 788.16s\n",
      "Time: 2019-09-22 16:31:26.tensor(721)~2019-09-22 16:46:30.859, Loss: 0.6857, Nodes_count: 486771, Cost Time: 811.92s\n",
      "Time: 2019-09-22 16:46:30.tensor(859)~2019-09-22 17:01:36.381, Loss: 0.6743, Nodes_count: 494486, Cost Time: 839.96s\n",
      "Time: 2019-09-22 17:01:36.tensor(381)~2019-09-22 17:17:02.779, Loss: 0.6171, Nodes_count: 501557, Cost Time: 868.72s\n",
      "Time: 2019-09-22 17:17:02.tensor(779)~2019-09-22 17:32:11.39, Loss: 0.6281, Nodes_count: 506736, Cost Time: 881.02s\n",
      "Time: 2019-09-22 17:32:11.tensor(39)~2019-09-22 17:47:14.127, Loss: 0.7331, Nodes_count: 514130, Cost Time: 903.96s\n",
      "Time: 2019-09-22 17:47:14.tensor(127)~2019-09-22 18:02:32.548, Loss: 0.6120, Nodes_count: 520734, Cost Time: 926.76s\n",
      "Time: 2019-09-22 18:02:32.tensor(548)~2019-09-22 18:18:08.547, Loss: 0.5566, Nodes_count: 526655, Cost Time: 952.78s\n",
      "Time: 2019-09-22 18:18:08.tensor(547)~2019-09-22 18:33:33.7, Loss: 0.8014, Nodes_count: 535611, Cost Time: 975.97s\n",
      "Time: 2019-09-22 18:33:33.tensor(7)~2019-09-22 18:49:03.623, Loss: 0.5686, Nodes_count: 540869, Cost Time: 995.29s\n",
      "Time: 2019-09-22 18:49:03.tensor(623)~2019-09-22 19:04:11.206, Loss: 0.8226, Nodes_count: 550257, Cost Time: 1024.83s\n",
      "Time: 2019-09-22 19:04:11.tensor(206)~2019-09-22 19:19:29.589, Loss: 0.6182, Nodes_count: 556414, Cost Time: 1052.23s\n",
      "Time: 2019-09-22 19:19:29.tensor(589)~2019-09-22 19:34:35.597, Loss: 0.8885, Nodes_count: 565307, Cost Time: 1076.34s\n",
      "Time: 2019-09-22 19:34:35.tensor(597)~2019-09-22 19:50:02.972, Loss: 0.6175, Nodes_count: 571622, Cost Time: 1098.97s\n",
      "Time: 2019-09-22 19:50:02.tensor(972)~2019-09-22 20:05:03.471, Loss: 0.6596, Nodes_count: 579112, Cost Time: 1125.63s\n",
      "Time: 2019-09-22 20:05:03.tensor(471)~2019-09-22 20:20:08.426, Loss: 0.6939, Nodes_count: 586630, Cost Time: 1158.38s\n",
      "Time: 2019-09-22 20:20:08.tensor(426)~2019-09-22 20:35:56.285, Loss: 0.6060, Nodes_count: 593207, Cost Time: 1175.75s\n",
      "Time: 2019-09-22 20:35:56.tensor(285)~2019-09-22 20:51:21.953, Loss: 0.7993, Nodes_count: 602833, Cost Time: 1207.30s\n",
      "Time: 2019-09-22 20:51:21.tensor(953)~2019-09-22 21:06:36.74, Loss: 0.5745, Nodes_count: 608599, Cost Time: 1233.83s\n",
      "Time: 2019-09-22 21:06:36.tensor(74)~2019-09-22 21:21:42.534, Loss: 0.6838, Nodes_count: 615559, Cost Time: 1264.21s\n",
      "Time: 2019-09-22 21:21:42.tensor(534)~2019-09-22 21:37:03.31, Loss: 0.5955, Nodes_count: 621477, Cost Time: 1280.01s\n",
      "Time: 2019-09-22 21:37:03.tensor(31)~2019-09-22 21:52:14.373, Loss: 0.7404, Nodes_count: 629408, Cost Time: 1312.75s\n",
      "Time: 2019-09-22 21:52:14.tensor(373)~2019-09-22 22:07:16.53, Loss: 0.7448, Nodes_count: 639965, Cost Time: 1348.54s\n",
      "Time: 2019-09-22 22:07:16.tensor(53)~2019-09-22 22:22:21.402, Loss: 0.5670, Nodes_count: 648292, Cost Time: 1378.35s\n",
      "Time: 2019-09-22 22:22:21.tensor(402)~2019-09-22 22:37:25.589, Loss: 0.8385, Nodes_count: 656833, Cost Time: 1408.01s\n",
      "Time: 2019-09-22 22:37:25.tensor(589)~2019-09-22 22:52:36.462, Loss: 0.6240, Nodes_count: 662634, Cost Time: 1433.36s\n",
      "Time: 2019-09-22 22:52:36.tensor(462)~2019-09-22 23:08:01.687, Loss: 0.6080, Nodes_count: 668112, Cost Time: 1464.02s\n",
      "Time: 2019-09-22 23:08:01.tensor(687)~2019-09-22 23:23:27.219, Loss: 0.6819, Nodes_count: 675880, Cost Time: 1495.43s\n",
      "Time: 2019-09-22 23:23:27.tensor(219)~2019-09-22 23:38:46.687, Loss: 0.6992, Nodes_count: 682257, Cost Time: 1513.43s\n",
      "Time: 2019-09-22 23:38:46.tensor(687)~2019-09-22 23:54:17.53, Loss: 0.7257, Nodes_count: 689970, Cost Time: 1547.16s\n"
     ]
    }
   ],
   "source": [
    "ans_9_22_h402=test_day_new(graph_9_22_h402,\"/home/shahidul2k9/data/optc/test/graph_9_22_h402\",node_uuid2index_9_22_h402)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after merge: TemporalData(dst=[3886983], msg=[3886983, 42], src=[3886983], t=[3886983])\n",
      "Time: 2019-09-22 00:00:00.61~2019-09-22 00:15:05.35, Loss: 0.8379, Nodes_count: 8706, Cost Time: 3.96s\n",
      "Time: 2019-09-22 00:15:05.tensor(35)~2019-09-22 00:30:36.758, Loss: 0.6762, Nodes_count: 14202, Cost Time: 6.29s\n",
      "Time: 2019-09-22 00:30:36.tensor(758)~2019-09-22 00:45:45.546, Loss: 0.8503, Nodes_count: 23391, Cost Time: 10.28s\n",
      "Time: 2019-09-22 00:45:45.tensor(546)~2019-09-22 01:01:09.871, Loss: 0.6899, Nodes_count: 28969, Cost Time: 15.44s\n",
      "Time: 2019-09-22 01:01:09.tensor(871)~2019-09-22 01:16:25.366, Loss: 0.6787, Nodes_count: 35428, Cost Time: 21.09s\n",
      "Time: 2019-09-22 01:16:25.tensor(366)~2019-09-22 01:32:01.901, Loss: 0.6835, Nodes_count: 40339, Cost Time: 23.69s\n",
      "Time: 2019-09-22 01:32:01.tensor(901)~2019-09-22 01:47:03.66, Loss: 0.8264, Nodes_count: 48623, Cost Time: 28.42s\n",
      "Time: 2019-09-22 01:47:03.tensor(66)~2019-09-22 02:02:20.609, Loss: 0.7055, Nodes_count: 54441, Cost Time: 33.45s\n",
      "Time: 2019-09-22 02:02:20.tensor(609)~2019-09-22 02:17:29.185, Loss: 0.6606, Nodes_count: 60198, Cost Time: 39.44s\n",
      "Time: 2019-09-22 02:17:29.tensor(185)~2019-09-22 02:32:36.402, Loss: 0.7280, Nodes_count: 65304, Cost Time: 42.51s\n",
      "Time: 2019-09-22 02:32:36.tensor(402)~2019-09-22 02:47:59.136, Loss: 0.6765, Nodes_count: 71234, Cost Time: 47.18s\n",
      "Time: 2019-09-22 02:47:59.tensor(136)~2019-09-22 03:03:02.556, Loss: 0.6501, Nodes_count: 76184, Cost Time: 51.66s\n",
      "Time: 2019-09-22 03:03:02.tensor(556)~2019-09-22 03:18:27.373, Loss: 0.6491, Nodes_count: 82581, Cost Time: 58.33s\n",
      "Time: 2019-09-22 03:18:27.tensor(373)~2019-09-22 03:34:25.232, Loss: 0.7181, Nodes_count: 87943, Cost Time: 61.73s\n",
      "Time: 2019-09-22 03:34:25.tensor(232)~2019-09-22 03:49:32.728, Loss: 0.6828, Nodes_count: 93755, Cost Time: 67.34s\n",
      "Time: 2019-09-22 03:49:32.tensor(728)~2019-09-22 04:04:45.198, Loss: 0.7966, Nodes_count: 101452, Cost Time: 74.12s\n",
      "Time: 2019-09-22 04:04:45.tensor(198)~2019-09-22 04:20:10.637, Loss: 0.6439, Nodes_count: 107759, Cost Time: 81.26s\n",
      "Time: 2019-09-22 04:20:10.tensor(637)~2019-09-22 04:35:13.566, Loss: 0.7044, Nodes_count: 114073, Cost Time: 85.39s\n",
      "Time: 2019-09-22 04:35:13.tensor(566)~2019-09-22 04:50:34.954, Loss: 0.6700, Nodes_count: 123886, Cost Time: 92.40s\n",
      "Time: 2019-09-22 04:50:34.tensor(954)~2019-09-22 05:05:52.664, Loss: 0.6437, Nodes_count: 128685, Cost Time: 98.92s\n",
      "Time: 2019-09-22 05:05:52.tensor(664)~2019-09-22 05:21:19.769, Loss: 0.6467, Nodes_count: 134231, Cost Time: 106.66s\n",
      "Time: 2019-09-22 05:21:19.tensor(769)~2019-09-22 05:36:32.966, Loss: 0.9087, Nodes_count: 141227, Cost Time: 111.05s\n",
      "Time: 2019-09-22 05:36:32.tensor(966)~2019-09-22 05:51:48.4, Loss: 0.7004, Nodes_count: 147493, Cost Time: 117.45s\n",
      "Time: 2019-09-22 05:51:48.tensor(4)~2019-09-22 06:06:52.814, Loss: 0.6978, Nodes_count: 152908, Cost Time: 126.38s\n",
      "Time: 2019-09-22 06:06:52.tensor(814)~2019-09-22 06:22:38.365, Loss: 0.6581, Nodes_count: 158990, Cost Time: 133.35s\n",
      "Time: 2019-09-22 06:22:38.tensor(365)~2019-09-22 06:38:04.146, Loss: 0.6864, Nodes_count: 164583, Cost Time: 137.35s\n",
      "Time: 2019-09-22 06:38:04.tensor(146)~2019-09-22 06:53:38.233, Loss: 0.6974, Nodes_count: 171175, Cost Time: 144.77s\n",
      "Time: 2019-09-22 06:53:38.tensor(233)~2019-09-22 07:08:50.49, Loss: 0.9347, Nodes_count: 182455, Cost Time: 158.03s\n",
      "Time: 2019-09-22 07:08:50.tensor(49)~2019-09-22 07:23:51.378, Loss: 0.6454, Nodes_count: 189007, Cost Time: 166.60s\n",
      "Time: 2019-09-22 07:23:51.tensor(378)~2019-09-22 07:39:02.29, Loss: 0.8489, Nodes_count: 197005, Cost Time: 172.58s\n",
      "Time: 2019-09-22 07:39:02.tensor(29)~2019-09-22 07:54:09.732, Loss: 0.6717, Nodes_count: 202189, Cost Time: 180.49s\n",
      "Time: 2019-09-22 07:54:09.tensor(732)~2019-09-22 08:09:34.165, Loss: 0.6865, Nodes_count: 207887, Cost Time: 191.50s\n",
      "Time: 2019-09-22 08:09:34.tensor(165)~2019-09-22 08:24:56.743, Loss: 0.6451, Nodes_count: 213561, Cost Time: 199.36s\n",
      "Time: 2019-09-22 08:24:56.tensor(743)~2019-09-22 08:40:09.84, Loss: 0.7258, Nodes_count: 220336, Cost Time: 205.35s\n",
      "Time: 2019-09-22 08:40:09.tensor(84)~2019-09-22 08:55:18.165, Loss: 0.6576, Nodes_count: 225156, Cost Time: 215.13s\n",
      "Time: 2019-09-22 08:55:18.tensor(165)~2019-09-22 09:10:39.211, Loss: 0.6996, Nodes_count: 231338, Cost Time: 225.87s\n",
      "Time: 2019-09-22 09:10:39.tensor(211)~2019-09-22 09:26:04.15, Loss: 0.6161, Nodes_count: 236763, Cost Time: 234.11s\n",
      "Time: 2019-09-22 09:26:04.tensor(15)~2019-09-22 09:41:45.437, Loss: 0.6942, Nodes_count: 243236, Cost Time: 241.66s\n",
      "Time: 2019-09-22 09:41:45.tensor(437)~2019-09-22 09:57:05.185, Loss: 0.6996, Nodes_count: 248318, Cost Time: 254.30s\n",
      "Time: 2019-09-22 09:57:05.tensor(185)~2019-09-22 10:12:09.826, Loss: 0.7863, Nodes_count: 258150, Cost Time: 269.56s\n",
      "Time: 2019-09-22 10:12:09.tensor(826)~2019-09-22 10:27:17.31, Loss: 0.6557, Nodes_count: 265287, Cost Time: 279.22s\n",
      "Time: 2019-09-22 10:27:17.tensor(31)~2019-09-22 10:43:01.4, Loss: 0.8418, Nodes_count: 274045, Cost Time: 288.04s\n",
      "Time: 2019-09-22 10:43:01.tensor(4)~2019-09-22 10:58:03.191, Loss: 0.6950, Nodes_count: 278505, Cost Time: 301.33s\n",
      "Time: 2019-09-22 10:58:03.tensor(191)~2019-09-22 11:13:35.723, Loss: 0.6879, Nodes_count: 285950, Cost Time: 315.91s\n",
      "Time: 2019-09-22 11:13:35.tensor(723)~2019-09-22 11:28:52.633, Loss: 0.6982, Nodes_count: 290693, Cost Time: 322.87s\n",
      "Time: 2019-09-22 11:28:52.tensor(633)~2019-09-22 11:44:02.777, Loss: 0.9094, Nodes_count: 299239, Cost Time: 331.69s\n",
      "Time: 2019-09-22 11:44:02.tensor(777)~2019-09-22 11:59:29.704, Loss: 0.8668, Nodes_count: 308726, Cost Time: 349.44s\n",
      "Time: 2019-09-22 11:59:29.tensor(704)~2019-09-22 12:14:33.269, Loss: 0.6683, Nodes_count: 315672, Cost Time: 364.30s\n",
      "Time: 2019-09-22 12:14:33.tensor(269)~2019-09-22 12:29:42.48, Loss: 0.7785, Nodes_count: 320770, Cost Time: 372.66s\n",
      "Time: 2019-09-22 12:29:42.tensor(48)~2019-09-22 12:44:53.934, Loss: 0.6663, Nodes_count: 327513, Cost Time: 381.84s\n",
      "Time: 2019-09-22 12:44:53.tensor(934)~2019-09-22 13:00:12.454, Loss: 0.6830, Nodes_count: 332208, Cost Time: 397.67s\n",
      "Time: 2019-09-22 13:00:12.tensor(454)~2019-09-22 13:15:37.564, Loss: 0.7415, Nodes_count: 340258, Cost Time: 418.14s\n",
      "Time: 2019-09-22 13:15:37.tensor(564)~2019-09-22 13:31:03.219, Loss: 0.7263, Nodes_count: 345135, Cost Time: 427.24s\n",
      "Time: 2019-09-22 13:31:03.tensor(219)~2019-09-22 13:46:08.374, Loss: 0.7131, Nodes_count: 351121, Cost Time: 439.15s\n",
      "Time: 2019-09-22 13:46:08.tensor(374)~2019-09-22 14:01:25.262, Loss: 0.7547, Nodes_count: 356694, Cost Time: 457.15s\n",
      "Time: 2019-09-22 14:01:25.tensor(262)~2019-09-22 14:16:36.688, Loss: 0.6329, Nodes_count: 362500, Cost Time: 475.11s\n",
      "Time: 2019-09-22 14:16:36.tensor(688)~2019-09-22 14:31:55.495, Loss: 0.7417, Nodes_count: 367901, Cost Time: 486.47s\n",
      "Time: 2019-09-22 14:31:55.tensor(495)~2019-09-22 14:47:02.672, Loss: 0.7297, Nodes_count: 373761, Cost Time: 500.20s\n",
      "Time: 2019-09-22 14:47:02.tensor(672)~2019-09-22 15:02:23.99, Loss: 0.6863, Nodes_count: 379243, Cost Time: 516.33s\n",
      "Time: 2019-09-22 15:02:23.tensor(99)~2019-09-22 15:18:15.283, Loss: 0.6650, Nodes_count: 384662, Cost Time: 536.04s\n",
      "Time: 2019-09-22 15:18:15.tensor(283)~2019-09-22 15:33:17.679, Loss: 0.6649, Nodes_count: 390214, Cost Time: 547.42s\n",
      "Time: 2019-09-22 15:33:17.tensor(679)~2019-09-22 15:48:56.696, Loss: 0.6837, Nodes_count: 396298, Cost Time: 562.59s\n",
      "Time: 2019-09-22 15:48:56.tensor(696)~2019-09-22 16:04:04.769, Loss: 0.6459, Nodes_count: 401997, Cost Time: 577.45s\n",
      "Time: 2019-09-22 16:04:04.tensor(769)~2019-09-22 16:19:47.644, Loss: 0.6988, Nodes_count: 407793, Cost Time: 597.58s\n",
      "Time: 2019-09-22 16:19:47.tensor(644)~2019-09-22 16:34:53.4, Loss: 0.8450, Nodes_count: 415395, Cost Time: 613.80s\n",
      "Time: 2019-09-22 16:34:53.tensor(4)~2019-09-22 16:50:03.157, Loss: 0.7100, Nodes_count: 424964, Cost Time: 635.84s\n",
      "Time: 2019-09-22 16:50:03.tensor(157)~2019-09-22 17:05:07.192, Loss: 0.6518, Nodes_count: 430766, Cost Time: 653.21s\n",
      "Time: 2019-09-22 17:05:07.tensor(192)~2019-09-22 17:20:28.58, Loss: 0.6629, Nodes_count: 436648, Cost Time: 674.74s\n",
      "Time: 2019-09-22 17:20:28.tensor(58)~2019-09-22 17:35:28.387, Loss: 0.6680, Nodes_count: 441628, Cost Time: 687.27s\n",
      "Time: 2019-09-22 17:35:28.tensor(387)~2019-09-22 17:50:58.905, Loss: 0.6983, Nodes_count: 447139, Cost Time: 704.09s\n",
      "Time: 2019-09-22 17:50:58.tensor(905)~2019-09-22 18:06:02.314, Loss: 0.6990, Nodes_count: 454485, Cost Time: 726.50s\n",
      "Time: 2019-09-22 18:06:02.tensor(314)~2019-09-22 18:21:09.516, Loss: 0.6649, Nodes_count: 460269, Cost Time: 747.58s\n",
      "Time: 2019-09-22 18:21:09.tensor(516)~2019-09-22 18:36:49.93, Loss: 0.6504, Nodes_count: 465849, Cost Time: 760.15s\n",
      "Time: 2019-09-22 18:36:49.tensor(93)~2019-09-22 18:52:01.281, Loss: 0.6763, Nodes_count: 471697, Cost Time: 778.75s\n",
      "Time: 2019-09-22 18:52:01.tensor(281)~2019-09-22 19:07:07.51, Loss: 0.6587, Nodes_count: 476569, Cost Time: 802.12s\n",
      "Time: 2019-09-22 19:07:07.tensor(51)~2019-09-22 19:22:09.294, Loss: 0.7998, Nodes_count: 484793, Cost Time: 823.29s\n",
      "Time: 2019-09-22 19:22:09.tensor(294)~2019-09-22 19:37:51.357, Loss: 0.7528, Nodes_count: 489541, Cost Time: 834.20s\n",
      "Time: 2019-09-22 19:37:51.tensor(357)~2019-09-22 19:53:02.368, Loss: 0.8709, Nodes_count: 497946, Cost Time: 862.47s\n",
      "Time: 2019-09-22 19:53:02.tensor(368)~2019-09-22 20:08:24.799, Loss: 0.6758, Nodes_count: 503813, Cost Time: 888.42s\n",
      "Time: 2019-09-22 20:08:24.tensor(799)~2019-09-22 20:24:32.133, Loss: 0.6322, Nodes_count: 510188, Cost Time: 909.06s\n",
      "Time: 2019-09-22 20:24:32.tensor(133)~2019-09-22 20:39:57.135, Loss: 0.6825, Nodes_count: 516479, Cost Time: 924.91s\n",
      "Time: 2019-09-22 20:39:57.tensor(135)~2019-09-22 20:55:05.651, Loss: 0.6660, Nodes_count: 521697, Cost Time: 947.41s\n",
      "Time: 2019-09-22 20:55:05.tensor(651)~2019-09-22 21:11:00.679, Loss: 0.6602, Nodes_count: 527211, Cost Time: 971.80s\n",
      "Time: 2019-09-22 21:11:00.tensor(679)~2019-09-22 21:26:33.33, Loss: 0.6292, Nodes_count: 532374, Cost Time: 989.91s\n",
      "Time: 2019-09-22 21:26:33.tensor(33)~2019-09-22 21:41:58.534, Loss: 0.7404, Nodes_count: 538700, Cost Time: 1005.52s\n",
      "Time: 2019-09-22 21:41:58.tensor(534)~2019-09-22 21:57:02.809, Loss: 0.6871, Nodes_count: 544098, Cost Time: 1033.57s\n",
      "Time: 2019-09-22 21:57:02.tensor(809)~2019-09-22 22:12:03.215, Loss: 0.6678, Nodes_count: 551637, Cost Time: 1059.37s\n",
      "Time: 2019-09-22 22:12:03.tensor(215)~2019-09-22 22:27:42.799, Loss: 0.6546, Nodes_count: 558937, Cost Time: 1081.20s\n",
      "Time: 2019-09-22 22:27:42.tensor(799)~2019-09-22 22:42:49.331, Loss: 0.7041, Nodes_count: 565217, Cost Time: 1097.35s\n",
      "Time: 2019-09-22 22:42:49.tensor(331)~2019-09-22 22:59:02.482, Loss: 0.6687, Nodes_count: 570015, Cost Time: 1125.75s\n",
      "Time: 2019-09-22 22:59:02.tensor(482)~2019-09-22 23:14:02.654, Loss: 0.8829, Nodes_count: 580285, Cost Time: 1164.41s\n",
      "Time: 2019-09-22 23:14:02.tensor(654)~2019-09-22 23:30:08.203, Loss: 0.6764, Nodes_count: 585712, Cost Time: 1180.90s\n",
      "Time: 2019-09-22 23:30:08.tensor(203)~2019-09-22 23:45:17.699, Loss: 0.6881, Nodes_count: 591888, Cost Time: 1197.71s\n"
     ]
    }
   ],
   "source": [
    "ans_9_22_h660=test_day_new(graph_9_22_h660,\"/home/shahidul2k9/data/optc/test/graph_9_22_h660\",node_uuid2index_9_22_h660)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after merge: TemporalData(dst=[4237267], msg=[4237267, 42], src=[4237267], t=[4237267])\n",
      "Time: 2019-09-22 00:00:00.24~2019-09-22 00:15:04.496, Loss: 0.8670, Nodes_count: 12073, Cost Time: 5.52s\n",
      "Time: 2019-09-22 00:15:04.tensor(496)~2019-09-22 00:30:16.369, Loss: 1.2992, Nodes_count: 29016, Cost Time: 11.21s\n",
      "Time: 2019-09-22 00:30:16.tensor(369)~2019-09-22 00:45:17.879, Loss: 0.6959, Nodes_count: 35650, Cost Time: 14.53s\n",
      "Time: 2019-09-22 00:45:17.tensor(879)~2019-09-22 01:00:47.878, Loss: 0.5800, Nodes_count: 41287, Cost Time: 20.20s\n",
      "Time: 2019-09-22 01:00:47.tensor(878)~2019-09-22 01:15:52.8, Loss: 0.9626, Nodes_count: 54465, Cost Time: 27.72s\n",
      "Time: 2019-09-22 01:15:52.tensor(8)~2019-09-22 01:31:49.443, Loss: 0.6243, Nodes_count: 59407, Cost Time: 30.91s\n",
      "Time: 2019-09-22 01:31:49.tensor(443)~2019-09-22 01:46:51.671, Loss: 0.6761, Nodes_count: 65495, Cost Time: 35.31s\n",
      "Time: 2019-09-22 01:46:51.tensor(671)~2019-09-22 02:02:00.604, Loss: 0.6505, Nodes_count: 71595, Cost Time: 41.61s\n",
      "Time: 2019-09-22 02:02:00.tensor(604)~2019-09-22 02:17:02.361, Loss: 0.6524, Nodes_count: 78079, Cost Time: 49.78s\n",
      "Time: 2019-09-22 02:17:02.tensor(361)~2019-09-22 02:32:02.582, Loss: 0.6470, Nodes_count: 83262, Cost Time: 53.76s\n",
      "Time: 2019-09-22 02:32:02.tensor(582)~2019-09-22 02:47:02.586, Loss: 0.6357, Nodes_count: 88752, Cost Time: 58.33s\n",
      "Time: 2019-09-22 02:47:02.tensor(586)~2019-09-22 03:02:22.437, Loss: 0.5830, Nodes_count: 94126, Cost Time: 64.49s\n",
      "Time: 2019-09-22 03:02:22.tensor(437)~2019-09-22 03:17:29.833, Loss: 0.5804, Nodes_count: 99945, Cost Time: 72.04s\n",
      "Time: 2019-09-22 03:17:29.tensor(833)~2019-09-22 03:32:40.765, Loss: 0.7620, Nodes_count: 106702, Cost Time: 77.26s\n",
      "Time: 2019-09-22 03:32:40.tensor(765)~2019-09-22 03:47:45.237, Loss: 0.6720, Nodes_count: 112747, Cost Time: 84.34s\n",
      "Time: 2019-09-22 03:47:45.tensor(237)~2019-09-22 04:03:05.203, Loss: 0.6153, Nodes_count: 119134, Cost Time: 90.63s\n",
      "Time: 2019-09-22 04:03:05.tensor(203)~2019-09-22 04:18:37.243, Loss: 0.6882, Nodes_count: 127713, Cost Time: 100.73s\n",
      "Time: 2019-09-22 04:18:37.tensor(243)~2019-09-22 04:33:54.255, Loss: 0.6454, Nodes_count: 133030, Cost Time: 105.25s\n",
      "Time: 2019-09-22 04:33:54.tensor(255)~2019-09-22 04:49:12.52, Loss: 0.6045, Nodes_count: 143615, Cost Time: 114.00s\n",
      "Time: 2019-09-22 04:49:12.tensor(52)~2019-09-22 05:04:28.383, Loss: 0.5971, Nodes_count: 148931, Cost Time: 120.93s\n",
      "Time: 2019-09-22 05:04:28.tensor(383)~2019-09-22 05:19:56.88, Loss: 0.7314, Nodes_count: 156790, Cost Time: 132.45s\n",
      "Time: 2019-09-22 05:19:56.tensor(88)~2019-09-22 05:35:19.49, Loss: 0.7358, Nodes_count: 161326, Cost Time: 137.11s\n",
      "Time: 2019-09-22 05:35:19.tensor(49)~2019-09-22 05:50:54.997, Loss: 0.6238, Nodes_count: 168328, Cost Time: 145.06s\n",
      "Time: 2019-09-22 05:50:54.tensor(997)~2019-09-22 06:05:56.6, Loss: 0.7111, Nodes_count: 174756, Cost Time: 157.18s\n",
      "Time: 2019-09-22 06:05:56.tensor(6)~2019-09-22 06:20:58.225, Loss: 0.8068, Nodes_count: 183574, Cost Time: 168.45s\n",
      "Time: 2019-09-22 06:20:58.tensor(225)~2019-09-22 06:36:04.241, Loss: 0.8986, Nodes_count: 191741, Cost Time: 176.96s\n",
      "Time: 2019-09-22 06:36:04.tensor(241)~2019-09-22 06:51:50.961, Loss: 0.6480, Nodes_count: 198798, Cost Time: 185.69s\n",
      "Time: 2019-09-22 06:51:50.tensor(961)~2019-09-22 07:06:53.2, Loss: 0.6093, Nodes_count: 204588, Cost Time: 196.59s\n",
      "Time: 2019-09-22 07:06:53.tensor(2)~2019-09-22 07:22:42.452, Loss: 0.5756, Nodes_count: 211427, Cost Time: 206.59s\n",
      "Time: 2019-09-22 07:22:42.tensor(452)~2019-09-22 07:37:49.387, Loss: 0.6348, Nodes_count: 217243, Cost Time: 212.28s\n",
      "Time: 2019-09-22 07:37:49.tensor(387)~2019-09-22 07:53:01.646, Loss: 0.5882, Nodes_count: 222920, Cost Time: 221.55s\n",
      "Time: 2019-09-22 07:53:01.tensor(646)~2019-09-22 08:08:07.609, Loss: 0.6099, Nodes_count: 228575, Cost Time: 234.57s\n",
      "Time: 2019-09-22 08:08:07.tensor(609)~2019-09-22 08:23:33.412, Loss: 0.7355, Nodes_count: 236417, Cost Time: 244.98s\n",
      "Time: 2019-09-22 08:23:33.tensor(412)~2019-09-22 08:38:57.6, Loss: 0.6507, Nodes_count: 242566, Cost Time: 251.28s\n",
      "Time: 2019-09-22 08:38:57.tensor(6)~2019-09-22 08:54:17.808, Loss: 0.5854, Nodes_count: 248664, Cost Time: 261.58s\n",
      "Time: 2019-09-22 08:54:17.tensor(808)~2019-09-22 09:09:38.276, Loss: 0.6359, Nodes_count: 255300, Cost Time: 276.31s\n",
      "Time: 2019-09-22 09:09:38.tensor(276)~2019-09-22 09:25:05.713, Loss: 0.7810, Nodes_count: 263730, Cost Time: 289.41s\n",
      "Time: 2019-09-22 09:25:05.tensor(713)~2019-09-22 09:40:07.434, Loss: 0.9348, Nodes_count: 273185, Cost Time: 300.52s\n",
      "Time: 2019-09-22 09:40:07.tensor(434)~2019-09-22 09:55:18.414, Loss: 0.6304, Nodes_count: 278335, Cost Time: 313.52s\n",
      "Time: 2019-09-22 09:55:18.tensor(414)~2019-09-22 10:10:27.648, Loss: 0.7403, Nodes_count: 287874, Cost Time: 332.19s\n",
      "Time: 2019-09-22 10:10:27.tensor(648)~2019-09-22 10:25:42.347, Loss: 0.6115, Nodes_count: 296424, Cost Time: 346.99s\n",
      "Time: 2019-09-22 10:25:42.tensor(347)~2019-09-22 10:40:55.262, Loss: 0.6381, Nodes_count: 302486, Cost Time: 355.37s\n",
      "Time: 2019-09-22 10:40:55.tensor(262)~2019-09-22 10:56:02.276, Loss: 0.5867, Nodes_count: 307069, Cost Time: 369.49s\n",
      "Time: 2019-09-22 10:56:02.tensor(276)~2019-09-22 11:11:11.79, Loss: 0.5947, Nodes_count: 313286, Cost Time: 383.34s\n",
      "Time: 2019-09-22 11:11:11.tensor(79)~2019-09-22 11:26:22.636, Loss: 0.6048, Nodes_count: 317784, Cost Time: 393.72s\n",
      "Time: 2019-09-22 11:26:22.tensor(636)~2019-09-22 11:41:38.703, Loss: 0.7258, Nodes_count: 324144, Cost Time: 403.08s\n",
      "Time: 2019-09-22 11:41:38.tensor(703)~2019-09-22 11:56:48.753, Loss: 0.6830, Nodes_count: 330445, Cost Time: 420.51s\n",
      "Time: 2019-09-22 11:56:48.tensor(753)~2019-09-22 12:12:02.722, Loss: 0.5823, Nodes_count: 336516, Cost Time: 435.10s\n",
      "Time: 2019-09-22 12:12:02.tensor(722)~2019-09-22 12:27:49.251, Loss: 0.5950, Nodes_count: 341067, Cost Time: 445.23s\n",
      "Time: 2019-09-22 12:27:49.tensor(251)~2019-09-22 12:43:02.68, Loss: 0.8174, Nodes_count: 350138, Cost Time: 460.63s\n",
      "Time: 2019-09-22 12:43:02.tensor(68)~2019-09-22 12:58:57.361, Loss: 0.6570, Nodes_count: 355400, Cost Time: 483.07s\n",
      "Time: 2019-09-22 12:58:57.tensor(361)~2019-09-22 13:15:02.28, Loss: 0.5678, Nodes_count: 361937, Cost Time: 500.96s\n",
      "Time: 2019-09-22 13:15:02.tensor(28)~2019-09-22 13:30:13.55, Loss: 0.6474, Nodes_count: 366414, Cost Time: 510.28s\n",
      "Time: 2019-09-22 13:30:13.tensor(55)~2019-09-22 13:45:16.142, Loss: 0.6566, Nodes_count: 372514, Cost Time: 520.66s\n",
      "Time: 2019-09-22 13:45:16.tensor(142)~2019-09-22 14:00:28.936, Loss: 0.6653, Nodes_count: 377968, Cost Time: 539.78s\n",
      "Time: 2019-09-22 14:00:28.tensor(936)~2019-09-22 14:15:57.595, Loss: 0.5880, Nodes_count: 384346, Cost Time: 559.75s\n",
      "Time: 2019-09-22 14:15:57.tensor(595)~2019-09-22 14:31:17.442, Loss: 0.6747, Nodes_count: 389605, Cost Time: 570.49s\n",
      "Time: 2019-09-22 14:31:17.tensor(442)~2019-09-22 14:46:26.576, Loss: 0.6985, Nodes_count: 395781, Cost Time: 584.62s\n",
      "Time: 2019-09-22 14:46:26.tensor(576)~2019-09-22 15:01:59.48, Loss: 0.8673, Nodes_count: 407162, Cost Time: 610.57s\n",
      "Time: 2019-09-22 15:01:59.tensor(48)~2019-09-22 15:17:12.388, Loss: 0.5675, Nodes_count: 412678, Cost Time: 630.54s\n",
      "Time: 2019-09-22 15:17:12.tensor(388)~2019-09-22 15:32:35.591, Loss: 0.6979, Nodes_count: 418892, Cost Time: 646.94s\n",
      "Time: 2019-09-22 15:32:35.tensor(591)~2019-09-22 15:47:36.689, Loss: 0.8623, Nodes_count: 428350, Cost Time: 674.41s\n",
      "Time: 2019-09-22 15:47:36.tensor(689)~2019-09-22 16:03:00.972, Loss: 0.5919, Nodes_count: 434076, Cost Time: 690.97s\n",
      "Time: 2019-09-22 16:03:00.tensor(972)~2019-09-22 16:18:27.176, Loss: 0.6173, Nodes_count: 440115, Cost Time: 713.56s\n",
      "Time: 2019-09-22 16:18:27.tensor(176)~2019-09-22 16:33:45.751, Loss: 0.6851, Nodes_count: 445649, Cost Time: 726.52s\n",
      "Time: 2019-09-22 16:33:45.tensor(751)~2019-09-22 16:49:05.5, Loss: 0.6325, Nodes_count: 455563, Cost Time: 749.29s\n",
      "Time: 2019-09-22 16:49:05.tensor(5)~2019-09-22 17:04:17.2, Loss: 0.6346, Nodes_count: 461476, Cost Time: 767.94s\n",
      "Time: 2019-09-22 17:04:17.tensor(2)~2019-09-22 17:19:30.274, Loss: 0.7669, Nodes_count: 471604, Cost Time: 794.53s\n",
      "Time: 2019-09-22 17:19:30.tensor(274)~2019-09-22 17:35:01.106, Loss: 0.7414, Nodes_count: 480605, Cost Time: 810.61s\n",
      "Time: 2019-09-22 17:35:01.tensor(106)~2019-09-22 17:50:02.33, Loss: 0.6929, Nodes_count: 491232, Cost Time: 832.67s\n",
      "Time: 2019-09-22 17:50:02.tensor(33)~2019-09-22 18:05:07.14, Loss: 0.6644, Nodes_count: 502271, Cost Time: 858.24s\n",
      "Time: 2019-09-22 18:05:07.tensor(14)~2019-09-22 18:20:12.679, Loss: 0.6756, Nodes_count: 513340, Cost Time: 888.14s\n",
      "Time: 2019-09-22 18:20:12.tensor(679)~2019-09-22 18:35:41.21, Loss: 0.7438, Nodes_count: 527861, Cost Time: 910.58s\n",
      "Time: 2019-09-22 18:35:41.tensor(21)~2019-09-22 18:50:41.617, Loss: 0.8008, Nodes_count: 536153, Cost Time: 937.61s\n",
      "Time: 2019-09-22 18:50:41.tensor(617)~2019-09-22 19:05:49.165, Loss: 0.5935, Nodes_count: 541505, Cost Time: 960.59s\n",
      "Time: 2019-09-22 19:05:49.tensor(165)~2019-09-22 19:21:08.859, Loss: 0.8795, Nodes_count: 552280, Cost Time: 1000.66s\n",
      "Time: 2019-09-22 19:21:08.tensor(859)~2019-09-22 19:36:37.413, Loss: 0.9110, Nodes_count: 559226, Cost Time: 1016.87s\n",
      "Time: 2019-09-22 19:36:37.tensor(413)~2019-09-22 19:51:38.548, Loss: 0.6137, Nodes_count: 564926, Cost Time: 1038.02s\n",
      "Time: 2019-09-22 19:51:38.tensor(548)~2019-09-22 20:06:39.278, Loss: 0.7167, Nodes_count: 573489, Cost Time: 1074.92s\n",
      "Time: 2019-09-22 20:06:39.tensor(278)~2019-09-22 20:21:53.29, Loss: 0.6309, Nodes_count: 580029, Cost Time: 1103.75s\n",
      "Time: 2019-09-22 20:21:53.tensor(29)~2019-09-22 20:37:02.303, Loss: 0.7161, Nodes_count: 587906, Cost Time: 1123.42s\n",
      "Time: 2019-09-22 20:37:02.tensor(303)~2019-09-22 20:52:16.71, Loss: 0.6594, Nodes_count: 593524, Cost Time: 1149.98s\n",
      "Time: 2019-09-22 20:52:16.tensor(71)~2019-09-22 21:07:40.816, Loss: 0.5598, Nodes_count: 599257, Cost Time: 1178.84s\n",
      "Time: 2019-09-22 21:07:40.tensor(816)~2019-09-22 21:22:58.93, Loss: 0.7585, Nodes_count: 606759, Cost Time: 1204.21s\n",
      "Time: 2019-09-22 21:22:58.tensor(93)~2019-09-22 21:38:07.555, Loss: 0.6951, Nodes_count: 612875, Cost Time: 1220.36s\n",
      "Time: 2019-09-22 21:38:07.tensor(555)~2019-09-22 21:53:12.956, Loss: 0.5992, Nodes_count: 618506, Cost Time: 1244.00s\n",
      "Time: 2019-09-22 21:53:12.tensor(956)~2019-09-22 22:08:26.406, Loss: 0.7105, Nodes_count: 626874, Cost Time: 1286.07s\n",
      "Time: 2019-09-22 22:08:26.tensor(406)~2019-09-22 22:23:54.3, Loss: 0.6205, Nodes_count: 635850, Cost Time: 1317.85s\n",
      "Time: 2019-09-22 22:23:54.tensor(3)~2019-09-22 22:39:14.6, Loss: 0.6867, Nodes_count: 642000, Cost Time: 1340.30s\n",
      "Time: 2019-09-22 22:39:14.tensor(6)~2019-09-22 22:54:30.464, Loss: 0.6112, Nodes_count: 647524, Cost Time: 1365.92s\n",
      "Time: 2019-09-22 22:54:30.tensor(464)~2019-09-22 23:09:45.12, Loss: 0.6163, Nodes_count: 652346, Cost Time: 1398.67s\n",
      "Time: 2019-09-22 23:09:45.tensor(12)~2019-09-22 23:25:13.195, Loss: 0.6146, Nodes_count: 658856, Cost Time: 1424.78s\n",
      "Time: 2019-09-22 23:25:13.tensor(195)~2019-09-22 23:40:33.808, Loss: 0.7929, Nodes_count: 664895, Cost Time: 1443.19s\n",
      "Time: 2019-09-22 23:40:33.tensor(808)~2019-09-22 23:55:54.784, Loss: 0.7408, Nodes_count: 675766, Cost Time: 1480.85s\n"
     ]
    }
   ],
   "source": [
    "ans_9_22_h501=test_day_new(graph_9_22_h501,\"/home/shahidul2k9/data/optc/test/graph_9_22_h501\",node_uuid2index_9_22_h501)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after merge: TemporalData(dst=[4072257], msg=[4072257, 42], src=[4072257], t=[4072257])\n",
      "Time: 2019-09-22 00:00:00.259~2019-09-22 00:15:03.94, Loss: 0.9110, Nodes_count: 9852, Cost Time: 4.95s\n",
      "Time: 2019-09-22 00:15:03.tensor(94)~2019-09-22 00:30:49.979, Loss: 0.6415, Nodes_count: 15519, Cost Time: 7.61s\n",
      "Time: 2019-09-22 00:30:49.tensor(979)~2019-09-22 00:46:00.351, Loss: 0.7857, Nodes_count: 24029, Cost Time: 12.03s\n",
      "Time: 2019-09-22 00:46:00.tensor(351)~2019-09-22 01:01:09.92, Loss: 1.0906, Nodes_count: 35370, Cost Time: 19.41s\n",
      "Time: 2019-09-22 01:01:09.tensor(92)~2019-09-22 01:16:13.46, Loss: 0.5851, Nodes_count: 41378, Cost Time: 25.55s\n",
      "Time: 2019-09-22 01:16:13.tensor(46)~2019-09-22 01:31:16.802, Loss: 0.7048, Nodes_count: 46381, Cost Time: 28.35s\n",
      "Time: 2019-09-22 01:31:16.tensor(802)~2019-09-22 01:46:29.171, Loss: 0.8591, Nodes_count: 54687, Cost Time: 33.34s\n",
      "Time: 2019-09-22 01:46:29.tensor(171)~2019-09-22 02:02:00.957, Loss: 0.6051, Nodes_count: 60573, Cost Time: 39.09s\n",
      "Time: 2019-09-22 02:02:00.tensor(957)~2019-09-22 02:17:07.743, Loss: 0.6054, Nodes_count: 66343, Cost Time: 45.97s\n",
      "Time: 2019-09-22 02:17:07.tensor(743)~2019-09-22 02:32:15.882, Loss: 0.7782, Nodes_count: 72411, Cost Time: 49.59s\n",
      "Time: 2019-09-22 02:32:15.tensor(882)~2019-09-22 02:47:17.656, Loss: 0.6458, Nodes_count: 78050, Cost Time: 54.48s\n",
      "Time: 2019-09-22 02:47:17.tensor(656)~2019-09-22 03:02:25.472, Loss: 0.5848, Nodes_count: 83312, Cost Time: 60.04s\n",
      "Time: 2019-09-22 03:02:25.tensor(472)~2019-09-22 03:17:33.145, Loss: 0.5572, Nodes_count: 89058, Cost Time: 67.25s\n",
      "Time: 2019-09-22 03:17:33.tensor(145)~2019-09-22 03:32:37.556, Loss: 0.6468, Nodes_count: 93891, Cost Time: 71.53s\n",
      "Time: 2019-09-22 03:32:37.tensor(556)~2019-09-22 03:47:53.638, Loss: 0.6418, Nodes_count: 99344, Cost Time: 76.95s\n",
      "Time: 2019-09-22 03:47:53.tensor(638)~2019-09-22 04:03:19.876, Loss: 0.7787, Nodes_count: 107695, Cost Time: 85.47s\n",
      "Time: 2019-09-22 04:03:19.tensor(876)~2019-09-22 04:18:37.242, Loss: 0.6787, Nodes_count: 116110, Cost Time: 94.59s\n",
      "Time: 2019-09-22 04:18:37.tensor(242)~2019-09-22 04:34:01.902, Loss: 0.6961, Nodes_count: 122058, Cost Time: 99.54s\n",
      "Time: 2019-09-22 04:34:01.tensor(902)~2019-09-22 04:49:34.129, Loss: 0.6856, Nodes_count: 133849, Cost Time: 108.31s\n",
      "Time: 2019-09-22 04:49:34.tensor(129)~2019-09-22 05:04:37.52, Loss: 0.7802, Nodes_count: 141335, Cost Time: 117.56s\n",
      "Time: 2019-09-22 05:04:37.tensor(52)~2019-09-22 05:20:00.679, Loss: 0.5633, Nodes_count: 146834, Cost Time: 126.33s\n",
      "Time: 2019-09-22 05:20:00.tensor(679)~2019-09-22 05:35:07.227, Loss: 0.7741, Nodes_count: 151592, Cost Time: 131.00s\n",
      "Time: 2019-09-22 05:35:07.tensor(227)~2019-09-22 05:50:46.698, Loss: 0.6984, Nodes_count: 158925, Cost Time: 138.61s\n",
      "Time: 2019-09-22 05:50:46.tensor(698)~2019-09-22 06:05:52.678, Loss: 0.5704, Nodes_count: 163840, Cost Time: 146.78s\n",
      "Time: 2019-09-22 06:05:52.tensor(678)~2019-09-22 06:21:09.475, Loss: 0.6707, Nodes_count: 170446, Cost Time: 156.37s\n",
      "Time: 2019-09-22 06:21:09.tensor(475)~2019-09-22 06:36:32.805, Loss: 1.1574, Nodes_count: 182353, Cost Time: 165.96s\n",
      "Time: 2019-09-22 06:36:32.tensor(805)~2019-09-22 06:51:39.117, Loss: 0.6636, Nodes_count: 189274, Cost Time: 175.56s\n",
      "Time: 2019-09-22 06:51:39.tensor(117)~2019-09-22 07:06:52.804, Loss: 0.7280, Nodes_count: 196429, Cost Time: 190.29s\n",
      "Time: 2019-09-22 07:06:52.tensor(804)~2019-09-22 07:22:21.115, Loss: 0.8437, Nodes_count: 206725, Cost Time: 203.07s\n",
      "Time: 2019-09-22 07:22:21.tensor(115)~2019-09-22 07:37:36.674, Loss: 0.7267, Nodes_count: 212710, Cost Time: 209.01s\n",
      "Time: 2019-09-22 07:37:36.tensor(674)~2019-09-22 07:52:47.263, Loss: 0.7119, Nodes_count: 220099, Cost Time: 219.72s\n",
      "Time: 2019-09-22 07:52:47.tensor(263)~2019-09-22 08:08:01.979, Loss: 0.5937, Nodes_count: 225018, Cost Time: 232.35s\n",
      "Time: 2019-09-22 08:08:01.tensor(979)~2019-09-22 08:23:31.73, Loss: 0.6260, Nodes_count: 231260, Cost Time: 244.05s\n",
      "Time: 2019-09-22 08:23:31.tensor(73)~2019-09-22 08:39:02.61, Loss: 0.7172, Nodes_count: 237401, Cost Time: 251.33s\n",
      "Time: 2019-09-22 08:39:02.tensor(61)~2019-09-22 08:54:03.111, Loss: 0.6348, Nodes_count: 243309, Cost Time: 261.78s\n",
      "Time: 2019-09-22 08:54:03.tensor(111)~2019-09-22 09:09:14.655, Loss: 0.6513, Nodes_count: 249181, Cost Time: 277.40s\n",
      "Time: 2019-09-22 09:09:14.tensor(655)~2019-09-22 09:24:39.448, Loss: 0.5668, Nodes_count: 255013, Cost Time: 287.23s\n",
      "Time: 2019-09-22 09:24:39.tensor(448)~2019-09-22 09:39:57.669, Loss: 0.6251, Nodes_count: 261012, Cost Time: 294.60s\n",
      "Time: 2019-09-22 09:39:57.tensor(669)~2019-09-22 09:54:58.8, Loss: 0.6017, Nodes_count: 266002, Cost Time: 306.86s\n",
      "Time: 2019-09-22 09:54:58.tensor(8)~2019-09-22 10:10:19.27, Loss: 0.7363, Nodes_count: 274585, Cost Time: 326.11s\n",
      "Time: 2019-09-22 10:10:19.tensor(27)~2019-09-22 10:25:26.141, Loss: 0.6650, Nodes_count: 283684, Cost Time: 342.27s\n",
      "Time: 2019-09-22 10:25:26.tensor(141)~2019-09-22 10:40:36.919, Loss: 0.6219, Nodes_count: 289541, Cost Time: 350.65s\n",
      "Time: 2019-09-22 10:40:36.tensor(919)~2019-09-22 10:55:49.687, Loss: 0.6929, Nodes_count: 295454, Cost Time: 366.61s\n",
      "Time: 2019-09-22 10:55:49.tensor(687)~2019-09-22 11:10:56.962, Loss: 0.5771, Nodes_count: 301260, Cost Time: 380.39s\n"
     ]
    }
   ],
   "source": [
    "ans_9_22_h051=test_day_new(graph_9_22_h051,\"/home/shahidul2k9/data/optc/test/graph_9_22_h051\",node_uuid2index_9_22_h051)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans_9_22_h209=test_day_new(graph_9_22_h209,\"/home/shahidul2k9/data/optc/test/graph_9_22_h209\",node_uuid2index_9_22_h209)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9-23~25 hosts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the models\n",
    "model=torch.load(\"/home/shahidul2k9/data/optc/model/model_saved_traindata=hosts_9_22.pt\")\n",
    "memory,gnn, link_pred,neighbor_loader=model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Test the input graphs with trained models\n",
    "graph_9_23_h201=test_day_new(graph_9_23_h201,\"/home/shahidul2k9/data/optc/test/graph_9_23_h201\",node_uuid2index_9_23_h201)\n",
    "graph_9_23_h402=test_day_new(graph_9_23_h402,\"/home/shahidul2k9/data/optc/test/graph_9_23_h402\",node_uuid2index_9_23_h402)\n",
    "graph_9_23_h660=test_day_new(graph_9_23_h660,\"/home/shahidul2k9/data/optc/test/graph_9_23_h660\",node_uuid2index_9_23_h660)\n",
    "graph_9_23_h501=test_day_new(graph_9_23_h501,\"/home/shahidul2k9/data/optc/test/graph_9_23_h501\",node_uuid2index_9_23_h501)\n",
    "graph_9_23_h051=test_day_new(graph_9_23_h051,\"/home/shahidul2k9/data/optc/test/graph_9_23_h051\",node_uuid2index_9_23_h051)\n",
    "graph_9_23_h207=test_day_new(graph_9_23_h207,\"/home/shahidul2k9/data/optc/test/graph_9_23_h207\",node_uuid2index_9_23_h207)\n",
    "\n",
    "graph_9_24_h201=test_day_new(graph_9_24_h201,\"/home/shahidul2k9/data/optc/test/graph_9_24_h201\",node_uuid2index_9_24_h201)\n",
    "graph_9_24_h402=test_day_new(graph_9_24_h402,\"/home/shahidul2k9/data/optc/test/graph_9_24_h402\",node_uuid2index_9_24_h402)\n",
    "graph_9_24_h660=test_day_new(graph_9_24_h660,\"/home/shahidul2k9/data/optc/test/graph_9_24_h660\",node_uuid2index_9_24_h660)\n",
    "graph_9_24_h501=test_day_new(graph_9_24_h501,\"/home/shahidul2k9/data/optc/test/graph_9_24_h501\",node_uuid2index_9_24_h501)\n",
    "graph_9_24_h051=test_day_new(graph_9_24_h051,\"/home/shahidul2k9/data/optc/test/graph_9_24_h051\",node_uuid2index_9_24_h051)\n",
    "graph_9_24_h207=test_day_new(graph_9_24_h207,\"/home/shahidul2k9/data/optc/test/graph_9_24_h207\",node_uuid2index_9_24_h207)\n",
    "\n",
    "graph_9_25_h201=test_day_new(graph_9_25_h201,\"/home/shahidul2k9/data/optc/test/graph_9_25_h201\",node_uuid2index_9_25_h201)\n",
    "graph_9_25_h402=test_day_new(graph_9_25_h402,\"/home/shahidul2k9/data/optc/test/graph_9_25_h402\",node_uuid2index_9_25_h402)\n",
    "graph_9_25_h660=test_day_new(graph_9_25_h660,\"/home/shahidul2k9/data/optc/test/graph_9_25_h660\",node_uuid2index_9_25_h660)\n",
    "graph_9_25_h501=test_day_new(graph_9_25_h501,\"/home/shahidul2k9/data/optc/test/graph_9_25_h501\",node_uuid2index_9_25_h501)\n",
    "graph_9_25_h051=test_day_new(graph_9_25_h051,\"/home/shahidul2k9/data/optc/test/graph_9_25_h051\",node_uuid2index_9_25_h051)\n",
    "graph_9_25_h207=test_day_new(graph_9_25_h207,\"/home/shahidul2k9/data/optc/test/graph_9_25_h207\",node_uuid2index_9_25_h207)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the tested graphs\n",
    "torch.save(graph_9_23_h201,\"/home/shahidul2k9/data/optc/test_res/graph_9_23_h201\")\n",
    "torch.save(graph_9_23_h402,\"/home/shahidul2k9/data/optc/test_res/graph_9_23_h402\")\n",
    "torch.save(graph_9_23_h660,\"/home/shahidul2k9/data/optc/test_res/graph_9_23_h660\")\n",
    "torch.save(graph_9_23_h501,\"/home/shahidul2k9/data/optc/test_res/graph_9_23_h501\")\n",
    "torch.save(graph_9_23_h051,\"/home/shahidul2k9/data/optc/test_res/graph_9_23_h051\")\n",
    "torch.save(graph_9_23_h207,\"/home/shahidul2k9/data/optc/test_res/graph_9_23_h207\")\n",
    "\n",
    "torch.save(graph_9_24_h201,\"/home/shahidul2k9/data/optc/test_res/graph_9_24_h201\")\n",
    "torch.save(graph_9_24_h402,\"/home/shahidul2k9/data/optc/test_res/graph_9_24_h402\")\n",
    "torch.save(graph_9_24_h660,\"/home/shahidul2k9/data/optc/test_res/graph_9_24_h660\")\n",
    "torch.save(graph_9_24_h501,\"/home/shahidul2k9/data/optc/test_res/graph_9_24_h501\")\n",
    "torch.save(graph_9_24_h051,\"/home/shahidul2k9/data/optc/test_res/graph_9_24_h051\")\n",
    "torch.save(graph_9_24_h207,\"/home/shahidul2k9/data/optc/test_res/graph_9_24_h207\")\n",
    "\n",
    "torch.save(graph_9_25_h201,\"/home/shahidul2k9/data/optc/test_res/graph_9_25_h201\")\n",
    "torch.save(graph_9_25_h402,\"/home/shahidul2k9/data/optc/test_res/graph_9_25_h402\")\n",
    "torch.save(graph_9_25_h660,\"/home/shahidul2k9/data/optc/test_res/graph_9_25_h660\")\n",
    "torch.save(graph_9_25_h501,\"/home/shahidul2k9/data/optc/test_res/graph_9_25_h501\")\n",
    "torch.save(graph_9_25_h051,\"/home/shahidul2k9/data/optc/test_res/graph_9_25_h051\")\n",
    "torch.save(graph_9_25_h207,\"/home/shahidul2k9/data/optc/test_res/graph_9_25_h207\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute the anomalous score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_train_IDF(find_str,file_list):\n",
    "    include_count=0\n",
    "    for f_path in (file_list):\n",
    "        f=open(f_path)\n",
    "        if find_str in f.read():\n",
    "            include_count+=1             \n",
    "    IDF=math.log(len(file_list)/(include_count+1))\n",
    "    return IDF\n",
    "\n",
    "\n",
    "def cal_IDF(find_str,file_path,file_list):\n",
    "    file_list=os.listdir(file_path)\n",
    "    include_count=0\n",
    "    different_neighbor=set()\n",
    "    for f_path in (file_list):\n",
    "        f=open(file_path+f_path)\n",
    "        if find_str in f.read():\n",
    "            include_count+=1\n",
    "                \n",
    "    IDF=math.log(len(file_list)/(include_count+1))\n",
    "    \n",
    "    return IDF,1\n",
    "\n",
    "\n",
    "def cal_IDF_by_file_in_mem(find_str,file_list):\n",
    "    include_count=0\n",
    "    different_neighbor=set()\n",
    "    for f in (file_list):       \n",
    "        if find_str in f:\n",
    "            include_count+=1\n",
    "    IDF=math.log(len(file_list)/(include_count+1))    \n",
    "    return IDF\n",
    "\n",
    "def cal_redundant(find_str,edge_list):\n",
    "    \n",
    "    different_neighbor=set()\n",
    "    for e in edge_list:\n",
    "        if find_str in str(e):\n",
    "            different_neighbor.add(e[0])\n",
    "            different_neighbor.add(e[1])\n",
    "    return len(different_neighbor)-2\n",
    "\n",
    "def cal_anomaly_loss(loss_list,edge_list,file_path):\n",
    "    \n",
    "    if len(loss_list)!=len(edge_list):\n",
    "        print(\"error!\")\n",
    "        return 0\n",
    "    count=0\n",
    "    loss_sum=0\n",
    "    loss_std=std(loss_list)\n",
    "    loss_mean=mean(loss_list)\n",
    "    edge_set=set()\n",
    "    node_set=set()\n",
    "    node2redundant={}\n",
    "    \n",
    "    thr=loss_mean+2.5*loss_std\n",
    "\n",
    "    print(\"thr:\",thr)\n",
    "    \n",
    "    for i in range(len(loss_list)):\n",
    "        if loss_list[i]>thr:\n",
    "            count+=1\n",
    "            src_node=edge_list[i][0]\n",
    "            dst_node=edge_list[i][1]\n",
    "\n",
    "            loss_sum+=loss_list[i]\n",
    "    \n",
    "            node_set.add(src_node)\n",
    "            node_set.add(dst_node)\n",
    "            edge_set.add(edge_list[i][0]+edge_list[i][1])\n",
    "    return count, loss_sum/count,node_set,edge_set\n",
    "#     return count, count/len(loss_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construct the relations between time windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "file_list=[]\n",
    "\n",
    "file_path=\"/home/shahidul2k9/data/optc/test/graph_9_22_h201/\"\n",
    "file_l=os.listdir(\"/home/shahidul2k9/data/optc/test/graph_9_22_h201/\")\n",
    "for i in file_l:\n",
    "    file_list.append(file_path+i)\n",
    "\n",
    "file_path=\"/home/shahidul2k9/data/optc/test/graph_9_22_h402/\"\n",
    "file_l=os.listdir(\"/home/shahidul2k9/data/optc/test/graph_9_22_h402/\")\n",
    "for i in file_l:\n",
    "    file_list.append(file_path+i)\n",
    "    \n",
    "file_path=\"/home/shahidul2k9/data/optc/test/graph_9_22_h660/\"\n",
    "file_l=os.listdir(\"/home/shahidul2k9/data/optc/test/graph_9_22_h660/\")\n",
    "for i in file_l:\n",
    "    file_list.append(file_path+i)\n",
    "    \n",
    "file_path=\"/home/shahidul2k9/data/optc/test/graph_9_22_h501/\"\n",
    "file_l=os.listdir(\"/home/shahidul2k9/data/optc/test/graph_9_22_h501/\")\n",
    "for i in file_l:\n",
    "    file_list.append(file_path+i)\n",
    "    \n",
    "file_path=\"/home/shahidul2k9/data/optc/test/graph_9_22_h051/\"\n",
    "file_l=os.listdir(\"/home/shahidul2k9/data/optc/test/graph_9_22_h051/\")\n",
    "for i in file_l:\n",
    "    file_list.append(file_path+i)    \n",
    "    \n",
    "    \n",
    "file_path=\"/home/shahidul2k9/data/optc/test/graph_9_22_h209/\"\n",
    "file_l=os.listdir(\"/home/shahidul2k9/data/optc/test/graph_9_22_h209/\")\n",
    "for i in file_l:\n",
    "    file_list.append(file_path+i) \n",
    "\n",
    "    \n",
    "def is_include_key_word(s):\n",
    "    keywords=[\n",
    "         '->',\n",
    "        '.DLL',\n",
    "        '.dll',\n",
    "        '.dat', \n",
    "       '.DAT', \n",
    "        'CACHE',\n",
    "        'Cache',\n",
    "        '.docx',\n",
    "        '.lnk',\n",
    "        '.LNK',\n",
    "        '.pptx',\n",
    "        '.xlsx',\n",
    "        'CVR',\n",
    "        'cvr',\n",
    "        'ZLEAZER',\n",
    "        'zleazer',\n",
    "        'SOFTWAREPROTECTIONPLATFORM',\n",
    "        'documents',\n",
    "        '.log',\n",
    "        '.nls',\n",
    "        '.EVTX',\n",
    "        '.evtx',\n",
    "        '.tmp',\n",
    "        '.TMP',\n",
    "        'Windows/Logs/',\n",
    "        'Windows/system32/',\n",
    "        'Windows/System32/',\n",
    "        '/Temp/',\n",
    "        'Users',\n",
    "        'USERS',\n",
    "        'Program Files',\n",
    "        'WINDOWS',\n",
    "        'Windows',\n",
    "        '$SII',\n",
    "        'svchost.exe',\n",
    "        'gpscript.exe',\n",
    "        'python.exe',\n",
    "        'rundll32.exe',\n",
    "        'consent.exe',\n",
    "        'python27',\n",
    "        'Python27',\n",
    "      ]\n",
    "    flag=False\n",
    "    for i in keywords:\n",
    "        if i in s:\n",
    "            flag=True\n",
    "    return flag    \n",
    "    \n",
    "    \n",
    "def cal_set_rel(s1,s2):\n",
    "    new_s=s1 & s2\n",
    "    count=0\n",
    "    for i in new_s:\n",
    "        if is_include_key_word(i) is not True:\n",
    "\n",
    "            if i in node_IDF.keys():\n",
    "                IDF=node_IDF[i]\n",
    "            else:\n",
    "                IDF=math.log(len(file_list)/(1))\n",
    "            if IDF>(math.log(len(file_list)*0.9)):\n",
    "                print(\"node:\",i,\" IDF:\",IDF)\n",
    "                count+=1\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "math.log(len(file_list)/(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute the IDF-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "file_list=[]\n",
    "\n",
    "file_path=\"/home/shahidul2k9/data/optc/test/graph_9_22_h201/\"\n",
    "file_l=os.listdir(\"/home/shahidul2k9/data/optc/test/graph_9_22_h201/\")\n",
    "for i in file_l:\n",
    "    file_list.append(file_path+i)\n",
    "\n",
    "file_path=\"/home/shahidul2k9/data/optc/test/graph_9_22_h402/\"\n",
    "file_l=os.listdir(\"/home/shahidul2k9/data/optc/test/graph_9_22_h402/\")\n",
    "for i in file_l:\n",
    "    file_list.append(file_path+i)\n",
    "    \n",
    "file_path=\"/home/shahidul2k9/data/optc/test/graph_9_22_h660/\"\n",
    "file_l=os.listdir(\"/home/shahidul2k9/data/optc/test/graph_9_22_h660/\")\n",
    "for i in file_l:\n",
    "    file_list.append(file_path+i)\n",
    "    \n",
    "file_path=\"/home/shahidul2k9/data/optc/test/graph_9_22_h501/\"\n",
    "file_l=os.listdir(\"/home/shahidul2k9/data/optc/test/graph_9_22_h501/\")\n",
    "for i in file_l:\n",
    "    file_list.append(file_path+i)\n",
    "    \n",
    "file_path=\"/home/shahidul2k9/data/optc/test/graph_9_22_h051/\"\n",
    "file_l=os.listdir(\"/home/shahidul2k9/data/optc/test/graph_9_22_h051/\")\n",
    "for i in file_l:\n",
    "    file_list.append(file_path+i)    \n",
    "    \n",
    "    \n",
    "file_path=\"/home/shahidul2k9/data/optc/test/graph_9_22_h209/\"\n",
    "file_l=os.listdir(\"/home/shahidul2k9/data/optc/test/graph_9_22_h209/\")\n",
    "for i in file_l:\n",
    "    file_list.append(file_path+i)    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_set=set()\n",
    "\n",
    "for f_path in tqdm(file_list):\n",
    "    f=open(f_path)\n",
    "    for line in f:\n",
    "        l=line.strip()\n",
    "        jdata=eval(l)\n",
    "        if jdata['loss']>0:\n",
    "            if '->' not in str(jdata['srcmsg']):\n",
    "                node_set.add(str(jdata['srcmsg']).split(\"_@\")[-1])\n",
    "            if '->' not in str(jdata['dstmsg']):\n",
    "                node_set.add(str(jdata['dstmsg']).split(\"_@\")[-1]) \n",
    "\n",
    "\n",
    "node_list=list(node_set)\n",
    "del node_set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_mem_list=[]\n",
    "for f_path in (file_list):\n",
    "        f=open(f_path)\n",
    "        files_mem_list.append(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_node_idf(_node_list,_files_mem_list,share_node_IDF):\n",
    "    for n in tqdm(_node_list):  \n",
    "        find_str=n\n",
    "        IDF=cal_IDF_by_file_in_mem(n,_files_mem_list)\n",
    "        share_node_IDF[n]=IDF\n",
    "\n",
    "import multiprocessing as mp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "share_node_IDF = mp.Manager().dict()\n",
    "cores=28\n",
    "offset=math.ceil(len(node_list)/cores)\n",
    "node_list_split=[]\n",
    "for i in range(0,len(node_list),offset):\n",
    "    node_list_split+=[node_list[i:i+offset]]     \n",
    "process_list=[]\n",
    "for i in range(cores):\n",
    "    process_list.append(mp.Process(target=process_node_idf, args=(node_list_split[i],files_mem_list,share_node_IDF)))\n",
    "for i in process_list:\n",
    "    i.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_IDF=dict(share_node_IDF)\n",
    "torch.save(node_IDF,\"node_IDF_9_22_hosts\")\n",
    "print(\"IDF weight calculate complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# force to terminate all process\n",
    "for i in process_list:\n",
    "    i.terminate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The IDF of the date 9-22 h201\n",
    "node_IDF={}\n",
    "node_set=set()\n",
    "\n",
    "file_list=[]\n",
    "\n",
    "file_path=\"/home/shahidul2k9/data/optc/test/graph_9_22/\"\n",
    "file_l=os.listdir(\"/home/shahidul2k9/data/optc/test/graph_9_22/\")\n",
    "\n",
    "\n",
    "for i in file_l:\n",
    "    file_list.append(file_path+i)\n",
    "\n",
    "for f_path in tqdm(file_list):\n",
    "    f=open(f_path)\n",
    "    for line in f:\n",
    "        l=line.strip()\n",
    "        jdata=eval(l)\n",
    "        if jdata['loss']>0:\n",
    "            if '->' not in str(jdata['srcmsg']):\n",
    "                node_set.add(str(jdata['srcmsg']))\n",
    "            if '->' not in str(jdata['dstmsg']):\n",
    "                node_set.add(str(jdata['dstmsg'])) \n",
    "\n",
    "for n in tqdm(node_set):\n",
    "#     find_str=list(eval(n).values())[0]\n",
    "    IDF=cal_train_IDF(n,file_list)\n",
    "    node_IDF[n]=IDF\n",
    "\n",
    "\n",
    "torch.save(node_IDF,\"node_IDF_9_22_without_netflow\")\n",
    "print(\"IDF weight calculate complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# labelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "labels={}\n",
    "path=\"/home/shahidul2k9/data/optc/test/graph_9_23_h201/\"\n",
    "filelist = os.listdir(path)\n",
    "filelist.sort()\n",
    "for f in filelist:\n",
    "    labels[path+f]=0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## h201"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "label_h201={}\n",
    "\n",
    "test_path=\"/home/shahidul2k9/data/optc/test/graph_9_23_h201/\"\n",
    "\n",
    "filelist = os.listdir(test_path)\n",
    "filelist.sort()\n",
    "for f in filelist:\n",
    "    label_h201[test_path+f]=0\n",
    "\n",
    "test_path=\"/home/shahidul2k9/data/optc/test/graph_9_24_h201/\"\n",
    "\n",
    "filelist = os.listdir(test_path)\n",
    "filelist.sort()\n",
    "for f in filelist:\n",
    "    label_h201[test_path+f]=0\n",
    "\n",
    "test_path=\"/home/shahidul2k9/data/optc/test/graph_9_25_h201/\"\n",
    "\n",
    "filelist = os.listdir(test_path)\n",
    "filelist.sort()\n",
    "for f in filelist:\n",
    "    label_h201[test_path+f]=0\n",
    "\n",
    "attack_list=[\n",
    "'/home/shahidul2k9/data/optc/test/graph_9_23_h201/2019-09-23 11:23:44.136~2019-09-23 11:38:30.698.txt',\n",
    " '/home/shahidul2k9/data/optc/test/graph_9_23_h201/2019-09-23 11:38:40.698~2019-09-23 11:53:39.57.txt',\n",
    "\n",
    " '/home/shahidul2k9/data/optc/test/graph_9_23_h201/2019-09-23 12:38:24.95~2019-09-23 12:54:14.286.txt',\n",
    " '/home/shahidul2k9/data/optc/test/graph_9_23_h201/2019-09-23 12:55:28.286~2019-09-23 13:09:50.95.txt',\n",
    " '/home/shahidul2k9/data/optc/test/graph_9_23_h201/2019-09-23 13:10:24.95~2019-09-23 13:24:56.43.txt',\n",
    "\n",
    "]\n",
    "for i in attack_list:\n",
    "    label_h201[i]=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## h402"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "label_h402={}\n",
    "\n",
    "test_path=\"/home/shahidul2k9/data/optc/test/graph_9_23_h402/\"\n",
    "\n",
    "filelist = os.listdir(test_path)\n",
    "filelist.sort()\n",
    "for f in filelist:\n",
    "    label_h402[test_path+f]=0\n",
    "\n",
    "test_path=\"/home/shahidul2k9/data/optc/test/graph_9_24_h402/\"\n",
    "\n",
    "filelist = os.listdir(test_path)\n",
    "filelist.sort()\n",
    "for f in filelist:\n",
    "    label_h402[test_path+f]=0\n",
    "\n",
    "test_path=\"/home/shahidul2k9/data/optc/test/graph_9_25_h402/\"\n",
    "\n",
    "filelist = os.listdir(test_path)\n",
    "filelist.sort()\n",
    "for f in filelist:\n",
    "    label_h402[test_path+f]=0\n",
    "\n",
    "attack_list=[\n",
    " '/home/shahidul2k9/data/optc/test/graph_9_23_h402/2019-09-23 13:10:24.429~2019-09-23 13:25:09.374.txt',\n",
    " '/home/shahidul2k9/data/optc/test/graph_9_23_h402/2019-09-23 13:25:20.374~2019-09-23 13:40:21.268.txt',\n",
    "\n",
    "    # adjust\n",
    "#      'graph_9_23_h402/2019-09-23 13:40:16.268~2019-09-23 13:55:31.31.txt',\n",
    "#  'graph_9_23_h402/2019-09-23 13:55:12.31~2019-09-23 14:10:58.2.txt',\n",
    "]\n",
    "for i in attack_list:\n",
    "    label_h402[i]=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## h660"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "label_h660={}\n",
    "\n",
    "test_path=\"/home/shahidul2k9/data/optc/test/graph_9_23_h660/\"\n",
    "\n",
    "filelist = os.listdir(test_path)\n",
    "filelist.sort()\n",
    "for f in filelist:\n",
    "    label_h660[test_path+f]=0\n",
    "\n",
    "test_path=\"/home/shahidul2k9/data/optc/test/graph_9_24_h660/\"\n",
    "\n",
    "filelist = os.listdir(test_path)\n",
    "filelist.sort()\n",
    "for f in filelist:\n",
    "    label_h660[test_path+f]=0\n",
    "\n",
    "test_path=\"/home/shahidul2k9/data/optc/test/graph_9_25_h660/\"\n",
    "\n",
    "filelist = os.listdir(test_path)\n",
    "filelist.sort()\n",
    "for f in filelist:\n",
    "    label_h660[test_path+f]=0\n",
    "\n",
    "attack_list=[\n",
    "'/home/shahidul2k9/data/optc/test/graph_9_23_h660/2019-09-23 13:27:28.512~2019-09-23 13:42:30.682.txt',\n",
    " '/home/shahidul2k9/data/optc/test/graph_9_23_h660/2019-09-23 13:42:24.682~2019-09-23 13:57:57.566.txt',\n",
    " '/home/shahidul2k9/data/optc/test/graph_9_23_h660/2019-09-23 13:57:20.566~2019-09-23 14:12:59.139.txt',\n",
    "]\n",
    "for i in attack_list:\n",
    "    label_h660[i]=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "label_h660"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## h501"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "label_h501={}\n",
    "\n",
    "test_path=\"/home/shahidul2k9/data/optc/test/graph_9_23_h501/\"\n",
    "\n",
    "filelist = os.listdir(test_path)\n",
    "filelist.sort()\n",
    "for f in filelist:\n",
    "    label_h501[test_path+f]=0\n",
    "\n",
    "test_path=\"/home/shahidul2k9/data/optc/test/graph_9_24_h501/\"\n",
    "\n",
    "filelist = os.listdir(test_path)\n",
    "filelist.sort()\n",
    "for f in filelist:\n",
    "    label_h501[test_path+f]=0\n",
    "\n",
    "test_path=\"/home/shahidul2k9/data/optc/test/graph_9_25_h501/\"\n",
    "\n",
    "filelist = os.listdir(test_path)\n",
    "filelist.sort()\n",
    "for f in filelist:\n",
    "    label_h501[test_path+f]=0\n",
    "\n",
    "attack_list=[\n",
    "\n",
    " '/home/shahidul2k9/data/optc/test/graph_9_24_h501/2019-09-24 10:15:28.241~2019-09-24 10:30:00.201.txt',\n",
    " '/home/shahidul2k9/data/optc/test/graph_9_24_h501/2019-09-24 10:30:24.201~2019-09-24 10:45:02.7.txt',\n",
    " '/home/shahidul2k9/data/optc/test/graph_9_24_h501/2019-09-24 10:45:20.7~2019-09-24 11:00:31.385.txt',\n",
    " '/home/shahidul2k9/data/optc/test/graph_9_24_h501/2019-09-24 11:00:16.385~2019-09-24 11:16:09.755.txt',\n",
    " '/home/shahidul2k9/data/optc/test/graph_9_24_h501/2019-09-24 11:15:12.755~2019-09-24 11:31:14.287.txt',\n",
    " '/home/shahidul2k9/data/optc/test/graph_9_24_h501/2019-09-24 11:32:16.287~2019-09-24 11:46:31.541.txt',\n",
    "\n",
    " '/home/shahidul2k9/data/optc/test/graph_9_24_h501/2019-09-24 13:04:00.804~2019-09-24 13:17:29.451.txt',\n",
    " '/home/shahidul2k9/data/optc/test/graph_9_24_h501/2019-09-24 13:18:56.451~2019-09-24 13:32:46.454.txt',\n",
    " '/home/shahidul2k9/data/optc/test/graph_9_24_h501/2019-09-24 13:33:52.454~2019-09-24 13:48:02.493.txt',\n",
    "\n",
    "# adjust\n",
    "\n",
    "#  'graph_9_24_h501/2019-09-24 11:47:12.541~2019-09-24 12:01:32.699.txt',\n",
    "#  'graph_9_24_h501/2019-09-24 12:02:08.699~2019-09-24 12:16:34.904.txt',\n",
    "#  'graph_9_24_h501/2019-09-24 12:32:00.112~2019-09-24 12:46:58.691.txt',\n",
    "#  'graph_9_24_h501/2019-09-24 12:46:56.691~2019-09-24 13:02:02.804.txt',\n",
    "#  'graph_9_24_h501/2019-09-24 16:22:24.281~2019-09-24 16:36:06.878.txt',\n",
    "\n",
    "]\n",
    "for i in attack_list:\n",
    "    label_h501[i]=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "label_h501"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## h051"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "label_h051={}\n",
    "\n",
    "test_path=\"/home/shahidul2k9/data/optc/test/graph_9_23_h051/\"\n",
    "\n",
    "filelist = os.listdir(test_path)\n",
    "filelist.sort()\n",
    "for f in filelist:\n",
    "    label_h051[test_path+f]=0\n",
    "\n",
    "test_path=\"/home/shahidul2k9/data/optc/test/graph_9_24_h051/\"\n",
    "\n",
    "filelist = os.listdir(test_path)\n",
    "filelist.sort()\n",
    "for f in filelist:\n",
    "    label_h051[test_path+f]=0\n",
    "\n",
    "test_path=\"/home/shahidul2k9/data/optc/test/graph_9_25_h051/\"\n",
    "\n",
    "filelist = os.listdir(test_path)\n",
    "filelist.sort()\n",
    "for f in filelist:\n",
    "    label_h051[test_path+f]=0\n",
    "\n",
    "attack_list=[\n",
    "'/home/shahidul2k9/data/optc/test/graph_9_25_h051/2019-09-25 10:26:08.397~2019-09-25 10:41:40.247.txt',\n",
    " '/home/shahidul2k9/data/optc/test/graph_9_25_h051/2019-09-25 10:41:04.247~2019-09-25 10:56:56.92.txt',\n",
    " '/home/shahidul2k9/data/optc/test/graph_9_25_h051/2019-09-25 10:56:00.92~2019-09-25 11:12:03.608.txt',\n",
    "]\n",
    "for i in attack_list:\n",
    "    label_h051[i]=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "label_h051"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## h207"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "label_h207={}\n",
    "\n",
    "test_path=\"/home/shahidul2k9/data/optc/test/graph_9_23_h207/\"\n",
    "\n",
    "filelist = os.listdir(test_path)\n",
    "filelist.sort()\n",
    "for f in filelist:\n",
    "    label_h207[test_path+f]=0\n",
    "\n",
    "test_path=\"/home/shahidul2k9/data/optc/test/graph_9_24_h207/\"\n",
    "\n",
    "filelist = os.listdir(test_path)\n",
    "filelist.sort()\n",
    "for f in filelist:\n",
    "    label_h207[test_path+f]=0\n",
    "\n",
    "test_path=\"/home/shahidul2k9/data/optc/test/graph_9_25_h207/\"\n",
    "\n",
    "filelist = os.listdir(test_path)\n",
    "filelist.sort()\n",
    "for f in filelist:\n",
    "    label_h207[test_path+f]=0\n",
    "\n",
    "attack_list=[\n",
    "\n",
    "]\n",
    "for i in attack_list:\n",
    "    label_h207[i]=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anoamly detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def classifier_evaluation(y_test, y_test_pred):\n",
    "    # groundtruth, pred_value\n",
    "    tn, fp, fn, tp =confusion_matrix(y_test, y_test_pred).ravel()\n",
    "#     tn+=100\n",
    "#     print(clf_name,\" : \")\n",
    "    print('tn:',tn)\n",
    "    print('fp:',fp)\n",
    "    print('fn:',fn)\n",
    "    print('tp:',tp)\n",
    "    precision=tp/(tp+fp)\n",
    "    recall=tp/(tp+fn)\n",
    "    accuracy=(tp+tn)/(tp+tn+fp+fn)\n",
    "    fscore=2*(precision*recall)/(precision+recall)\n",
    "    auc_val=roc_auc_score(y_test, y_test_pred)\n",
    "    print(\"precision:\",precision)\n",
    "    print(\"recall:\",recall)\n",
    "    print(\"fscore:\",fscore)\n",
    "    print(\"accuracy:\",accuracy)\n",
    "    print(\"auc_val:\",auc_val)\n",
    "    return precision,recall,fscore,accuracy,auc_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## h201"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9-23"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "node_IDF=torch.load(\"node_IDF_9_22_hosts\")\n",
    "\n",
    "# node_set_list=[]\n",
    "history_list_9_23_h201=[]\n",
    "tw_que=[]\n",
    "his_tw={}\n",
    "current_tw={}\n",
    "\n",
    "test_path=\"/home/shahidul2k9/data/optc/test/graph_9_23_h201/\"\n",
    "\n",
    "\n",
    "file_l=os.listdir(test_path)\n",
    "file_l.sort()\n",
    "index_count=0\n",
    "for f_path in (file_l):\n",
    "    f=open(test_path+f_path)\n",
    "    edge_loss_list=[]\n",
    "    edge_list=[]\n",
    "    print('index_count:',index_count)\n",
    "    \n",
    "    for line in f:\n",
    "        l=line.strip()\n",
    "        jdata=eval(l)\n",
    "        edge_loss_list.append(jdata['loss'])\n",
    "        edge_list.append([str(jdata['srcmsg']).split(\"_@\")[-1],str(jdata['dstmsg']).split(\"_@\")[-1]])\n",
    "#         edge_list.append([str(jdata['srcmsg']),str(jdata['dstmsg'])])\n",
    "        \n",
    "#     df_list_9_22.append(pd.DataFrame(edge_loss_list))\n",
    "    count,loss_avg,node_set,edge_set=cal_anomaly_loss(edge_loss_list,edge_list,test_path)\n",
    "    current_tw['name']=f_path\n",
    "    current_tw['loss']=loss_avg\n",
    "    current_tw['index']=index_count\n",
    "    current_tw['nodeset']=node_set\n",
    "\n",
    "    added_que_flag=False\n",
    "    for hq in history_list_9_23_h201:\n",
    "        for his_tw in hq:\n",
    "            if cal_set_rel(current_tw['nodeset'],his_tw['nodeset'])!=0 and current_tw['name']!=his_tw['name']:\n",
    "                hq.append(copy.deepcopy(current_tw))\n",
    "                print(f\"{his_tw['name']=}\")\n",
    "                added_que_flag=True\n",
    "                break\n",
    "            if added_que_flag:\n",
    "                break\n",
    "    if added_que_flag is False:\n",
    "        temp_hq=[copy.deepcopy(current_tw)]\n",
    "        history_list_9_23_h201.append(temp_hq)\n",
    "  \n",
    "    index_count+=1\n",
    "#     node_set_list.append(node_set)\n",
    "    print( f_path,\"  \",loss_avg,\" count:\",count,\" percentage:\",count/len(edge_list),\" node count:\",len(node_set),\" edge count:\",len(edge_set))\n",
    "#     y_data_4_10.append([loss_avg,labels_4_10[f_path],f_path])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_label_h201={}\n",
    "\n",
    "test_path=\"/home/shahidul2k9/data/optc/test/graph_9_23_h201/\"\n",
    "    \n",
    "filelist = os.listdir(test_path)\n",
    "filelist.sort()\n",
    "for f in filelist:\n",
    "    pred_label_h201[test_path+f]=0\n",
    "    \n",
    "test_path=\"/home/shahidul2k9/data/optc/test/graph_9_24_h201/\"\n",
    "    \n",
    "filelist = os.listdir(test_path)\n",
    "filelist.sort()\n",
    "for f in filelist:\n",
    "    pred_label_h201[test_path+f]=0\n",
    "    \n",
    "test_path=\"/home/shahidul2k9/data/optc/test/graph_9_25_h201/\"\n",
    "    \n",
    "filelist = os.listdir(test_path)\n",
    "filelist.sort()\n",
    "for f in filelist:\n",
    "    pred_label_h201[test_path+f]=0\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "for hl in history_list_9_23_h201:\n",
    "    loss_count=0\n",
    "    for hq in hl:\n",
    "        if loss_count==0:\n",
    "            loss_count=(loss_count+1)*(hq['loss']+1)\n",
    "        else:\n",
    "            loss_count=(loss_count)*(hq['loss']+1)\n",
    "    name_list=[]\n",
    "    if loss_count>1000:\n",
    "        name_list=[]\n",
    "        for i in hl:\n",
    "            name_list.append(i['name'])\n",
    "        print(*name_list, sep = \"\\n\")\n",
    "        for i in name_list:\n",
    "            pred_label_h201[\"/home/shahidul2k9/data/optc/test/graph_9_23_h201/\"+i]=1\n",
    "        print(loss_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate\n",
    "y=[]\n",
    "y_pred=[]\n",
    "for i in label_h201:\n",
    "    y.append(label_h201[i])\n",
    "    y_pred.append(pred_label_h201[i])\n",
    "    \n",
    "classifier_evaluation(y,y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9-24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "node_IDF=torch.load(\"node_IDF_9_22_hosts\")\n",
    "\n",
    "# node_set_list=[]\n",
    "history_list=[]\n",
    "tw_que=[]\n",
    "his_tw={}\n",
    "current_tw={}\n",
    "\n",
    "test_path=\"/home/shahidul2k9/data/optc/test/graph_9_24_h201/\"\n",
    "\n",
    "\n",
    "file_l=os.listdir(test_path)\n",
    "file_l.sort()\n",
    "index_count=0\n",
    "for f_path in (file_l):\n",
    "    f=open(test_path+f_path)\n",
    "    edge_loss_list=[]\n",
    "    edge_list=[]\n",
    "    print('index_count:',index_count)\n",
    "    \n",
    "    for line in f:\n",
    "        l=line.strip()\n",
    "        jdata=eval(l)\n",
    "        edge_loss_list.append(jdata['loss'])\n",
    "        edge_list.append([str(jdata['srcmsg']).split(\"_@\")[-1],str(jdata['dstmsg']).split(\"_@\")[-1]])\n",
    "#         edge_list.append([str(jdata['srcmsg']),str(jdata['dstmsg'])])\n",
    "        \n",
    "#     df_list_9_22.append(pd.DataFrame(edge_loss_list))\n",
    "    count,loss_avg,node_set,edge_set=cal_anomaly_loss(edge_loss_list,edge_list,test_path)\n",
    "    current_tw['name']=f_path\n",
    "    current_tw['loss']=loss_avg\n",
    "    current_tw['index']=index_count\n",
    "    current_tw['nodeset']=node_set\n",
    "\n",
    "    added_que_flag=False\n",
    "    for hq in history_list:\n",
    "        for his_tw in hq:\n",
    "            if cal_set_rel(current_tw['nodeset'],his_tw['nodeset'])!=0 and current_tw['name']!=his_tw['name']:\n",
    "                hq.append(copy.deepcopy(current_tw))\n",
    "                print(f\"{his_tw['name']=}\")\n",
    "                added_que_flag=True\n",
    "                break\n",
    "            if added_que_flag:\n",
    "                break\n",
    "    if added_que_flag is False:\n",
    "        temp_hq=[copy.deepcopy(current_tw)]\n",
    "        history_list.append(temp_hq)\n",
    "  \n",
    "    index_count+=1\n",
    "#     node_set_list.append(node_set)\n",
    "    print( f_path,\"  \",loss_avg,\" count:\",count,\" percentage:\",count/len(edge_list),\" node count:\",len(node_set),\" edge count:\",len(edge_set))\n",
    "#     y_data_4_10.append([loss_avg,labels_4_10[f_path],f_path])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for hl in history_list:\n",
    "    loss_count=0\n",
    "    for hq in hl:\n",
    "        if loss_count==0:\n",
    "            loss_count=(loss_count+1)*(hq['loss']+1)\n",
    "        else:\n",
    "            loss_count=(loss_count)*(hq['loss']+1)\n",
    "    name_list=[]\n",
    "    if loss_count>10000:\n",
    "        name_list=[]\n",
    "        for i in hl:\n",
    "            name_list.append(i['name'])\n",
    "        print(name_list)\n",
    "#         for i in name_list:\n",
    "#             pred_label[i]=1\n",
    "        print(loss_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9-25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "node_IDF=torch.load(\"node_IDF_9_22_hosts\")\n",
    "\n",
    "# node_set_list=[]\n",
    "history_list=[]\n",
    "tw_que=[]\n",
    "his_tw={}\n",
    "current_tw={}\n",
    "\n",
    "test_path=\"/home/shahidul2k9/data/optc/test/graph_9_25_h201/\"\n",
    "\n",
    "\n",
    "file_l=os.listdir(test_path)\n",
    "file_l.sort()\n",
    "index_count=0\n",
    "for f_path in (file_l):\n",
    "    f=open(test_path+f_path)\n",
    "    edge_loss_list=[]\n",
    "    edge_list=[]\n",
    "    print('index_count:',index_count)\n",
    "    \n",
    "    for line in f:\n",
    "        l=line.strip()\n",
    "        jdata=eval(l)\n",
    "        edge_loss_list.append(jdata['loss'])\n",
    "        edge_list.append([str(jdata['srcmsg']).split(\"_@\")[-1],str(jdata['dstmsg']).split(\"_@\")[-1]])\n",
    "#         edge_list.append([str(jdata['srcmsg']),str(jdata['dstmsg'])])\n",
    "    count,loss_avg,node_set,edge_set=cal_anomaly_loss(edge_loss_list,edge_list,test_path)\n",
    "    current_tw['name']=f_path\n",
    "    current_tw['loss']=loss_avg\n",
    "    current_tw['index']=index_count\n",
    "    current_tw['nodeset']=node_set\n",
    "\n",
    "    added_que_flag=False\n",
    "    for hq in history_list:\n",
    "        for his_tw in hq:\n",
    "            if cal_set_rel(current_tw['nodeset'],his_tw['nodeset'])!=0 and current_tw['name']!=his_tw['name']:\n",
    "                hq.append(copy.deepcopy(current_tw))\n",
    "                print(f\"{his_tw['name']=}\")\n",
    "                added_que_flag=True\n",
    "                break\n",
    "            if added_que_flag:\n",
    "                break\n",
    "    if added_que_flag is False:\n",
    "        temp_hq=[copy.deepcopy(current_tw)]\n",
    "        history_list.append(temp_hq)\n",
    "  \n",
    "    index_count+=1\n",
    "\n",
    "    print( f_path,\"  \",loss_avg,\" count:\",count,\" percentage:\",count/len(edge_list),\" node count:\",len(node_set),\" edge count:\",len(edge_set))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# pred_label={}\n",
    "\n",
    "    \n",
    "# filelist = os.listdir(\"graph_9_23\")\n",
    "# for f in filelist:\n",
    "#     pred_label[f]=0\n",
    "\n",
    "\n",
    "for hl in history_list:\n",
    "    loss_count=0\n",
    "    for hq in hl:\n",
    "        if loss_count==0:\n",
    "            loss_count=(loss_count+1)*(hq['loss']+1)\n",
    "        else:\n",
    "            loss_count=(loss_count)*(hq['loss']+1)\n",
    "    name_list=[]\n",
    "    if loss_count>10000:\n",
    "        name_list=[]\n",
    "        for i in hl:\n",
    "            name_list.append(i['name'])\n",
    "        print(name_list)\n",
    "#         for i in name_list:\n",
    "#             pred_label[i]=1\n",
    "        print(loss_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## h402"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "node_IDF=torch.load(\"node_IDF_9_22_hosts\")\n",
    "\n",
    "# node_set_list=[]\n",
    "history_list_9_23_h402=[]\n",
    "tw_que=[]\n",
    "his_tw={}\n",
    "current_tw={}\n",
    "\n",
    "test_path=\"/home/shahidul2k9/data/optc/test/graph_9_23_h402/\"\n",
    "\n",
    "\n",
    "file_l=os.listdir(test_path)\n",
    "file_l.sort()\n",
    "index_count=0\n",
    "for f_path in (file_l):\n",
    "    f=open(test_path+f_path)\n",
    "    edge_loss_list=[]\n",
    "    edge_list=[]\n",
    "    print('index_count:',index_count)\n",
    "    \n",
    "    for line in f:\n",
    "        l=line.strip()\n",
    "        jdata=eval(l)\n",
    "        edge_loss_list.append(jdata['loss'])\n",
    "        edge_list.append([str(jdata['srcmsg']).split(\"_@\")[-1],str(jdata['dstmsg']).split(\"_@\")[-1]])\n",
    "#         edge_list.append([str(jdata['srcmsg']),str(jdata['dstmsg'])])\n",
    "        \n",
    "#     df_list_9_22.append(pd.DataFrame(edge_loss_list))\n",
    "    count,loss_avg,node_set,edge_set=cal_anomaly_loss(edge_loss_list,edge_list,test_path)\n",
    "    current_tw['name']=f_path\n",
    "    current_tw['loss']=loss_avg\n",
    "    current_tw['index']=index_count\n",
    "    current_tw['nodeset']=node_set\n",
    "\n",
    "    added_que_flag=False\n",
    "    for hq in history_list_9_23_h402:\n",
    "        for his_tw in hq:\n",
    "            if cal_set_rel(current_tw['nodeset'],his_tw['nodeset'])!=0 and current_tw['name']!=his_tw['name']:\n",
    "                hq.append(copy.deepcopy(current_tw))\n",
    "                print(f\"{his_tw['name']=}\")\n",
    "                added_que_flag=True\n",
    "                break\n",
    "            if added_que_flag:\n",
    "                break\n",
    "    if added_que_flag is False:\n",
    "        temp_hq=[copy.deepcopy(current_tw)]\n",
    "        history_list_9_23_h402.append(temp_hq)\n",
    "  \n",
    "    index_count+=1\n",
    "#     node_set_list.append(node_set)\n",
    "    print( f_path,\"  \",loss_avg,\" count:\",count,\" percentage:\",count/len(edge_list),\" node count:\",len(node_set),\" edge count:\",len(edge_set))\n",
    "#     y_data_4_10.append([loss_avg,labels_4_10[f_path],f_path])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_label_h402={}\n",
    "\n",
    "test_path=\"/home/shahidul2k9/data/optc/test/graph_9_23_h402/\"\n",
    "    \n",
    "filelist = os.listdir(test_path)\n",
    "filelist.sort()\n",
    "for f in filelist:\n",
    "    pred_label_h402[test_path+f]=0\n",
    "    \n",
    "test_path=\"/home/shahidul2k9/data/optc/test/graph_9_24_h402/\"\n",
    "    \n",
    "filelist = os.listdir(test_path)\n",
    "filelist.sort()\n",
    "for f in filelist:\n",
    "    pred_label_h402[test_path+f]=0\n",
    "    \n",
    "test_path=\"/home/shahidul2k9/data/optc/test/graph_9_25_h402/\"\n",
    "    \n",
    "filelist = os.listdir(test_path)\n",
    "filelist.sort()\n",
    "for f in filelist:\n",
    "    pred_label_h402[test_path+f]=0\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "for hl in history_list_9_23_h402:\n",
    "    loss_count=0\n",
    "    for hq in hl:\n",
    "        if loss_count==0:\n",
    "            loss_count=(loss_count+1)*(hq['loss']+1)\n",
    "        else:\n",
    "            loss_count=(loss_count)*(hq['loss']+1)\n",
    "    name_list=[]\n",
    "    if loss_count>1000:\n",
    "        name_list=[]\n",
    "        for i in hl:\n",
    "            name_list.append(i['name'])\n",
    "        print(*name_list, sep = \"\\n\")\n",
    "        for i in name_list:\n",
    "            pred_label_h402[\"/home/shahidul2k9/data/optc/test/graph_9_23_h402/\"+i]=1\n",
    "        print(loss_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evalaute\n",
    "y=[]\n",
    "y_pred=[]\n",
    "for i in label_h402:\n",
    "    y.append(label_h402[i])\n",
    "    y_pred.append(pred_label_h402[i])\n",
    "    \n",
    "classifier_evaluation(y,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## h660"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "node_IDF=torch.load(\"node_IDF_9_22_hosts\")\n",
    "\n",
    "# node_set_list=[]\n",
    "history_list_9_23_h660=[]\n",
    "tw_que=[]\n",
    "his_tw={}\n",
    "current_tw={}\n",
    "\n",
    "test_path=\"/home/shahidul2k9/data/optc/test/graph_9_23_h660/\"\n",
    "\n",
    "\n",
    "file_l=os.listdir(test_path)\n",
    "file_l.sort()\n",
    "index_count=0\n",
    "for f_path in (file_l):\n",
    "    f=open(test_path+f_path)\n",
    "    edge_loss_list=[]\n",
    "    edge_list=[]\n",
    "    print('index_count:',index_count)\n",
    "    \n",
    "    for line in f:\n",
    "        l=line.strip()\n",
    "        jdata=eval(l)\n",
    "        edge_loss_list.append(jdata['loss'])\n",
    "        edge_list.append([str(jdata['srcmsg']).split(\"_@\")[-1],str(jdata['dstmsg']).split(\"_@\")[-1]])\n",
    "#         edge_list.append([str(jdata['srcmsg']),str(jdata['dstmsg'])])\n",
    "        \n",
    "#     df_list_9_22.append(pd.DataFrame(edge_loss_list))\n",
    "    count,loss_avg,node_set,edge_set=cal_anomaly_loss(edge_loss_list,edge_list,test_path)\n",
    "    current_tw['name']=f_path\n",
    "    current_tw['loss']=loss_avg\n",
    "    current_tw['index']=index_count\n",
    "    current_tw['nodeset']=node_set\n",
    "\n",
    "    added_que_flag=False\n",
    "    for hq in history_list_9_23_h660:\n",
    "        for his_tw in hq:\n",
    "            if cal_set_rel(current_tw['nodeset'],his_tw['nodeset'])!=0 and current_tw['name']!=his_tw['name']:\n",
    "                hq.append(copy.deepcopy(current_tw))\n",
    "                print(f\"{his_tw['name']=}\")\n",
    "                added_que_flag=True\n",
    "                break\n",
    "            if added_que_flag:\n",
    "                break\n",
    "    if added_que_flag is False:\n",
    "        temp_hq=[copy.deepcopy(current_tw)]\n",
    "        history_list_9_23_h660.append(temp_hq)\n",
    "  \n",
    "    index_count+=1\n",
    "#     node_set_list.append(node_set)\n",
    "    print( f_path,\"  \",loss_avg,\" count:\",count,\" percentage:\",count/len(edge_list),\" node count:\",len(node_set),\" edge count:\",len(edge_set))\n",
    "#     y_data_4_10.append([loss_avg,labels_4_10[f_path],f_path])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_label_h660={}\n",
    "\n",
    "test_path=\"/home/shahidul2k9/data/optc/test/graph_9_23_h660/\"\n",
    "    \n",
    "filelist = os.listdir(test_path)\n",
    "filelist.sort()\n",
    "for f in filelist:\n",
    "    pred_label_h660[test_path+f]=0\n",
    "    \n",
    "test_path=\"/home/shahidul2k9/data/optc/test/graph_9_24_h660/\"\n",
    "    \n",
    "filelist = os.listdir(test_path)\n",
    "filelist.sort()\n",
    "for f in filelist:\n",
    "    pred_label_h660[test_path+f]=0\n",
    "    \n",
    "test_path=\"/home/shahidul2k9/data/optc/test/graph_9_25_h660/\"\n",
    "    \n",
    "filelist = os.listdir(test_path)\n",
    "filelist.sort()\n",
    "for f in filelist:\n",
    "    pred_label_h660[test_path+f]=0\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "for hl in history_list_9_23_h660:\n",
    "    loss_count=0\n",
    "    for hq in hl:\n",
    "        if loss_count==0:\n",
    "            loss_count=(loss_count+1)*(hq['loss']+1)\n",
    "        else:\n",
    "            loss_count=(loss_count)*(hq['loss']+1)\n",
    "    name_list=[]\n",
    "    if loss_count>100:\n",
    "        name_list=[]\n",
    "        for i in hl:\n",
    "            name_list.append(i['name'])\n",
    "        print(*name_list, sep = \"\\n\")\n",
    "        for i in name_list:\n",
    "            pred_label_h660[\"/home/shahidul2k9/data/optc/test/graph_9_23_h660/\"+i]=1\n",
    "        print(loss_count)\n",
    "        \n",
    "        \n",
    "# evalute\n",
    "y=[]\n",
    "y_pred=[]\n",
    "for i in label_h660:\n",
    "    y.append(label_h660[i])\n",
    "    y_pred.append(pred_label_h660[i])\n",
    "    \n",
    "classifier_evaluation(y,y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## h501"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "node_IDF=torch.load(\"node_IDF_9_22_hosts\")\n",
    "\n",
    "# node_set_list=[]\n",
    "history_list_9_24_h501=[]\n",
    "tw_que=[]\n",
    "his_tw={}\n",
    "current_tw={}\n",
    "\n",
    "test_path=\"/home/shahidul2k9/data/optc/test/graph_9_24_h501/\"\n",
    "\n",
    "\n",
    "file_l=os.listdir(test_path)\n",
    "file_l.sort()\n",
    "index_count=0\n",
    "for f_path in (file_l):\n",
    "    f=open(test_path+f_path)\n",
    "    edge_loss_list=[]\n",
    "    edge_list=[]\n",
    "    print('index_count:',index_count)\n",
    "    \n",
    "    for line in f:\n",
    "        l=line.strip()\n",
    "        jdata=eval(l)\n",
    "        edge_loss_list.append(jdata['loss'])\n",
    "        edge_list.append([str(jdata['srcmsg']).split(\"_@\")[-1],str(jdata['dstmsg']).split(\"_@\")[-1]])\n",
    "#         edge_list.append([str(jdata['srcmsg']),str(jdata['dstmsg'])])\n",
    "        \n",
    "#     df_list_9_22.append(pd.DataFrame(edge_loss_list))\n",
    "    count,loss_avg,node_set,edge_set=cal_anomaly_loss(edge_loss_list,edge_list,test_path)\n",
    "    current_tw['name']=f_path\n",
    "    current_tw['loss']=loss_avg\n",
    "    current_tw['index']=index_count\n",
    "    current_tw['nodeset']=node_set\n",
    "\n",
    "    added_que_flag=False\n",
    "    for hq in history_list_9_24_h501:\n",
    "        for his_tw in hq:\n",
    "            if cal_set_rel(current_tw['nodeset'],his_tw['nodeset'])!=0 and current_tw['name']!=his_tw['name']:\n",
    "                hq.append(copy.deepcopy(current_tw))\n",
    "                print(f\"{his_tw['name']=}\")\n",
    "                added_que_flag=True\n",
    "                break\n",
    "            if added_que_flag:\n",
    "                break\n",
    "    if added_que_flag is False:\n",
    "        temp_hq=[copy.deepcopy(current_tw)]\n",
    "        history_list_9_24_h501.append(temp_hq)\n",
    "  \n",
    "    index_count+=1\n",
    "#     node_set_list.append(node_set)\n",
    "    print( f_path,\"  \",loss_avg,\" count:\",count,\" percentage:\",count/len(edge_list),\" node count:\",len(node_set),\" edge count:\",len(edge_set))\n",
    "#     y_data_4_10.append([loss_avg,labels_4_10[f_path],f_path])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_label_h501={}\n",
    "\n",
    "test_path=\"/home/shahidul2k9/data/optc/test/graph_9_23_h501/\"\n",
    "    \n",
    "filelist = os.listdir(test_path)\n",
    "filelist.sort()\n",
    "for f in filelist:\n",
    "    pred_label_h501[test_path+f]=0\n",
    "    \n",
    "test_path=\"/home/shahidul2k9/data/optc/test/graph_9_24_h501/\"\n",
    "    \n",
    "filelist = os.listdir(test_path)\n",
    "filelist.sort()\n",
    "for f in filelist:\n",
    "    pred_label_h501[test_path+f]=0\n",
    "    \n",
    "test_path=\"/home/shahidul2k9/data/optc/test/graph_9_25_h501/\"\n",
    "    \n",
    "filelist = os.listdir(test_path)\n",
    "filelist.sort()\n",
    "for f in filelist:\n",
    "    pred_label_h501[test_path+f]=0\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "for hl in history_list_9_24_h501:\n",
    "    loss_count=0\n",
    "    for hq in hl:\n",
    "        if loss_count==0:\n",
    "            loss_count=(loss_count+1)*(hq['loss']+1)\n",
    "        else:\n",
    "            loss_count=(loss_count)*(hq['loss']+1)\n",
    "    name_list=[]\n",
    "    if loss_count>200:\n",
    "        name_list=[]\n",
    "        for i in hl:\n",
    "            name_list.append(i['name'])\n",
    "        print(*name_list, sep = \"\\n\")\n",
    "        for i in name_list:\n",
    "            pred_label_h501[\"/home/shahidul2k9/data/optc/test/graph_9_24_h501/\"+i]=1\n",
    "        print(loss_count)\n",
    "        \n",
    "        \n",
    "# evaluate\n",
    "y=[]\n",
    "y_pred=[]\n",
    "for i in label_h501:\n",
    "    y.append(label_h501[i])\n",
    "    y_pred.append(pred_label_h501[i])\n",
    "    \n",
    "classifier_evaluation(y,y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## h051"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# node_IDF=torch.load(\"node_IDF_4_6\")\n",
    "node_IDF=torch.load(\"node_IDF_9_22_hosts\")\n",
    "\n",
    "# node_set_list=[]\n",
    "history_list_9_25_h051=[]\n",
    "tw_que=[]\n",
    "his_tw={}\n",
    "current_tw={}\n",
    "\n",
    "test_path=\"/home/shahidul2k9/data/optc/test/graph_9_25_h051/\"\n",
    "\n",
    "\n",
    "file_l=os.listdir(test_path)\n",
    "file_l.sort()\n",
    "index_count=0\n",
    "for f_path in (file_l):\n",
    "    f=open(test_path+f_path)\n",
    "    edge_loss_list=[]\n",
    "    edge_list=[]\n",
    "    print('index_count:',index_count)\n",
    "    \n",
    "    for line in f:\n",
    "        l=line.strip()\n",
    "        jdata=eval(l)\n",
    "        edge_loss_list.append(jdata['loss'])\n",
    "        edge_list.append([str(jdata['srcmsg']).split(\"_@\")[-1],str(jdata['dstmsg']).split(\"_@\")[-1]])\n",
    "#         edge_list.append([str(jdata['srcmsg']),str(jdata['dstmsg'])])\n",
    "        \n",
    "#     df_list_9_22.append(pd.DataFrame(edge_loss_list))\n",
    "    count,loss_avg,node_set,edge_set=cal_anomaly_loss(edge_loss_list,edge_list,test_path)\n",
    "    current_tw['name']=f_path\n",
    "    current_tw['loss']=loss_avg\n",
    "    current_tw['index']=index_count\n",
    "    current_tw['nodeset']=node_set\n",
    "\n",
    "    added_que_flag=False\n",
    "    for hq in history_list_9_25_h051:\n",
    "        for his_tw in hq:\n",
    "            if cal_set_rel(current_tw['nodeset'],his_tw['nodeset'])!=0 and current_tw['name']!=his_tw['name']:\n",
    "                hq.append(copy.deepcopy(current_tw))\n",
    "                print(f\"{his_tw['name']=}\")\n",
    "                added_que_flag=True\n",
    "                break\n",
    "            if added_que_flag:\n",
    "                break\n",
    "    if added_que_flag is False:\n",
    "        temp_hq=[copy.deepcopy(current_tw)]\n",
    "        history_list_9_25_h051.append(temp_hq)\n",
    "  \n",
    "    index_count+=1\n",
    "#     node_set_list.append(node_set)\n",
    "    print( f_path,\"  \",loss_avg,\" count:\",count,\" percentage:\",count/len(edge_list),\" node count:\",len(node_set),\" edge count:\",len(edge_set))\n",
    "#     y_data_4_10.append([loss_avg,labels_4_10[f_path],f_path])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pred_label_h051={}\n",
    "\n",
    "test_path=\"/home/shahidul2k9/data/optc/test/graph_9_23_h051/\"\n",
    "    \n",
    "filelist = os.listdir(test_path)\n",
    "filelist.sort()\n",
    "for f in filelist:\n",
    "    pred_label_h051[test_path+f]=0\n",
    "    \n",
    "test_path=\"/home/shahidul2k9/data/optc/test/graph_9_24_h051/\"\n",
    "    \n",
    "filelist = os.listdir(test_path)\n",
    "filelist.sort()\n",
    "for f in filelist:\n",
    "    pred_label_h051[test_path+f]=0\n",
    "    \n",
    "test_path=\"/home/shahidul2k9/data/optc/test/graph_9_25_h051/\"\n",
    "    \n",
    "filelist = os.listdir(test_path)\n",
    "filelist.sort()\n",
    "for f in filelist:\n",
    "    pred_label_h051[test_path+f]=0\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "for hl in history_list_9_25_h051:\n",
    "    loss_count=0\n",
    "    for hq in hl:\n",
    "        if loss_count==0:\n",
    "            loss_count=(loss_count+1)*(hq['loss']+1)\n",
    "        else:\n",
    "            loss_count=(loss_count)*(hq['loss']+1)\n",
    "    name_list=[]\n",
    "    if loss_count>2000:\n",
    "        name_list=[]\n",
    "        for i in hl:\n",
    "            name_list.append(i['name'])\n",
    "\n",
    "        print(*name_list, sep = \"\\n\")\n",
    "        for i in name_list:\n",
    "            pred_label_h051[\"/home/shahidul2k9/data/optc/test/graph_9_25_h051/\"+i]=1\n",
    "        print(loss_count)\n",
    "        \n",
    "        \n",
    "# evaluate\n",
    "y=[]\n",
    "y_pred=[]\n",
    "for i in label_h051:\n",
    "    y.append(label_h051[i])\n",
    "    y_pred.append(pred_label_h051[i])\n",
    "    \n",
    "classifier_evaluation(y,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## h207"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "node_IDF=torch.load(\"node_IDF_9_22_hosts\")\n",
    "\n",
    "# node_set_list=[]\n",
    "history_list_9_25_h207=[]\n",
    "tw_que=[]\n",
    "his_tw={}\n",
    "current_tw={}\n",
    "\n",
    "test_path=\"/home/shahidul2k9/data/optc/test/graph_9_25_h207/\"\n",
    "\n",
    "\n",
    "file_l=os.listdir(test_path)\n",
    "file_l.sort()\n",
    "index_count=0\n",
    "for f_path in (file_l):\n",
    "    f=open(test_path+f_path)\n",
    "    edge_loss_list=[]\n",
    "    edge_list=[]\n",
    "    print('index_count:',index_count)\n",
    "    \n",
    "    for line in f:\n",
    "        l=line.strip()\n",
    "        jdata=eval(l)\n",
    "        edge_loss_list.append(jdata['loss'])\n",
    "        edge_list.append([str(jdata['srcmsg']).split(\"_@\")[-1],str(jdata['dstmsg']).split(\"_@\")[-1]])\n",
    "#         edge_list.append([str(jdata['srcmsg']),str(jdata['dstmsg'])])\n",
    "        \n",
    "#     df_list_9_22.append(pd.DataFrame(edge_loss_list))\n",
    "    count,loss_avg,node_set,edge_set=cal_anomaly_loss(edge_loss_list,edge_list,test_path)\n",
    "    current_tw['name']=f_path\n",
    "    current_tw['loss']=loss_avg\n",
    "    current_tw['index']=index_count\n",
    "    current_tw['nodeset']=node_set\n",
    "\n",
    "    added_que_flag=False\n",
    "    for hq in history_list_9_25_h207:\n",
    "        for his_tw in hq:\n",
    "            if cal_set_rel(current_tw['nodeset'],his_tw['nodeset'])!=0 and current_tw['name']!=his_tw['name']:\n",
    "                hq.append(copy.deepcopy(current_tw))\n",
    "                print(f\"{his_tw['name']=}\")\n",
    "                added_que_flag=True\n",
    "                break\n",
    "            if added_que_flag:\n",
    "                break\n",
    "    if added_que_flag is False:\n",
    "        temp_hq=[copy.deepcopy(current_tw)]\n",
    "        history_list_9_25_h207.append(temp_hq)\n",
    "  \n",
    "    index_count+=1\n",
    "\n",
    "    print( f_path,\"  \",loss_avg,\" count:\",count,\" percentage:\",count/len(edge_list),\" node count:\",len(node_set),\" edge count:\",len(edge_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_label_h207={}\n",
    "\n",
    "test_path=\"/home/shahidul2k9/data/optc/test/graph_9_23_h207/\"\n",
    "    \n",
    "filelist = os.listdir(test_path)\n",
    "filelist.sort()\n",
    "for f in filelist:\n",
    "    pred_label_h207[test_path+f]=0\n",
    "    \n",
    "test_path=\"/home/shahidul2k9/data/optc/test/graph_9_24_h207/\"\n",
    "    \n",
    "filelist = os.listdir(test_path)\n",
    "filelist.sort()\n",
    "for f in filelist:\n",
    "    pred_label_h207[test_path+f]=0\n",
    "    \n",
    "test_path=\"/home/shahidul2k9/data/optc/test/graph_9_25_h207/\"\n",
    "    \n",
    "filelist = os.listdir(test_path)\n",
    "filelist.sort()\n",
    "for f in filelist:\n",
    "    pred_label_h207[test_path+f]=0\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "for hl in history_list_9_25_h207:\n",
    "    loss_count=0\n",
    "    for hq in hl:\n",
    "        if loss_count==0:\n",
    "            loss_count=(loss_count+1)*(hq['loss']+1)\n",
    "        else:\n",
    "            loss_count=(loss_count)*(hq['loss']+1)\n",
    "    name_list=[]\n",
    "    if loss_count>2000:\n",
    "        name_list=[]\n",
    "        for i in hl:\n",
    "            name_list.append(i['name'])\n",
    "\n",
    "        print(*name_list, sep = \"\\n\")\n",
    "        for i in name_list:\n",
    "            pred_label_h207[\"/home/shahidul2k9/data/optc/test/graph_9_25_h207/\"+i]=1\n",
    "        print(loss_count)\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate\n",
    "y=[]\n",
    "y_pred=[]\n",
    "for i in label_h207:\n",
    "    y.append(label_h207[i])\n",
    "    y_pred.append(pred_label_h207[i])\n",
    "    \n",
    "    \n",
    "classifier_evaluation(y,y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overall evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "y=[]\n",
    "y_pred=[]\n",
    "\n",
    "for i in label_h201:\n",
    "    y.append(label_h201[i])\n",
    "    y_pred.append(pred_label_h201[i])\n",
    "\n",
    "for i in label_h402:\n",
    "    y.append(label_h402[i])\n",
    "    y_pred.append(pred_label_h402[i])\n",
    "\n",
    "for i in label_h660:\n",
    "    y.append(label_h660[i])\n",
    "    y_pred.append(pred_label_h660[i])\n",
    "\n",
    "for i in label_h501:\n",
    "    y.append(label_h501[i])\n",
    "    y_pred.append(pred_label_h501[i])\n",
    "\n",
    "for i in label_h051:\n",
    "    y.append(label_h051[i])\n",
    "    y_pred.append(pred_label_h051[i])\n",
    "\n",
    "for i in label_h207:\n",
    "    y.append(label_h207[i])\n",
    "    y_pred.append(pred_label_h207[i])\n",
    "    \n",
    "    \n",
    "classifier_evaluation(y,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Attack investigation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "replace_dic={\n",
    "    '.pyc':'*.pyc',\n",
    "#     '.dll':'*.dll',\n",
    "#     '.DLL':'*.DLL',\n",
    "}\n",
    "\n",
    "def replace_path_name(path_name):\n",
    "    for i in replace_dic:\n",
    "        if i in path_name:\n",
    "            return replace_dic[i]\n",
    "    if '->' in path_name:\n",
    "        if 'outbound' in path_name:\n",
    "            msg=re.findall(\"->(.*?):\",path_name)[0]\n",
    "            return msg\n",
    "        elif 'inbound' in path_name:\n",
    "            msg=re.findall(\"#(.*?):\",path_name)[0]\n",
    "            return msg\n",
    "    return path_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count the number of attack edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "label_df=pd.read_csv(\"./labels.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### h201"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes_attack_h201={}\n",
    "attack_node_set_h201=set()\n",
    "edges_attack_hashset_h201=set()\n",
    "h201_df_label=label_df[label_df['hostname']=='SysClient0201.systemia.com']\n",
    "\n",
    "for idx,row in h201_df_label.iterrows():\n",
    "    \n",
    "    srcflag=False\n",
    "    dstflag=False\n",
    "    if row['objectID'] in node_uuid2path:\n",
    "        nodes_attack_h201[row['objectID']]=replace_path_name(node_uuid2path[row['objectID']])\n",
    "\n",
    "        dstflag=True\n",
    "    if row['actorID'] in node_uuid2path:\n",
    "        nodes_attack_h201[row['actorID']]=replace_path_name(node_uuid2path[row['actorID']])\n",
    "        \n",
    "        srcflag=True\n",
    "    if srcflag and dstflag and row['action'] in rel2id:    \n",
    "        temp_edge=replace_path_name(node_uuid2path[row['actorID']])+','+replace_path_name(node_uuid2path[row['objectID']])+','+str(datetime_to_timestamp_US(row['timestamp']))\n",
    "        edges_attack_hashset_h201.add(hashgen(temp_edge))\n",
    "\n",
    "for i in nodes_attack_h201:\n",
    "    attack_node_set_h201.add(nodes_attack_h201[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### h402"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes_attack_h402={}\n",
    "attack_node_set_h402=set()\n",
    "edges_attack_hashset_h402=set()\n",
    "h402_df_label=label_df[label_df['hostname']=='SysClient0402.systemia.com']\n",
    "\n",
    "for idx,row in h402_df_label.iterrows():\n",
    "    \n",
    "    srcflag=False\n",
    "    dstflag=False\n",
    "    if row['objectID'] in node_uuid2path:\n",
    "        nodes_attack_h402[row['objectID']]=replace_path_name(node_uuid2path[row['objectID']])\n",
    "        dstflag=True\n",
    "    if row['actorID'] in node_uuid2path:\n",
    "        nodes_attack_h402[row['actorID']]=replace_path_name(node_uuid2path[row['actorID']])\n",
    "        \n",
    "        srcflag=True\n",
    "    if srcflag and dstflag and row['action'] in rel2id:    \n",
    "        temp_edge=replace_path_name(node_uuid2path[row['actorID']])+','+replace_path_name(node_uuid2path[row['objectID']])+','+str(datetime_to_timestamp_US(row['timestamp']))\n",
    "        edges_attack_hashset_h402.add(hashgen(temp_edge))\n",
    "\n",
    "for i in nodes_attack_h402:\n",
    "    attack_node_set_h402.add(nodes_attack_h402[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(edges_attack_hashset_h402)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### h660"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes_attack_h660={}\n",
    "attack_node_set_h660=set()\n",
    "edges_attack_hashset_h660=set()\n",
    "h660_df_label=label_df[label_df['hostname']=='SysClient0660.systemia.com']\n",
    "\n",
    "for idx,row in h660_df_label.iterrows():\n",
    "    \n",
    "    srcflag=False\n",
    "    dstflag=False\n",
    "    if row['objectID'] in node_uuid2path:\n",
    "        nodes_attack_h660[row['objectID']]=replace_path_name(node_uuid2path[row['objectID']])\n",
    "        dstflag=True\n",
    "    if row['actorID'] in node_uuid2path:\n",
    "        nodes_attack_h660[row['actorID']]=replace_path_name(node_uuid2path[row['actorID']])\n",
    "        srcflag=True\n",
    "    if srcflag and dstflag and row['action'] in rel2id:    \n",
    "        temp_edge=replace_path_name(node_uuid2path[row['actorID']])+','+replace_path_name(node_uuid2path[row['objectID']])+','+str(datetime_to_timestamp_US(row['timestamp']))\n",
    "        edges_attack_hashset_h660.add(hashgen(temp_edge))\n",
    "\n",
    "for i in nodes_attack_h660:\n",
    "    attack_node_set_h660.add(nodes_attack_h660[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(edges_attack_hashset_h660)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### h501"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes_attack_h501={}\n",
    "attack_node_set_h501=set()\n",
    "edges_attack_hashset_h501=set()\n",
    "h501_df_label=label_df[label_df['hostname']=='SysClient0501.systemia.com']\n",
    "\n",
    "for idx,row in h501_df_label.iterrows():\n",
    "    \n",
    "    srcflag=False\n",
    "    dstflag=False\n",
    "    if row['objectID'] in node_uuid2path:\n",
    "        nodes_attack_h501[row['objectID']]=replace_path_name(node_uuid2path[row['objectID']])\n",
    "        dstflag=True\n",
    "    if row['actorID'] in node_uuid2path:\n",
    "        nodes_attack_h501[row['actorID']]=replace_path_name(node_uuid2path[row['actorID']])\n",
    "        srcflag=True\n",
    "    if srcflag and dstflag and row['action'] in rel2id:    \n",
    "        temp_edge=replace_path_name(node_uuid2path[row['actorID']])+','+replace_path_name(node_uuid2path[row['objectID']])+','+str(datetime_to_timestamp_US(row['timestamp']))\n",
    "        edges_attack_hashset_h501.add(hashgen(temp_edge))\n",
    "\n",
    "for i in nodes_attack_h501:\n",
    "    attack_node_set_h501.add(nodes_attack_h501[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### h051"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes_attack_h051={}\n",
    "attack_node_set_h051=set()\n",
    "edges_attack_hashset_h051=set()\n",
    "h051_df_label=label_df[label_df['hostname']=='SysClient0051.systemia.com']\n",
    "\n",
    "for idx,row in h051_df_label.iterrows():\n",
    "    \n",
    "    srcflag=False\n",
    "    dstflag=False\n",
    "    if row['objectID'] in node_uuid2path:\n",
    "        nodes_attack_h051[row['objectID']]=replace_path_name(node_uuid2path[row['objectID']])\n",
    "        dstflag=True\n",
    "    if row['actorID'] in node_uuid2path:\n",
    "        nodes_attack_h051[row['actorID']]=replace_path_name(node_uuid2path[row['actorID']])\n",
    "        srcflag=True\n",
    "    if srcflag and dstflag and row['action'] in rel2id:    \n",
    "        temp_edge=replace_path_name(node_uuid2path[row['actorID']])+','+replace_path_name(node_uuid2path[row['objectID']])+','+str(datetime_to_timestamp_US(row['timestamp']))\n",
    "        edges_attack_hashset_h051.add(hashgen(temp_edge))\n",
    "\n",
    "for i in nodes_attack_h051:\n",
    "    attack_node_set_h051.add(nodes_attack_h051[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## h201"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attack_list=[\n",
    "    '/home/shahidul2k9/data/optc/test/graph_9_23_h201/2019-09-23 09:52:00.187~2019-09-23 10:06:52.508.txt',\n",
    "  '/home/shahidul2k9/data/optc/test/graph_9_23_h201/2019-09-23 11:23:44.136~2019-09-23 11:38:30.698.txt',\n",
    " '/home/shahidul2k9/data/optc/test/graph_9_23_h201/2019-09-23 11:38:40.698~2019-09-23 11:53:39.57.txt',\n",
    " \n",
    " '/home/shahidul2k9/data/optc/test/graph_9_23_h201/2019-09-23 12:38:24.95~2019-09-23 12:54:14.286.txt',\n",
    " '/home/shahidul2k9/data/optc/test/graph_9_23_h201/2019-09-23 12:55:28.286~2019-09-23 13:09:50.95.txt',\n",
    " '/home/shahidul2k9/data/optc/test/graph_9_23_h201/2019-09-23 13:10:24.95~2019-09-23 13:24:56.43.txt',\n",
    "    '/home/shahidul2k9/data/optc/test/graph_9_23_h201/2019-09-23 15:24:48.426~2019-09-23 15:42:02.967.txt',\n",
    "            ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timezone\n",
    "attack_dic=[]\n",
    "for a in attack_list:\n",
    "    temp_dic={}\n",
    "    aa=a.split(\"/\")[-1]\n",
    "    sp=aa.split('~')\n",
    "    start_timestamp=sp[0].split('.')[0]\n",
    "    end_timestamp=sp[1].split('.')[0]\n",
    "    temp_dic['start']=datetime_to_timestamp_US(start_timestamp)\n",
    "    temp_dic['end']=datetime_to_timestamp_US(end_timestamp)\n",
    "    temp_dic['file']=a\n",
    "    attack_dic.append(temp_dic)\n",
    "    print(temp_dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "original_edges_count=0\n",
    "hash2msg={}\n",
    "graphs=[]\n",
    "gg=nx.DiGraph()\n",
    "count=0\n",
    "# file_list=os.listdir(\"./test_day_data4_10_emb100/\")\n",
    "for path in tqdm(attack_list):\n",
    "#     print(path)\n",
    "    if \".txt\" in path:\n",
    "        line_count=0\n",
    "        node_set=set()\n",
    "        tempg=nx.DiGraph()\n",
    "        f=open(path,\"r\")       \n",
    "        edge_list=[]\n",
    "        for line in f:\n",
    "            count+=1\n",
    "            l=line.strip()\n",
    "            jdata=eval(l)\n",
    "#             temp_key=jdata['srcmsg']+jdata['dstmsg']+jdata['edge_type']\n",
    "#             if temp_key in train_edge_set:\n",
    "#                 jdata['loss']=(jdata['loss']-train_edge_set[temp_key]) if jdata['loss']>=train_edge_set[temp_key] else 0  \n",
    "#             jdata['loss']=abs(jdata['loss']-train_edge_set[temp_key])  if temp_key in train_edge_set else jdata['loss']\n",
    "            edge_list.append(jdata)\n",
    "            \n",
    "        edge_list = sorted(edge_list, key=lambda x:x['loss'],reverse=True) \n",
    "        original_edges_count+=len(edge_list)\n",
    "        \n",
    "        loss_list=[]\n",
    "        for i in edge_list:\n",
    "            loss_list.append(i['loss'])\n",
    "        loss_mean=mean(loss_list)\n",
    "        loss_std=std(loss_list)\n",
    "        print(loss_mean)\n",
    "        print(loss_std)\n",
    "        thr=loss_mean+1.5*loss_std\n",
    "#         thr=-99\n",
    "        print(\"thr:\",thr)\n",
    "        for e in edge_list:\n",
    "            if e['loss']>thr:    \n",
    "#             if True:  \n",
    "#                 if \"'/home/admin/profile'\" in e['srcmsg'] or \" '/home/admin/profile'\" in e['dstmsg']:\n",
    "#                     print(e['srcmsg'])\n",
    "#                     print(e['dstmsg'])\n",
    "                tempg.add_edge(str(hashgen(replace_path_name(e['srcmsg']))),str(hashgen(replace_path_name(e['dstmsg']))))\n",
    "                gg.add_edge(str(hashgen(replace_path_name(e['srcmsg']))),str(hashgen(replace_path_name(e['dstmsg']))),loss=e['loss'],srcmsg=e['srcmsg'],dstmsg=e['dstmsg'],edge_type=e['edge_type'],time=e['time'])\n",
    "                \n",
    "                hash2msg[str(hashgen(replace_path_name(e['srcmsg'])))]=replace_path_name(e['srcmsg'])\n",
    "                hash2msg[str(hashgen(replace_path_name(e['dstmsg'])))]=replace_path_name(e['dstmsg'])\n",
    "\n",
    "#                 gg.add_edge(e['srcnode'],e['dstnode'],loss=e['loss'],srcmsg=e['srcmsg'],dstmsg=e['dstmsg'],edge_type=e['edge_type'],time=e['time'])\n",
    "        print(path)\n",
    "        print(\"tempg edges:\",len(tempg.edges))\n",
    "        print(\"tempg nodes:\",len(tempg.nodes))\n",
    "        print(\"tempg weakly components:\",nx.number_weakly_connected_components(tempg))\n",
    "        \n",
    "        print(\"gg edges:\",len(gg.edges))\n",
    "        print(\"gg nodes:\",len(gg.nodes))\n",
    "        print(\"gg weakly components:\",nx.number_weakly_connected_components(gg))\n",
    "        print(f\"{original_edges_count=}\")\n",
    "#                 gg.add_edge(e['srcnode'],e['dstnode'],loss=e['loss'],srcmsg=e['srcmsg'],dstmsg=e['dstmsg'],edge_type=e['edge_type'],time=e['time'])\n",
    "#         print(path,\" line_count:\",line_count,\"  nodes count:\",len(node_set))\n",
    "#         print(path,\" line_count:\",line_count,\"  nodes count:\",len(node_set))\n",
    "                \n",
    "                \n",
    "                #         graphs.append(g)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(hash2msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the best partition\n",
    "import datetime\n",
    "import community as community_louvain\n",
    "starttime = datetime.datetime.now()\n",
    "#long running\n",
    "partition = community_louvain.best_partition(gg.to_undirected())\n",
    "#do something other\n",
    "endtime = datetime.datetime.now()\n",
    "print(\"Finished the computation of community discovery. Execution time:{:d}\".format((endtime - starttime).seconds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "communities={}\n",
    "max_partition=0\n",
    "for i in partition:\n",
    "    if partition[i]>max_partition:\n",
    "        max_partition=partition[i]\n",
    "        \n",
    "for i in range(max_partition+1):\n",
    "    communities[i]=nx.DiGraph()\n",
    "for e in gg.edges:\n",
    "#     if partition[e[0]]==partition[e[1]]:\n",
    "    communities[partition[e[0]]].add_edge(e[0],e[1])\n",
    "    communities[partition[e[1]]].add_edge(e[0],e[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attack_edge_flag(msg):\n",
    "    attack_edge_type=[\n",
    "            '142.20.56.204',\n",
    "        'lsass.exe',\n",
    "        '142.20.61.130',\n",
    "        '132.197.158.98',\n",
    "        'Credentials',\n",
    "    ]\n",
    "    attack_edge_type=attack_node_set_h201\n",
    "    flag=False\n",
    "    for i in attack_edge_type:\n",
    "        if i in msg:\n",
    "            flag=True\n",
    "    return flag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_time_window(edge,tw_dic):\n",
    "    for t in tw_dic:\n",
    "#         print(t['start'])\n",
    "        if edge['time']>=t['start'] and edge['time']<=t['end']:\n",
    "#             print(t['file'])\n",
    "            return t['file']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "max_edge_count=-99\n",
    "max_index=-99\n",
    "max_node_count=0\n",
    "\n",
    "graph_index=0\n",
    "for c in communities:\n",
    "    file_set=set()\n",
    "    if len(communities[c].edges)>max_edge_count:\n",
    "        max_edge_count=len(communities[c].edges)\n",
    "        max_index=graph_index\n",
    "        max_node_count=len(communities[c].nodes)\n",
    "    for e in communities[c].edges:    \n",
    "        try:\n",
    "            temp_edge=gg.edges[e]\n",
    "            file_set.add(find_time_window(temp_edge,attack_dic))\n",
    "        except:\n",
    "            pass    \n",
    "    print(f\"{graph_index=}\")\n",
    "    print(f\"file_set：\")\n",
    "    print(*file_set, sep = \"\\n\")\n",
    "    print(f\"{file_set=}\")\n",
    "    graph_index+=1\n",
    "    \n",
    "print(f\"{max_index=}\")\n",
    "print(f\"{max_edge_count=}\")\n",
    "print(f\"{max_node_count=}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_sub_list={\n",
    "    '/home/shahidul2k9/data/optc/test/graph_9_23_h201/2019-09-23 12:55:28.286~2019-09-23 13:09:50.95.txt', \n",
    "    '/home/shahidul2k9/data/optc/test/graph_9_23_h201/2019-09-23 13:10:24.95~2019-09-23 13:24:56.43.txt', \n",
    "    '/home/shahidul2k9/data/optc/test/graph_9_23_h201/2019-09-23 12:38:24.95~2019-09-23 12:54:14.286.txt', \n",
    "    '/home/shahidul2k9/data/optc/test/graph_9_23_h201/2019-09-23 11:38:40.698~2019-09-23 11:53:39.57.txt', \n",
    "    '/home/shahidul2k9/data/optc/test/graph_9_23_h201/2019-09-23 11:23:44.136~2019-09-23 11:38:30.698.txt'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "original_edges_count=0\n",
    "\n",
    "for path in tqdm(file_sub_list):\n",
    "\n",
    "    if \".txt\" in path:\n",
    "        line_count=0\n",
    "        node_set=set()\n",
    "        tempg=nx.DiGraph()\n",
    "        f=open(path,\"r\")       \n",
    "        edge_list=[]\n",
    "        for line in f:\n",
    "            edge_list.append(line)\n",
    "\n",
    "        original_edges_count+=len(edge_list)\n",
    "\n",
    "        print(f\"{original_edges_count=}\")\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from graphviz import Digraph\n",
    "\n",
    "\n",
    "graph_index=0\n",
    "\n",
    "total_nodes=0\n",
    "total_edges=0\n",
    "attck_nodes_set_visual=set()\n",
    "for c in communities:\n",
    "    dot = Digraph(name=\"MyPicture\", comment=\"the test\", format=\"pdf\")\n",
    "    dot.graph_attr['rankdir'] = 'LR'\n",
    "    \n",
    "    total_nodes+=len(communities[c].nodes)\n",
    "    total_edges+=len(communities[c].edges)\n",
    "    \n",
    "    subgraph_loss_sum=0\n",
    "    attack_node_count=0\n",
    "    attack_edge_count=0\n",
    "    for e in communities[c].edges:\n",
    "        try:\n",
    "            temp_edge=gg.edges[e]\n",
    "            srcnode=e['srcnode']\n",
    "            dstnode=e['dstnode']\n",
    "        except:\n",
    "            pass        \n",
    "\n",
    "        if True:\n",
    "            subgraph_loss_sum+=temp_edge['loss']\n",
    "   \n",
    "\n",
    "            if \"'subject': '\" in temp_edge['srcmsg']:\n",
    "                src_shape='box'\n",
    "            elif \"'file': '\" in temp_edge['srcmsg']:\n",
    "                src_shape='oval'\n",
    "            elif \"'netflow': '\" in temp_edge['srcmsg']:\n",
    "                src_shape='diamond'\n",
    "                \n",
    "            src_shape='box'\n",
    "            if attack_edge_flag(temp_edge['srcmsg']):\n",
    "                src_node_color='red'\n",
    "                total_nodes+=1\n",
    "            else:\n",
    "                src_node_color='blue'\n",
    "                \n",
    "            src_node_color='blue'\n",
    "            \n",
    "            dot.node( name=str(hashgen(replace_path_name(temp_edge['srcmsg']))),\n",
    "                     label=str(replace_path_name(temp_edge['srcmsg'])+'\\t partition:'+str(partition[str(hashgen(replace_path_name(temp_edge['srcmsg'])))])), \n",
    "                     color=src_node_color,\n",
    "                     shape = src_shape)\n",
    "\n",
    "\n",
    "            if \"'subject': '\" in temp_edge['dstmsg']:\n",
    "                dst_shape='box'\n",
    "            elif \"'file': '\" in temp_edge['dstmsg']:\n",
    "                dst_shape='oval'\n",
    "            elif \"'netflow': '\" in temp_edge['dstmsg']:\n",
    "                dst_shape='diamond'\n",
    "\n",
    "                    \n",
    "            if \"->\" in temp_edge['dstmsg']:\n",
    "                dst_shape='diamond'\n",
    "            else:\n",
    "                dst_shape='oval'\n",
    "                \n",
    "                \n",
    "            if attack_edge_flag(temp_edge['dstmsg']):\n",
    "                dst_node_color='red'\n",
    "                total_nodes+=1\n",
    "            else:\n",
    "                dst_node_color='blue'\n",
    "            dst_node_color='blue'\n",
    "            \n",
    "            \n",
    "            dot.node( name=str(hashgen(replace_path_name(temp_edge['dstmsg']))),\n",
    "                     label=str(replace_path_name(temp_edge['dstmsg'])+'\\t partition:'+str(partition[str(hashgen(replace_path_name(temp_edge['dstmsg'])))])), \n",
    "                     color=dst_node_color,\n",
    "                     shape = dst_shape)\n",
    "\n",
    "\n",
    "    #         edgeindex=tensor_find(test_data.msg[i][16:-16],1)\n",
    "\n",
    "    \n",
    "            temp_edge_visual=replace_path_name(temp_edge['srcmsg'])+','+replace_path_name(temp_edge['dstmsg'])+','+str(temp_edge['time'])\n",
    "            temp_edge_hash_val=hashgen(temp_edge_visual)\n",
    "            \n",
    "            if temp_edge_hash_val in edges_attack_hashset_h201:\n",
    "                edge_color='red'\n",
    "                attck_nodes_set_visual.add(replace_path_name(temp_edge['srcmsg']))\n",
    "                attck_nodes_set_visual.add(replace_path_name(temp_edge['dstmsg']))\n",
    "                attack_edge_count+=1\n",
    "            else:\n",
    "                edge_color='blue'\n",
    "            \n",
    "#             if attack_edge_flag(temp_edge['srcmsg']) and attack_edge_flag(temp_edge['dstmsg']):\n",
    "#                 edge_color='red'\n",
    "#                 attack_edge_count+=1\n",
    "#             else:\n",
    "#                 edge_color='blue'\n",
    "                \n",
    "                \n",
    "            dot.edge(str(hashgen(replace_path_name(temp_edge['srcmsg']))),str(hashgen(replace_path_name(temp_edge['dstmsg']))), label= temp_edge['edge_type'] , color=edge_color)#+ \"  loss: \"+str(temp_edge['loss']) + \"  time: \"+str(temp_edge['time'])\n",
    "\n",
    "    #         dot.edge(str(srcnode), str(dstnode), label= temp_edge['edge_type']+ \"  loss: \"+str((temp_edge['loss'])) + \"  time: \"+str(temp_edge['time']) , color='red')\n",
    "\n",
    "\n",
    "#     if len(communities[c].edges)<2:\n",
    "#         graph_index+=1\n",
    "#         continue\n",
    "#     if len(communities[c].edges)>1000:\n",
    "#         graph_index+=1\n",
    "#         continue\n",
    "#         print(f\"edge num:{len(communities[c].edges)}\")\n",
    "        \n",
    "    print(\"Start to render the figures···\")\n",
    "    \n",
    "    dot.render('./graph_visual_h201/subgraph_'+str(graph_index), view=False)\n",
    "    print(\"subgraph loss:\",(subgraph_loss_sum/len(communities[c].edges)))\n",
    "    print(\"graph_index:\",graph_index)\n",
    "    print(\"edge_count:\",len(communities[c].edges))\n",
    "    print(\"node_count:\",len(communities[c].nodes))\n",
    "    print(f\"{attack_node_count=}\")\n",
    "    print(f\"{attack_edge_count=}\")\n",
    "    graph_index+=1\n",
    "\n",
    "print(f\"avg edges:{total_edges/len(communities)}\")\n",
    "print(f\"avg nodes:{total_nodes/len(communities)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attck_nodes_set_visual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(attck_nodes_set_visual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_edge_visual"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## h402"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attack_list=[\n",
    " '/home/shahidul2k9/data/optc/test/graph_9_23_h402/2019-09-23 13:10:24.429~2019-09-23 13:25:09.374.txt',\n",
    " '/home/shahidul2k9/data/optc/test/graph_9_23_h402/2019-09-23 13:10:24.429~2019-09-23 13:25:09.374.txt',\n",
    " '/home/shahidul2k9/data/optc/test/graph_9_23_h402/2019-09-23 13:25:20.374~2019-09-23 13:40:21.268.txt',\n",
    " '/home/shahidul2k9/data/optc/test/graph_9_23_h402/2019-09-23 13:40:16.268~2019-09-23 13:55:31.310.txt',\n",
    " '/home/shahidul2k9/data/optc/test/graph_9_23_h402/2019-09-23 13:55:12.31~2019-09-23 14:10:58.200.txt',\n",
    "            ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timezone\n",
    "attack_dic=[]\n",
    "for a in attack_list:\n",
    "    temp_dic={}\n",
    "    aa=a.split(\"/\")[-1]\n",
    "    sp=aa.split('~')\n",
    "    start_timestamp=sp[0].split('.')[0]\n",
    "    end_timestamp=sp[1].split('.')[0]\n",
    "    temp_dic['start']=datetime_to_timestamp_US(start_timestamp)\n",
    "    temp_dic['end']=datetime_to_timestamp_US(end_timestamp)\n",
    "    temp_dic['file']=a\n",
    "    attack_dic.append(temp_dic)\n",
    "    print(temp_dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replace_dic={\n",
    "    '.pyc':'*.pyc',\n",
    "    '.dll':'*.dll',\n",
    "    '.DLL':'*.DLL',\n",
    "}\n",
    "\n",
    "def replace_path_name(path_name):\n",
    "    for i in replace_dic:\n",
    "        if i in path_name:\n",
    "            return replace_dic[i]\n",
    "    if '->' in path_name:\n",
    "        if 'outbound' in path_name:\n",
    "            msg=re.findall(\"->(.*?):\",path_name)[0]\n",
    "            return msg\n",
    "        elif 'inbound' in path_name:\n",
    "            msg=re.findall(\"#(.*?):\",path_name)[0]\n",
    "            return msg\n",
    "    return path_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "original_edges_count=0\n",
    "hash2msg={}\n",
    "graphs=[]\n",
    "gg=nx.DiGraph()\n",
    "count=0\n",
    "# file_list=os.listdir(\"./test_day_data4_10_emb100/\")\n",
    "for path in tqdm(attack_list):\n",
    "#     print(path)\n",
    "    if \".txt\" in path:\n",
    "        line_count=0\n",
    "        node_set=set()\n",
    "        tempg=nx.DiGraph()\n",
    "        f=open(path,\"r\")       \n",
    "        edge_list=[]\n",
    "        for line in f:\n",
    "            count+=1\n",
    "            l=line.strip()\n",
    "            jdata=eval(l)\n",
    "#             temp_key=jdata['srcmsg']+jdata['dstmsg']+jdata['edge_type']\n",
    "#             if temp_key in train_edge_set:\n",
    "#                 jdata['loss']=(jdata['loss']-train_edge_set[temp_key]) if jdata['loss']>=train_edge_set[temp_key] else 0  \n",
    "#             jdata['loss']=abs(jdata['loss']-train_edge_set[temp_key])  if temp_key in train_edge_set else jdata['loss']\n",
    "            edge_list.append(jdata)\n",
    "            \n",
    "        edge_list = sorted(edge_list, key=lambda x:x['loss'],reverse=True) \n",
    "        original_edges_count+=len(edge_list)\n",
    "        \n",
    "        loss_list=[]\n",
    "        for i in edge_list:\n",
    "            loss_list.append(i['loss'])\n",
    "        loss_mean=mean(loss_list)\n",
    "        loss_std=std(loss_list)\n",
    "        print(loss_mean)\n",
    "        print(loss_std)\n",
    "        thr=loss_mean+1.5*loss_std\n",
    "#         thr=-99\n",
    "        print(\"thr:\",thr)\n",
    "        for e in edge_list:\n",
    "            if e['loss']>thr:    \n",
    "#             if True:  \n",
    "#                 if \"'/home/admin/profile'\" in e['srcmsg'] or \" '/home/admin/profile'\" in e['dstmsg']:\n",
    "#                     print(e['srcmsg'])\n",
    "#                     print(e['dstmsg'])\n",
    "                tempg.add_edge(str(hashgen(replace_path_name(e['srcmsg']))),str(hashgen(replace_path_name(e['dstmsg']))))\n",
    "                gg.add_edge(str(hashgen(replace_path_name(e['srcmsg']))),str(hashgen(replace_path_name(e['dstmsg']))),loss=e['loss'],srcmsg=e['srcmsg'],dstmsg=e['dstmsg'],edge_type=e['edge_type'],time=e['time'])\n",
    "                \n",
    "                hash2msg[str(hashgen(replace_path_name(e['srcmsg'])))]=replace_path_name(e['srcmsg'])\n",
    "                hash2msg[str(hashgen(replace_path_name(e['dstmsg'])))]=replace_path_name(e['dstmsg'])\n",
    "                \n",
    "\n",
    "#                 gg.add_edge(e['srcnode'],e['dstnode'],loss=e['loss'],srcmsg=e['srcmsg'],dstmsg=e['dstmsg'],edge_type=e['edge_type'],time=e['time'])\n",
    "        print(path)\n",
    "        print(\"tempg edges:\",len(tempg.edges))\n",
    "        print(\"tempg nodes:\",len(tempg.nodes))\n",
    "        print(\"tempg weakly components:\",nx.number_weakly_connected_components(tempg))\n",
    "        \n",
    "        print(\"gg edges:\",len(gg.edges))\n",
    "        print(\"gg nodes:\",len(gg.nodes))\n",
    "        print(\"gg weakly components:\",nx.number_weakly_connected_components(gg))\n",
    "        print(f\"{original_edges_count=}\")\n",
    "#\n",
    "#                 gg.add_edge(e['srcnode'],e['dstnode'],loss=e['loss'],srcmsg=e['srcmsg'],dstmsg=e['dstmsg'],edge_type=e['edge_type'],time=e['time'])\n",
    "#         print(path,\" line_count:\",line_count,\"  nodes count:\",len(node_set))\n",
    "#         print(path,\" line_count:\",line_count,\"  nodes count:\",len(node_set))\n",
    "                \n",
    "                \n",
    "                #         graphs.append(g)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the best partition\n",
    "import datetime\n",
    "import community as community_louvain\n",
    "starttime = datetime.datetime.now()\n",
    "#long running\n",
    "partition = community_louvain.best_partition(gg.to_undirected())\n",
    "#do something other\n",
    "endtime = datetime.datetime.now()\n",
    "print(\"Finished the computation of community discovery. Execution time:{:d}\".format((endtime - starttime).seconds))\n",
    "\n",
    "\n",
    "communities={}\n",
    "max_partition=0\n",
    "for i in partition:\n",
    "    if partition[i]>max_partition:\n",
    "        max_partition=partition[i]\n",
    "        \n",
    "for i in range(max_partition+1):\n",
    "    communities[i]=nx.DiGraph()\n",
    "for e in gg.edges:\n",
    "#     if partition[e[0]]==partition[e[1]]:\n",
    "    communities[partition[e[0]]].add_edge(e[0],e[1])\n",
    "    communities[partition[e[1]]].add_edge(e[0],e[1])\n",
    "    \n",
    "print(f\"{max_partition=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "max_edge_count=-99\n",
    "max_index=-99\n",
    "max_node_count=0\n",
    "\n",
    "graph_index=0\n",
    "for c in communities:\n",
    "    file_set=set()\n",
    "    if len(communities[c].edges)>max_edge_count:\n",
    "        max_edge_count=len(communities[c].edges)\n",
    "        max_index=graph_index\n",
    "        max_node_count=len(communities[c].nodes)\n",
    "    for e in communities[c].edges:    \n",
    "        try:\n",
    "            temp_edge=gg.edges[e]\n",
    "            file_set.add(find_time_window(temp_edge,attack_dic))\n",
    "        except:\n",
    "            pass    \n",
    "    print(f\"{graph_index=}\")\n",
    "    print(f\"file_set：\")\n",
    "    print(*file_set, sep = \"\\n\")\n",
    "    print(f\"{file_set=}\")\n",
    "    graph_index+=1\n",
    "    \n",
    "print(f\"{max_index=}\")\n",
    "print(f\"{max_edge_count=}\")\n",
    "print(f\"{max_node_count=}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_sub_list={'/home/shahidul2k9/data/optc/test/graph_9_23_h402/2019-09-23 13:25:20.374~2019-09-23 13:40:21.268.txt', \n",
    "               '/home/shahidul2k9/data/optc/test/graph_9_23_h402/2019-09-23 13:10:24.429~2019-09-23 13:25:09.374.txt'}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "original_edges_count=0\n",
    "\n",
    "for path in tqdm(file_sub_list):\n",
    "\n",
    "    if \".txt\" in path:\n",
    "        line_count=0\n",
    "        node_set=set()\n",
    "        tempg=nx.DiGraph()\n",
    "        f=open(path,\"r\")       \n",
    "        edge_list=[]\n",
    "        for line in f:\n",
    "            edge_list.append(line)\n",
    "\n",
    "        original_edges_count+=len(edge_list)\n",
    "\n",
    "        print(f\"{original_edges_count=}\")\n",
    "#\n",
    "#                 gg.add_edge(e['srcnode'],e['dstnode'],loss=e['loss'],srcmsg=e['srcmsg'],dstmsg=e['dstmsg'],edge_type=e['edge_type'],time=e['time'])\n",
    "#         print(path,\" line_count:\",line_count,\"  nodes count:\",len(node_set))\n",
    "#         print(path,\" line_count:\",line_count,\"  nodes count:\",len(node_set))\n",
    "                \n",
    "                \n",
    "                #         graphs.append(g)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(communities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "from graphviz import Digraph\n",
    "\n",
    "\n",
    "graph_index=0\n",
    "\n",
    "total_nodes=0\n",
    "total_edges=0\n",
    "attck_nodes_set_visual=set()\n",
    "for c in communities:\n",
    "\n",
    "    dot = Digraph(name=\"MyPicture\", comment=\"the test\", format=\"pdf\")\n",
    "    dot.graph_attr['rankdir'] = 'LR'\n",
    "    # dot.node(name='a', label='wo', color='purple')\n",
    "    # dot.node(name='b', label='niu', color='purple')\n",
    "    # dot.node(name='c', label='che', color='purple')\n",
    "    \n",
    "    total_nodes+=len(communities[c].nodes)\n",
    "    total_edges+=len(communities[c].edges)\n",
    "    \n",
    "    subgraph_loss_sum=0\n",
    "    attack_node_count=0\n",
    "    attack_edge_count=0\n",
    "    for e in communities[c].edges:\n",
    "        try:\n",
    "            temp_edge=gg.edges[e]\n",
    "            srcnode=e['srcnode']\n",
    "            dstnode=e['dstnode']\n",
    "        except:\n",
    "            pass        \n",
    "\n",
    "        if True:\n",
    "            subgraph_loss_sum+=temp_edge['loss']\n",
    "\n",
    "    #     g.add_edge(srcnode,dstnode,srcmsg=node2msg[indexid2nodeid[srcnode]],dstmsg=node2msg[indexid2nodeid[dstnode]],loss=df['loss'][i])\n",
    "\n",
    "\n",
    "    #         dot.node( name=str(srcnode),label=str(node_index_id2msg_mal[srcnode]), color='purple',shape = 'box')\n",
    "            if \"'subject': '\" in temp_edge['srcmsg']:\n",
    "                src_shape='box'\n",
    "            elif \"'file': '\" in temp_edge['srcmsg']:\n",
    "                src_shape='oval'\n",
    "            elif \"'netflow': '\" in temp_edge['srcmsg']:\n",
    "                src_shape='diamond'\n",
    "                \n",
    "            src_shape='box'\n",
    "            if attack_edge_flag(temp_edge['srcmsg']):\n",
    "                src_node_color='red'\n",
    "                total_nodes+=1\n",
    "            else:\n",
    "                src_node_color='blue'\n",
    "                \n",
    "            src_node_color='blue'\n",
    "            \n",
    "            dot.node( name=str(hashgen(replace_path_name(temp_edge['srcmsg']))),\n",
    "                     label=str(replace_path_name(temp_edge['srcmsg'])+'\\t partition:'+str(partition[str(hashgen(replace_path_name(temp_edge['srcmsg'])))])), \n",
    "                     color=src_node_color,\n",
    "                     shape = src_shape)\n",
    "\n",
    "\n",
    "            if \"'subject': '\" in temp_edge['dstmsg']:\n",
    "                dst_shape='box'\n",
    "            elif \"'file': '\" in temp_edge['dstmsg']:\n",
    "                dst_shape='oval'\n",
    "            elif \"'netflow': '\" in temp_edge['dstmsg']:\n",
    "                dst_shape='diamond'\n",
    "\n",
    "                    \n",
    "            if \"->\" in temp_edge['dstmsg']:\n",
    "                dst_shape='diamond'\n",
    "            else:\n",
    "                dst_shape='oval'\n",
    "                \n",
    "                \n",
    "            if attack_edge_flag(temp_edge['dstmsg']):\n",
    "                dst_node_color='red'\n",
    "                total_nodes+=1\n",
    "            else:\n",
    "                dst_node_color='blue'\n",
    "            dst_node_color='blue'\n",
    "            \n",
    "            \n",
    "            dot.node( name=str(hashgen(replace_path_name(temp_edge['dstmsg']))),\n",
    "                     label=str(replace_path_name(temp_edge['dstmsg'])+'\\t partition:'+str(partition[str(hashgen(replace_path_name(temp_edge['dstmsg'])))])), \n",
    "                     color=dst_node_color,\n",
    "                     shape = dst_shape)\n",
    "\n",
    "\n",
    "    #         edgeindex=tensor_find(test_data.msg[i][16:-16],1)\n",
    "\n",
    "    \n",
    "            temp_edge_visual=replace_path_name(temp_edge['srcmsg'])+','+replace_path_name(temp_edge['dstmsg'])+','+str(temp_edge['time'])\n",
    "            temp_edge_hash_val=hashgen(temp_edge_visual)\n",
    "            \n",
    "            if temp_edge_hash_val in edges_attack_hashset_h402:\n",
    "                edge_color='red'\n",
    "                attck_nodes_set_visual.add(replace_path_name(temp_edge['srcmsg']))\n",
    "                attck_nodes_set_visual.add(replace_path_name(temp_edge['dstmsg']))\n",
    "                attack_edge_count+=1\n",
    "            else:\n",
    "                edge_color='blue'\n",
    "            \n",
    "#             if attack_edge_flag(temp_edge['srcmsg']) and attack_edge_flag(temp_edge['dstmsg']):\n",
    "#                 edge_color='red'\n",
    "#                 attack_edge_count+=1\n",
    "#             else:\n",
    "#                 edge_color='blue'\n",
    "                \n",
    "                \n",
    "            dot.edge(str(hashgen(replace_path_name(temp_edge['srcmsg']))),str(hashgen(replace_path_name(temp_edge['dstmsg']))), label= temp_edge['edge_type'] , color=edge_color)#+ \"  loss: \"+str(temp_edge['loss']) + \"  time: \"+str(temp_edge['time'])\n",
    "\n",
    "    #         dot.edge(str(srcnode), str(dstnode), label= temp_edge['edge_type']+ \"  loss: \"+str((temp_edge['loss'])) + \"  time: \"+str(temp_edge['time']) , color='red')\n",
    "\n",
    "\n",
    "    if len(communities[c].edges)<2:\n",
    "        graph_index+=1\n",
    "        continue\n",
    "    if len(communities[c].edges)>1000:\n",
    "        graph_index+=1\n",
    "        continue\n",
    "        print(f\"edge num:{len(communities[c].edges)}，skip rendering\")\n",
    "        \n",
    "    print(\"Start to reder the figures···\")\n",
    "    \n",
    "    dot.render('./graph_visual_h402/subgraph_'+str(graph_index), view=False)\n",
    "    print(\"subgraph loss:\",(subgraph_loss_sum/len(communities[c].edges)))\n",
    "    print(\"graph_index:\",graph_index)\n",
    "    print(\"edge_count:\",len(communities[c].edges))\n",
    "    print(\"node_count:\",len(communities[c].nodes))\n",
    "    print(f\"{attack_node_count=}\")\n",
    "    print(f\"{attack_edge_count=}\")\n",
    "    graph_index+=1\n",
    "\n",
    "print(f\"avg edges:{total_edges/len(communities)}\")\n",
    "print(f\"avg nodes:{total_nodes/len(communities)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attck_nodes_set_visual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## h660"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attack_list=[\n",
    "    \n",
    "    '/home/shahidul2k9/data/optc/test/graph_9_23_h660/2019-09-23 09:54:08.697~2019-09-23 10:09:11.523.txt',\n",
    "'/home/shahidul2k9/data/optc/test/graph_9_23_h660/2019-09-23 13:27:28.512~2019-09-23 13:42:30.682.txt',\n",
    " '/home/shahidul2k9/data/optc/test/graph_9_23_h660/2019-09-23 13:42:24.682~2019-09-23 13:57:57.566.txt',\n",
    " '/home/shahidul2k9/data/optc/test/graph_9_23_h660/2019-09-23 13:57:20.566~2019-09-23 14:12:59.139.txt',\n",
    "            ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timezone\n",
    "attack_dic=[]\n",
    "for a in attack_list:\n",
    "    temp_dic={}\n",
    "    aa=a.split(\"/\")[-1]\n",
    "    sp=aa.split('~')\n",
    "    start_timestamp=sp[0].split('.')[0]\n",
    "    end_timestamp=sp[1].split('.')[0]\n",
    "    temp_dic['start']=datetime_to_timestamp_US(start_timestamp)\n",
    "    temp_dic['end']=datetime_to_timestamp_US(end_timestamp)\n",
    "    temp_dic['file']=a\n",
    "    attack_dic.append(temp_dic)\n",
    "    print(temp_dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replace_dic={\n",
    "    '.pyc':'*.pyc',\n",
    "#     '.dll':'*.dll',\n",
    "#     '.DLL':'*.DLL',\n",
    "}\n",
    "\n",
    "def replace_path_name(path_name):\n",
    "    for i in replace_dic:\n",
    "        if i in path_name:\n",
    "            return replace_dic[i]\n",
    "    if '->' in path_name:\n",
    "        if 'outbound' in path_name:\n",
    "            msg=re.findall(\"->(.*?):\",path_name)[0]\n",
    "            return msg\n",
    "        elif 'inbound' in path_name:\n",
    "            msg=re.findall(\"#(.*?):\",path_name)[0]\n",
    "            return msg\n",
    "    return path_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "original_edges_count=0\n",
    "hash2msg={}\n",
    "graphs=[]\n",
    "gg=nx.DiGraph()\n",
    "count=0\n",
    "# file_list=os.listdir(\"./test_day_data4_10_emb100/\")\n",
    "for path in tqdm(attack_list):\n",
    "#     print(path)\n",
    "    if \".txt\" in path:\n",
    "        line_count=0\n",
    "        node_set=set()\n",
    "        tempg=nx.DiGraph()\n",
    "        f=open(path,\"r\")       \n",
    "        edge_list=[]\n",
    "        for line in f:\n",
    "            count+=1\n",
    "            l=line.strip()\n",
    "            jdata=eval(l)\n",
    "#             temp_key=jdata['srcmsg']+jdata['dstmsg']+jdata['edge_type']\n",
    "#             if temp_key in train_edge_set:\n",
    "#                 jdata['loss']=(jdata['loss']-train_edge_set[temp_key]) if jdata['loss']>=train_edge_set[temp_key] else 0  \n",
    "#             jdata['loss']=abs(jdata['loss']-train_edge_set[temp_key])  if temp_key in train_edge_set else jdata['loss']\n",
    "            edge_list.append(jdata)\n",
    "            \n",
    "        edge_list = sorted(edge_list, key=lambda x:x['loss'],reverse=True) \n",
    "        original_edges_count+=len(edge_list)\n",
    "        \n",
    "        loss_list=[]\n",
    "        for i in edge_list:\n",
    "            loss_list.append(i['loss'])\n",
    "        loss_mean=mean(loss_list)\n",
    "        loss_std=std(loss_list)\n",
    "        print(loss_mean)\n",
    "        print(loss_std)\n",
    "        thr=loss_mean+1.5*loss_std\n",
    "#         thr=-99\n",
    "        print(\"thr:\",thr)\n",
    "        for e in edge_list:\n",
    "            if e['loss']>thr:    \n",
    "#             if True:  \n",
    "#                 if \"'/home/admin/profile'\" in e['srcmsg'] or \" '/home/admin/profile'\" in e['dstmsg']:\n",
    "#                     print(e['srcmsg'])\n",
    "#                     print(e['dstmsg'])\n",
    "                tempg.add_edge(str(hashgen(replace_path_name(e['srcmsg']))),str(hashgen(replace_path_name(e['dstmsg']))))\n",
    "                gg.add_edge(str(hashgen(replace_path_name(e['srcmsg']))),str(hashgen(replace_path_name(e['dstmsg']))),loss=e['loss'],srcmsg=e['srcmsg'],dstmsg=e['dstmsg'],edge_type=e['edge_type'],time=e['time'])\n",
    "                \n",
    "                hash2msg[str(hashgen(replace_path_name(e['srcmsg'])))]=replace_path_name(e['srcmsg'])\n",
    "                hash2msg[str(hashgen(replace_path_name(e['dstmsg'])))]=replace_path_name(e['dstmsg'])\n",
    "                \n",
    "\n",
    "#                 gg.add_edge(e['srcnode'],e['dstnode'],loss=e['loss'],srcmsg=e['srcmsg'],dstmsg=e['dstmsg'],edge_type=e['edge_type'],time=e['time'])\n",
    "        print(path)\n",
    "        print(\"tempg edges:\",len(tempg.edges))\n",
    "        print(\"tempg nodes:\",len(tempg.nodes))\n",
    "        print(\"tempg weakly components:\",nx.number_weakly_connected_components(tempg))\n",
    "        \n",
    "        print(\"gg edges:\",len(gg.edges))\n",
    "        print(\"gg nodes:\",len(gg.nodes))\n",
    "        print(\"gg weakly components:\",nx.number_weakly_connected_components(gg))\n",
    "        print(f\"{original_edges_count=}\")\n",
    "#\n",
    "#                 gg.add_edge(e['srcnode'],e['dstnode'],loss=e['loss'],srcmsg=e['srcmsg'],dstmsg=e['dstmsg'],edge_type=e['edge_type'],time=e['time'])\n",
    "#         print(path,\" line_count:\",line_count,\"  nodes count:\",len(node_set))\n",
    "#         print(path,\" line_count:\",line_count,\"  nodes count:\",len(node_set))\n",
    "                \n",
    "                \n",
    "                #         graphs.append(g)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the best partition\n",
    "import datetime\n",
    "import community as community_louvain\n",
    "starttime = datetime.datetime.now()\n",
    "#long running\n",
    "partition = community_louvain.best_partition(gg.to_undirected())\n",
    "#do something other\n",
    "endtime = datetime.datetime.now()\n",
    "print(\"Finished the computation of community discovery. Execution time:{:d}\".format((endtime - starttime).seconds))\n",
    "\n",
    "\n",
    "communities={}\n",
    "max_partition=0\n",
    "for i in partition:\n",
    "    if partition[i]>max_partition:\n",
    "        max_partition=partition[i]\n",
    "        \n",
    "for i in range(max_partition+1):\n",
    "    communities[i]=nx.DiGraph()\n",
    "for e in gg.edges:\n",
    "#     if partition[e[0]]==partition[e[1]]:\n",
    "    communities[partition[e[0]]].add_edge(e[0],e[1])\n",
    "    communities[partition[e[1]]].add_edge(e[0],e[1])\n",
    "    \n",
    "print(f\"{max_partition=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "max_edge_count=-99\n",
    "max_index=-99\n",
    "max_node_count=0\n",
    "\n",
    "graph_index=0\n",
    "for c in communities:\n",
    "    file_set=set()\n",
    "    if len(communities[c].edges)>max_edge_count:\n",
    "        max_edge_count=len(communities[c].edges)\n",
    "        max_index=graph_index\n",
    "        max_node_count=len(communities[c].nodes)\n",
    "    for e in communities[c].edges:    \n",
    "        try:\n",
    "            temp_edge=gg.edges[e]\n",
    "            file_set.add(find_time_window(temp_edge,attack_dic))\n",
    "        except:\n",
    "            pass    \n",
    "    print(f\"{graph_index=}\")\n",
    "    print(f\"file_set：\")\n",
    "    print(*file_set, sep = \"\\n\")\n",
    "    print(f\"{file_set=}\")\n",
    "    graph_index+=1\n",
    "    \n",
    "print(f\"{max_index=}\")\n",
    "print(f\"{max_edge_count=}\")\n",
    "print(f\"{max_node_count=}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_sub_list={'/home/shahidul2k9/data/optc/test/graph_9_23_h660/2019-09-23 13:27:28.512~2019-09-23 13:42:30.682.txt', \n",
    "               '/home/shahidul2k9/data/optc/test/graph_9_23_h660/2019-09-23 13:42:24.682~2019-09-23 13:57:57.566.txt',\n",
    "                '/home/shahidul2k9/data/optc/test/graph_9_23_h660/2019-09-23 13:57:20.566~2019-09-23 14:12:59.139.txt'}\n",
    "\n",
    "\n",
    "\n",
    "original_edges_count=0\n",
    "\n",
    "for path in tqdm(file_sub_list):\n",
    "\n",
    "    if \".txt\" in path:\n",
    "        line_count=0\n",
    "        node_set=set()\n",
    "        tempg=nx.DiGraph()\n",
    "        f=open(path,\"r\")       \n",
    "        edge_list=[]\n",
    "        for line in f:\n",
    "            edge_list.append(line)\n",
    "\n",
    "        original_edges_count+=len(edge_list)\n",
    "\n",
    "        print(f\"{original_edges_count=}\")\n",
    "#\n",
    "#                 gg.add_edge(e['srcnode'],e['dstnode'],loss=e['loss'],srcmsg=e['srcmsg'],dstmsg=e['dstmsg'],edge_type=e['edge_type'],time=e['time'])\n",
    "#         print(path,\" line_count:\",line_count,\"  nodes count:\",len(node_set))\n",
    "#         print(path,\" line_count:\",line_count,\"  nodes count:\",len(node_set))\n",
    "                \n",
    "                \n",
    "                #         graphs.append(g)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "from graphviz import Digraph\n",
    "\n",
    "\n",
    "graph_index=0\n",
    "\n",
    "total_nodes=0\n",
    "total_edges=0\n",
    "attck_nodes_set_visual=set()\n",
    "for c in communities:\n",
    "    dot = Digraph(name=\"MyPicture\", comment=\"the test\", format=\"pdf\")\n",
    "    dot.graph_attr['rankdir'] = 'LR'\n",
    "    # dot.node(name='a', label='wo', color='purple')\n",
    "    # dot.node(name='b', label='niu', color='purple')\n",
    "    # dot.node(name='c', label='che', color='purple')\n",
    "    \n",
    "    total_nodes+=len(communities[c].nodes)\n",
    "    total_edges+=len(communities[c].edges)\n",
    "    \n",
    "    subgraph_loss_sum=0\n",
    "    attack_node_count=0\n",
    "    attack_edge_count=0\n",
    "    for e in communities[c].edges:\n",
    "        try:\n",
    "            temp_edge=gg.edges[e]\n",
    "            srcnode=e['srcnode']\n",
    "            dstnode=e['dstnode']\n",
    "        except:\n",
    "            pass        \n",
    "\n",
    "        if True:\n",
    "            subgraph_loss_sum+=temp_edge['loss']\n",
    "\n",
    "    #     g.add_edge(srcnode,dstnode,srcmsg=node2msg[indexid2nodeid[srcnode]],dstmsg=node2msg[indexid2nodeid[dstnode]],loss=df['loss'][i])\n",
    "\n",
    "\n",
    "    #         dot.node( name=str(srcnode),label=str(node_index_id2msg_mal[srcnode]), color='purple',shape = 'box')\n",
    "            if \"'subject': '\" in temp_edge['srcmsg']:\n",
    "                src_shape='box'\n",
    "            elif \"'file': '\" in temp_edge['srcmsg']:\n",
    "                src_shape='oval'\n",
    "            elif \"'netflow': '\" in temp_edge['srcmsg']:\n",
    "                src_shape='diamond'\n",
    "                \n",
    "            src_shape='box'\n",
    "            if attack_edge_flag(temp_edge['srcmsg']):\n",
    "                src_node_color='red'\n",
    "                total_nodes+=1\n",
    "            else:\n",
    "                src_node_color='blue'\n",
    "                \n",
    "            src_node_color='blue'\n",
    "            \n",
    "            dot.node( name=str(hashgen(replace_path_name(temp_edge['srcmsg']))),\n",
    "                     label=str(replace_path_name(temp_edge['srcmsg'])+'\\t partition:'+str(partition[str(hashgen(replace_path_name(temp_edge['srcmsg'])))])), \n",
    "                     color=src_node_color,\n",
    "                     shape = src_shape)\n",
    "\n",
    "\n",
    "            if \"'subject': '\" in temp_edge['dstmsg']:\n",
    "                dst_shape='box'\n",
    "            elif \"'file': '\" in temp_edge['dstmsg']:\n",
    "                dst_shape='oval'\n",
    "            elif \"'netflow': '\" in temp_edge['dstmsg']:\n",
    "                dst_shape='diamond'\n",
    "\n",
    "                    \n",
    "            if \"->\" in temp_edge['dstmsg']:\n",
    "                dst_shape='diamond'\n",
    "            else:\n",
    "                dst_shape='oval'\n",
    "                \n",
    "                \n",
    "            if attack_edge_flag(temp_edge['dstmsg']):\n",
    "                dst_node_color='red'\n",
    "                total_nodes+=1\n",
    "            else:\n",
    "                dst_node_color='blue'\n",
    "            dst_node_color='blue'\n",
    "            \n",
    "            \n",
    "            dot.node( name=str(hashgen(replace_path_name(temp_edge['dstmsg']))),\n",
    "                     label=str(replace_path_name(temp_edge['dstmsg'])+'\\t partition:'+str(partition[str(hashgen(replace_path_name(temp_edge['dstmsg'])))])), \n",
    "                     color=dst_node_color,\n",
    "                     shape = dst_shape)\n",
    "\n",
    "\n",
    "    #         edgeindex=tensor_find(test_data.msg[i][16:-16],1)\n",
    "\n",
    "    \n",
    "            temp_edge_visual=replace_path_name(temp_edge['srcmsg'])+','+replace_path_name(temp_edge['dstmsg'])+','+str(temp_edge['time'])\n",
    "            temp_edge_hash_val=hashgen(temp_edge_visual)\n",
    "            \n",
    "            if temp_edge_hash_val in edges_attack_hashset_h660:\n",
    "                edge_color='red'\n",
    "                attck_nodes_set_visual.add(replace_path_name(temp_edge['srcmsg']))\n",
    "                attck_nodes_set_visual.add(replace_path_name(temp_edge['dstmsg']))\n",
    "                attack_edge_count+=1\n",
    "            else:\n",
    "                edge_color='blue'\n",
    "            \n",
    "#             if attack_edge_flag(temp_edge['srcmsg']) and attack_edge_flag(temp_edge['dstmsg']):\n",
    "#                 edge_color='red'\n",
    "#                 attack_edge_count+=1\n",
    "#             else:\n",
    "#                 edge_color='blue'\n",
    "                \n",
    "                \n",
    "            dot.edge(str(hashgen(replace_path_name(temp_edge['srcmsg']))),str(hashgen(replace_path_name(temp_edge['dstmsg']))), label= temp_edge['edge_type'] , color=edge_color)#+ \"  loss: \"+str(temp_edge['loss']) + \"  time: \"+str(temp_edge['time'])\n",
    "\n",
    "    #         dot.edge(str(srcnode), str(dstnode), label= temp_edge['edge_type']+ \"  loss: \"+str((temp_edge['loss'])) + \"  time: \"+str(temp_edge['time']) , color='red')\n",
    "\n",
    "\n",
    "#     if len(communities[c].edges)<2:\n",
    "#         continue\n",
    "#     if len(communities[c].edges)>1000:\n",
    "#         continue\n",
    "#         print(f\"edge num:{len(communities[c].edges)}，skip rendering\")\n",
    "        \n",
    "    print(\"Start to render the figures···\")\n",
    "    \n",
    "    dot.render('./graph_visual_h660/subgraph_'+str(graph_index), view=False)\n",
    "    print(\"subgraph loss:\",(subgraph_loss_sum/len(communities[c].edges)))\n",
    "    print(\"graph_index:\",graph_index)\n",
    "    print(\"edge_count:\",len(communities[c].edges))\n",
    "    print(\"node_count:\",len(communities[c].nodes))\n",
    "    print(f\"{attack_node_count=}\")\n",
    "    print(f\"{attack_edge_count=}\")\n",
    "    graph_index+=1\n",
    "\n",
    "print(f\"avg edges:{total_edges/len(communities)}\")\n",
    "print(f\"avg nodes:{total_nodes/len(communities)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attck_nodes_set_visual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## h501"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attack_list=[\n",
    " 'graph_9_24_h501/2019-09-24 10:15:28.241~2019-09-24 10:30:00.201.txt',\n",
    " 'graph_9_24_h501/2019-09-24 10:30:24.201~2019-09-24 10:45:02.7.txt',\n",
    " 'graph_9_24_h501/2019-09-24 10:45:20.7~2019-09-24 11:00:31.385.txt',\n",
    " 'graph_9_24_h501/2019-09-24 11:00:16.385~2019-09-24 11:16:09.755.txt',\n",
    " 'graph_9_24_h501/2019-09-24 11:15:12.755~2019-09-24 11:31:14.287.txt',\n",
    " 'graph_9_24_h501/2019-09-24 11:32:16.287~2019-09-24 11:46:31.541.txt',\n",
    "    \n",
    " 'graph_9_24_h501/2019-09-24 13:04:00.804~2019-09-24 13:17:29.451.txt',\n",
    " 'graph_9_24_h501/2019-09-24 13:18:56.451~2019-09-24 13:32:46.454.txt',\n",
    " 'graph_9_24_h501/2019-09-24 13:33:52.454~2019-09-24 13:48:02.493.txt',\n",
    "            ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timezone\n",
    "attack_dic=[]\n",
    "for a in attack_list:\n",
    "    temp_dic={}\n",
    "    aa=a.split(\"/\")[-1]\n",
    "    sp=aa.split('~')\n",
    "    start_timestamp=sp[0].split('.')[0]\n",
    "    end_timestamp=sp[1].split('.')[0]\n",
    "    temp_dic['start']=datetime_to_timestamp_US(start_timestamp)\n",
    "    temp_dic['end']=datetime_to_timestamp_US(end_timestamp)\n",
    "    temp_dic['file']=a\n",
    "    attack_dic.append(temp_dic)\n",
    "    print(temp_dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replace_dic={\n",
    "    '.pyc':'*.pyc',\n",
    "    '.dll':'*.dll',\n",
    "    '.DLL':'*.DLL',\n",
    "}\n",
    "\n",
    "def replace_path_name(path_name):\n",
    "    for i in replace_dic:\n",
    "        if i in path_name:\n",
    "            return replace_dic[i]\n",
    "    if '->' in path_name:\n",
    "        if 'outbound' in path_name:\n",
    "            msg=re.findall(\"->(.*?):\",path_name)[0]\n",
    "            return msg\n",
    "        elif 'inbound' in path_name:\n",
    "            msg=re.findall(\"#(.*?):\",path_name)[0]\n",
    "            return msg\n",
    "    return path_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "original_edges_count=0\n",
    "hash2msg={}\n",
    "graphs=[]\n",
    "gg=nx.DiGraph()\n",
    "count=0\n",
    "# file_list=os.listdir(\"./test_day_data4_10_emb100/\")\n",
    "for path in tqdm(attack_list):\n",
    "#     print(path)\n",
    "    if \".txt\" in path:\n",
    "        line_count=0\n",
    "        node_set=set()\n",
    "        tempg=nx.DiGraph()\n",
    "        f=open(path,\"r\")       \n",
    "        edge_list=[]\n",
    "        for line in f:\n",
    "            count+=1\n",
    "            l=line.strip()\n",
    "            jdata=eval(l)\n",
    "#             temp_key=jdata['srcmsg']+jdata['dstmsg']+jdata['edge_type']\n",
    "#             if temp_key in train_edge_set:\n",
    "#                 jdata['loss']=(jdata['loss']-train_edge_set[temp_key]) if jdata['loss']>=train_edge_set[temp_key] else 0  \n",
    "#             jdata['loss']=abs(jdata['loss']-train_edge_set[temp_key])  if temp_key in train_edge_set else jdata['loss']\n",
    "            edge_list.append(jdata)\n",
    "            \n",
    "        edge_list = sorted(edge_list, key=lambda x:x['loss'],reverse=True) \n",
    "        original_edges_count+=len(edge_list)\n",
    "        \n",
    "        loss_list=[]\n",
    "        for i in edge_list:\n",
    "            loss_list.append(i['loss'])\n",
    "        loss_mean=mean(loss_list)\n",
    "        loss_std=std(loss_list)\n",
    "        print(loss_mean)\n",
    "        print(loss_std)\n",
    "        thr=loss_mean+2.5*loss_std\n",
    "#         thr=-99\n",
    "        print(\"thr:\",thr)\n",
    "        for e in edge_list:\n",
    "            if e['loss']>thr:    \n",
    "#             if True:  \n",
    "#                 if \"'/home/admin/profile'\" in e['srcmsg'] or \" '/home/admin/profile'\" in e['dstmsg']:\n",
    "#                     print(e['srcmsg'])\n",
    "#                     print(e['dstmsg'])\n",
    "                tempg.add_edge(str(hashgen(replace_path_name(e['srcmsg']))),str(hashgen(replace_path_name(e['dstmsg']))))\n",
    "                gg.add_edge(str(hashgen(replace_path_name(e['srcmsg']))),str(hashgen(replace_path_name(e['dstmsg']))),loss=e['loss'],srcmsg=e['srcmsg'],dstmsg=e['dstmsg'],edge_type=e['edge_type'],time=e['time'])\n",
    "                \n",
    "                hash2msg[str(hashgen(replace_path_name(e['srcmsg'])))]=replace_path_name(e['srcmsg'])\n",
    "                hash2msg[str(hashgen(replace_path_name(e['dstmsg'])))]=replace_path_name(e['dstmsg'])\n",
    "                \n",
    "\n",
    "#                 gg.add_edge(e['srcnode'],e['dstnode'],loss=e['loss'],srcmsg=e['srcmsg'],dstmsg=e['dstmsg'],edge_type=e['edge_type'],time=e['time'])\n",
    "        print(path)\n",
    "        print(\"tempg edges:\",len(tempg.edges))\n",
    "        print(\"tempg nodes:\",len(tempg.nodes))\n",
    "        print(\"tempg weakly components:\",nx.number_weakly_connected_components(tempg))\n",
    "        \n",
    "        print(\"gg edges:\",len(gg.edges))\n",
    "        print(\"gg nodes:\",len(gg.nodes))\n",
    "        print(\"gg weakly components:\",nx.number_weakly_connected_components(gg))\n",
    "        print(f\"{original_edges_count=}\")\n",
    "#\n",
    "#                 gg.add_edge(e['srcnode'],e['dstnode'],loss=e['loss'],srcmsg=e['srcmsg'],dstmsg=e['dstmsg'],edge_type=e['edge_type'],time=e['time'])\n",
    "#         print(path,\" line_count:\",line_count,\"  nodes count:\",len(node_set))\n",
    "#         print(path,\" line_count:\",line_count,\"  nodes count:\",len(node_set))\n",
    "                \n",
    "                \n",
    "                #         graphs.append(g)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the best partition\n",
    "import datetime\n",
    "import community as community_louvain\n",
    "starttime = datetime.datetime.now()\n",
    "#long running\n",
    "partition = community_louvain.best_partition(gg.to_undirected())\n",
    "#do something other\n",
    "endtime = datetime.datetime.now()\n",
    "print(\"Finished the computation of community discovery. Execution time:{:d}\".format((endtime - starttime).seconds))\n",
    "\n",
    "\n",
    "communities={}\n",
    "max_partition=0\n",
    "for i in partition:\n",
    "    if partition[i]>max_partition:\n",
    "        max_partition=partition[i]\n",
    "        \n",
    "for i in range(max_partition+1):\n",
    "    communities[i]=nx.DiGraph()\n",
    "for e in gg.edges:\n",
    "#     if partition[e[0]]==partition[e[1]]:\n",
    "    communities[partition[e[0]]].add_edge(e[0],e[1])\n",
    "    communities[partition[e[1]]].add_edge(e[0],e[1])\n",
    "    \n",
    "print(f\"{max_partition=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_edge_count=-99\n",
    "max_index=-99\n",
    "max_node_count=0\n",
    "\n",
    "graph_index=0\n",
    "for c in communities:\n",
    "    file_set=set()\n",
    "    if len(communities[c].edges)>max_edge_count:\n",
    "        max_edge_count=len(communities[c].edges)\n",
    "        max_index=graph_index\n",
    "        max_node_count=len(communities[c].nodes)\n",
    "    for e in communities[c].edges:    \n",
    "        try:\n",
    "            temp_edge=gg.edges[e]\n",
    "            file_set.add(find_time_window(temp_edge,attack_dic))\n",
    "        except:\n",
    "            pass    \n",
    "    print(f\"{graph_index=}\")\n",
    "    print(f\"file_set：\")\n",
    "    print(*file_set, sep = \"\\n\")\n",
    "    print(f\"{file_set=}\")\n",
    "    graph_index+=1\n",
    "    \n",
    "print(f\"{max_index=}\")\n",
    "print(f\"{max_edge_count=}\")\n",
    "print(f\"{max_node_count=}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_sub_list={'graph_9_24_h501/2019-09-24 10:45:20.7~2019-09-24 11:00:31.385.txt', \n",
    "               'graph_9_24_h501/2019-09-24 11:00:16.385~2019-09-24 11:16:09.755.txt', \n",
    "               'graph_9_24_h501/2019-09-24 10:15:28.241~2019-09-24 10:30:00.201.txt', \n",
    "               'graph_9_24_h501/2019-09-24 11:15:12.755~2019-09-24 11:31:14.287.txt', \n",
    "               'graph_9_24_h501/2019-09-24 11:32:16.287~2019-09-24 11:46:31.541.txt', \n",
    "               'graph_9_24_h501/2019-09-24 10:30:24.201~2019-09-24 10:45:02.7.txt',\n",
    "               'graph_9_24_h501/2019-09-24 10:15:28.241~2019-09-24 10:30:00.201.txt', \n",
    "               'graph_9_24_h501/2019-09-24 13:04:00.804~2019-09-24 13:17:29.451.txt', \n",
    "               'graph_9_24_h501/2019-09-24 11:15:12.755~2019-09-24 11:31:14.287.txt', \n",
    "               'graph_9_24_h501/2019-09-24 11:32:16.287~2019-09-24 11:46:31.541.txt', \n",
    "               'graph_9_24_h501/2019-09-24 13:33:52.454~2019-09-24 13:48:02.493.txt', \n",
    "               'graph_9_24_h501/2019-09-24 10:30:24.201~2019-09-24 10:45:02.7.txt', \n",
    "               'graph_9_24_h501/2019-09-24 13:18:56.451~2019-09-24 13:32:46.454.txt'}\n",
    "\n",
    "original_edges_count=0\n",
    "\n",
    "for path in tqdm(file_sub_list):\n",
    "\n",
    "    if \".txt\" in path:\n",
    "        line_count=0\n",
    "        node_set=set()\n",
    "        tempg=nx.DiGraph()\n",
    "        f=open(path,\"r\")       \n",
    "        edge_list=[]\n",
    "        for line in f:\n",
    "            edge_list.append(line)\n",
    "\n",
    "        original_edges_count+=len(edge_list)\n",
    "\n",
    "        print(f\"{original_edges_count=}\")\n",
    "#\n",
    "#                 gg.add_edge(e['srcnode'],e['dstnode'],loss=e['loss'],srcmsg=e['srcmsg'],dstmsg=e['dstmsg'],edge_type=e['edge_type'],time=e['time'])\n",
    "#         print(path,\" line_count:\",line_count,\"  nodes count:\",len(node_set))\n",
    "#         print(path,\" line_count:\",line_count,\"  nodes count:\",len(node_set))\n",
    "                \n",
    "                \n",
    "                #         graphs.append(g)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from graphviz import Digraph\n",
    "\n",
    "\n",
    "graph_index=0\n",
    "\n",
    "total_nodes=0\n",
    "total_edges=0\n",
    "attck_nodes_set_visual=set()\n",
    "for c in communities:\n",
    "    dot = Digraph(name=\"MyPicture\", comment=\"the test\", format=\"pdf\")\n",
    "    dot.graph_attr['rankdir'] = 'LR'\n",
    "    # dot.node(name='a', label='wo', color='purple')\n",
    "    # dot.node(name='b', label='niu', color='purple')\n",
    "    # dot.node(name='c', label='che', color='purple')\n",
    "    \n",
    "    total_nodes+=len(communities[c].nodes)\n",
    "    total_edges+=len(communities[c].edges)\n",
    "    \n",
    "    subgraph_loss_sum=0\n",
    "    attack_node_count=0\n",
    "    attack_edge_count=0\n",
    "    for e in communities[c].edges:\n",
    "        try:\n",
    "            temp_edge=gg.edges[e]\n",
    "            srcnode=e['srcnode']\n",
    "            dstnode=e['dstnode']\n",
    "        except:\n",
    "            pass        \n",
    "\n",
    "        if True:\n",
    "            subgraph_loss_sum+=temp_edge['loss']\n",
    "\n",
    "    #     g.add_edge(srcnode,dstnode,srcmsg=node2msg[indexid2nodeid[srcnode]],dstmsg=node2msg[indexid2nodeid[dstnode]],loss=df['loss'][i])\n",
    "\n",
    "\n",
    "    #         dot.node( name=str(srcnode),label=str(node_index_id2msg_mal[srcnode]), color='purple',shape = 'box')\n",
    "            if \"'subject': '\" in temp_edge['srcmsg']:\n",
    "                src_shape='box'\n",
    "            elif \"'file': '\" in temp_edge['srcmsg']:\n",
    "                src_shape='oval'\n",
    "            elif \"'netflow': '\" in temp_edge['srcmsg']:\n",
    "                src_shape='diamond'\n",
    "                \n",
    "            src_shape='box'\n",
    "            if attack_edge_flag(temp_edge['srcmsg']):\n",
    "                src_node_color='red'\n",
    "                total_nodes+=1\n",
    "            else:\n",
    "                src_node_color='blue'\n",
    "                \n",
    "            src_node_color='blue'\n",
    "            \n",
    "            dot.node( name=str(hashgen(replace_path_name(temp_edge['srcmsg']))),\n",
    "                     label=str(replace_path_name(temp_edge['srcmsg'])+'\\t partition:'+str(partition[str(hashgen(replace_path_name(temp_edge['srcmsg'])))])), \n",
    "                     color=src_node_color,\n",
    "                     shape = src_shape)\n",
    "\n",
    "\n",
    "            if \"'subject': '\" in temp_edge['dstmsg']:\n",
    "                dst_shape='box'\n",
    "            elif \"'file': '\" in temp_edge['dstmsg']:\n",
    "                dst_shape='oval'\n",
    "            elif \"'netflow': '\" in temp_edge['dstmsg']:\n",
    "                dst_shape='diamond'\n",
    "\n",
    "                    \n",
    "            if \"->\" in temp_edge['dstmsg']:\n",
    "                dst_shape='diamond'\n",
    "            else:\n",
    "                dst_shape='oval'\n",
    "                \n",
    "                \n",
    "            if attack_edge_flag(temp_edge['dstmsg']):\n",
    "                dst_node_color='red'\n",
    "                total_nodes+=1\n",
    "            else:\n",
    "                dst_node_color='blue'\n",
    "            dst_node_color='blue'\n",
    "            \n",
    "            \n",
    "            dot.node( name=str(hashgen(replace_path_name(temp_edge['dstmsg']))),\n",
    "                     label=str(replace_path_name(temp_edge['dstmsg'])+'\\t partition:'+str(partition[str(hashgen(replace_path_name(temp_edge['dstmsg'])))])), \n",
    "                     color=dst_node_color,\n",
    "                     shape = dst_shape)\n",
    "\n",
    "\n",
    "    #         edgeindex=tensor_find(test_data.msg[i][16:-16],1)\n",
    "    \n",
    "            temp_edge_visual=replace_path_name(temp_edge['srcmsg'])+','+replace_path_name(temp_edge['dstmsg'])+','+str(temp_edge['time'])\n",
    "            temp_edge_hash_val=hashgen(temp_edge_visual)\n",
    "            \n",
    "            if temp_edge_hash_val in edges_attack_hashset_h501:\n",
    "                edge_color='red'\n",
    "                attck_nodes_set_visual.add(replace_path_name(temp_edge['srcmsg']))\n",
    "                attck_nodes_set_visual.add(replace_path_name(temp_edge['dstmsg']))\n",
    "                attack_edge_count+=1\n",
    "            else:\n",
    "                edge_color='blue'\n",
    "            \n",
    "#             if attack_edge_flag(temp_edge['srcmsg']) and attack_edge_flag(temp_edge['dstmsg']):\n",
    "#                 edge_color='red'\n",
    "#                 attack_edge_count+=1\n",
    "#             else:\n",
    "#                 edge_color='blue'\n",
    "                \n",
    "                \n",
    "            dot.edge(str(hashgen(replace_path_name(temp_edge['srcmsg']))),str(hashgen(replace_path_name(temp_edge['dstmsg']))), label= temp_edge['edge_type'] , color=edge_color)#+ \"  loss: \"+str(temp_edge['loss']) + \"  time: \"+str(temp_edge['time'])\n",
    "\n",
    "    #         dot.edge(str(srcnode), str(dstnode), label= temp_edge['edge_type']+ \"  loss: \"+str((temp_edge['loss'])) + \"  time: \"+str(temp_edge['time']) , color='red')\n",
    "\n",
    "\n",
    "#     if len(communities[c].edges)<2:\n",
    "#         continue\n",
    "#     if len(communities[c].edges)>1000:\n",
    "#         continue\n",
    "#         print(f\"edge num:{len(communities[c].edges)}，skip rendering\")\n",
    "        \n",
    "    print(\"Start to render the figures···\")\n",
    "    \n",
    "    dot.render('./graph_visual_h501/subgraph_'+str(graph_index), view=False)\n",
    "    print(\"subgraph loss:\",(subgraph_loss_sum/len(communities[c].edges)))\n",
    "    print(\"graph_index:\",graph_index)\n",
    "    print(\"edge_count:\",len(communities[c].edges))\n",
    "    print(\"node_count:\",len(communities[c].nodes))\n",
    "    print(f\"{attack_node_count=}\")\n",
    "    print(f\"{attack_edge_count=}\")\n",
    "    graph_index+=1\n",
    "\n",
    "print(f\"avg edges:{total_edges/len(communities)}\")\n",
    "print(f\"avg nodes:{total_nodes/len(communities)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attck_nodes_set_visual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## h051"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attack_list=[\n",
    "    'graph_9_25_h051/2019-09-25 09:11:28.846~2019-09-25 09:25:34.393.txt',\n",
    "    'graph_9_25_h051/2019-09-25 09:26:24.393~2019-09-25 09:40:36.393.txt',\n",
    "'graph_9_25_h051/2019-09-25 10:26:08.397~2019-09-25 10:41:40.247.txt',\n",
    " 'graph_9_25_h051/2019-09-25 10:41:04.247~2019-09-25 10:56:56.92.txt',\n",
    " 'graph_9_25_h051/2019-09-25 10:56:00.92~2019-09-25 11:12:03.608.txt',\n",
    "    'graph_9_25_h051/2019-09-25 14:16:32.762~2019-09-25 14:32:02.764.txt',\n",
    "            ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timezone\n",
    "attack_dic=[]\n",
    "for a in attack_list:\n",
    "    temp_dic={}\n",
    "    aa=a.split(\"/\")[-1]\n",
    "    sp=aa.split('~')\n",
    "    start_timestamp=sp[0].split('.')[0]\n",
    "    end_timestamp=sp[1].split('.')[0]\n",
    "    temp_dic['start']=datetime_to_timestamp_US(start_timestamp)\n",
    "    temp_dic['end']=datetime_to_timestamp_US(end_timestamp)\n",
    "    temp_dic['file']=a\n",
    "    attack_dic.append(temp_dic)\n",
    "    print(temp_dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replace_dic={\n",
    "    '.pyc':'*.pyc',\n",
    "    '.dll':'*.dll',\n",
    "    '.DLL':'*.DLL',\n",
    "}\n",
    "\n",
    "def replace_path_name(path_name):\n",
    "    for i in replace_dic:\n",
    "        if i in path_name:\n",
    "            return replace_dic[i]\n",
    "    if '->' in path_name:\n",
    "        if 'outbound' in path_name:\n",
    "            msg=re.findall(\"->(.*?):\",path_name)[0]\n",
    "            return msg\n",
    "        elif 'inbound' in path_name:\n",
    "            msg=re.findall(\"#(.*?):\",path_name)[0]\n",
    "            return msg\n",
    "    return path_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "original_edges_count=0\n",
    "hash2msg={}\n",
    "graphs=[]\n",
    "gg=nx.DiGraph()\n",
    "count=0\n",
    "# file_list=os.listdir(\"./test_day_data4_10_emb100/\")\n",
    "for path in tqdm(attack_list):\n",
    "#     print(path)\n",
    "    if \".txt\" in path:\n",
    "        line_count=0\n",
    "        node_set=set()\n",
    "        tempg=nx.DiGraph()\n",
    "        f=open(path,\"r\")       \n",
    "        edge_list=[]\n",
    "        for line in f:\n",
    "            count+=1\n",
    "            l=line.strip()\n",
    "            jdata=eval(l)\n",
    "#             temp_key=jdata['srcmsg']+jdata['dstmsg']+jdata['edge_type']\n",
    "#             if temp_key in train_edge_set:\n",
    "#                 jdata['loss']=(jdata['loss']-train_edge_set[temp_key]) if jdata['loss']>=train_edge_set[temp_key] else 0  \n",
    "#             jdata['loss']=abs(jdata['loss']-train_edge_set[temp_key])  if temp_key in train_edge_set else jdata['loss']\n",
    "            edge_list.append(jdata)\n",
    "            \n",
    "        edge_list = sorted(edge_list, key=lambda x:x['loss'],reverse=True) \n",
    "        original_edges_count+=len(edge_list)\n",
    "        \n",
    "        loss_list=[]\n",
    "        for i in edge_list:\n",
    "            loss_list.append(i['loss'])\n",
    "        loss_mean=mean(loss_list)\n",
    "        loss_std=std(loss_list)\n",
    "        print(loss_mean)\n",
    "        print(loss_std)\n",
    "        thr=loss_mean+2.5*loss_std\n",
    "#         thr=-99\n",
    "        print(\"thr:\",thr)\n",
    "        for e in edge_list:\n",
    "            if e['loss']>thr:    \n",
    "#             if True:  \n",
    "#                 if \"'/home/admin/profile'\" in e['srcmsg'] or \" '/home/admin/profile'\" in e['dstmsg']:\n",
    "#                     print(e['srcmsg'])\n",
    "#                     print(e['dstmsg'])\n",
    "                tempg.add_edge(str(hashgen(replace_path_name(e['srcmsg']))),str(hashgen(replace_path_name(e['dstmsg']))))\n",
    "                gg.add_edge(str(hashgen(replace_path_name(e['srcmsg']))),str(hashgen(replace_path_name(e['dstmsg']))),loss=e['loss'],srcmsg=e['srcmsg'],dstmsg=e['dstmsg'],edge_type=e['edge_type'],time=e['time'])\n",
    "                \n",
    "                hash2msg[str(hashgen(replace_path_name(e['srcmsg'])))]=replace_path_name(e['srcmsg'])\n",
    "                hash2msg[str(hashgen(replace_path_name(e['dstmsg'])))]=replace_path_name(e['dstmsg'])\n",
    "                \n",
    "\n",
    "#                 gg.add_edge(e['srcnode'],e['dstnode'],loss=e['loss'],srcmsg=e['srcmsg'],dstmsg=e['dstmsg'],edge_type=e['edge_type'],time=e['time'])\n",
    "        print(path)\n",
    "        print(\"tempg edges:\",len(tempg.edges))\n",
    "        print(\"tempg nodes:\",len(tempg.nodes))\n",
    "        print(\"tempg weakly components:\",nx.number_weakly_connected_components(tempg))\n",
    "        \n",
    "        print(\"gg edges:\",len(gg.edges))\n",
    "        print(\"gg nodes:\",len(gg.nodes))\n",
    "        print(\"gg weakly components:\",nx.number_weakly_connected_components(gg))\n",
    "        print(f\"{original_edges_count=}\")\n",
    "#\n",
    "#                 gg.add_edge(e['srcnode'],e['dstnode'],loss=e['loss'],srcmsg=e['srcmsg'],dstmsg=e['dstmsg'],edge_type=e['edge_type'],time=e['time'])\n",
    "#         print(path,\" line_count:\",line_count,\"  nodes count:\",len(node_set))\n",
    "#         print(path,\" line_count:\",line_count,\"  nodes count:\",len(node_set))\n",
    "                \n",
    "                \n",
    "                #         graphs.append(g)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the best partition\n",
    "import datetime\n",
    "import community as community_louvain\n",
    "starttime = datetime.datetime.now()\n",
    "#long running\n",
    "partition = community_louvain.best_partition(gg.to_undirected())\n",
    "#do something other\n",
    "endtime = datetime.datetime.now()\n",
    "print(\"Finished the computation of community discovery. Execution time:{:d}\".format((endtime - starttime).seconds))\n",
    "\n",
    "\n",
    "communities={}\n",
    "max_partition=0\n",
    "for i in partition:\n",
    "    if partition[i]>max_partition:\n",
    "        max_partition=partition[i]\n",
    "        \n",
    "for i in range(max_partition+1):\n",
    "    communities[i]=nx.DiGraph()\n",
    "for e in gg.edges:\n",
    "#     if partition[e[0]]==partition[e[1]]:\n",
    "    communities[partition[e[0]]].add_edge(e[0],e[1])\n",
    "    communities[partition[e[1]]].add_edge(e[0],e[1])\n",
    "    \n",
    "print(f\"{max_partition=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from graphviz import Digraph\n",
    "\n",
    "\n",
    "graph_index=0\n",
    "\n",
    "total_nodes=0\n",
    "total_edges=0\n",
    "attck_nodes_set_visual=set()\n",
    "for c in communities:\n",
    "    dot = Digraph(name=\"MyPicture\", comment=\"the test\", format=\"pdf\")\n",
    "    dot.graph_attr['rankdir'] = 'LR'\n",
    "    # dot.node(name='a', label='wo', color='purple')\n",
    "    # dot.node(name='b', label='niu', color='purple')\n",
    "    # dot.node(name='c', label='che', color='purple')\n",
    "    \n",
    "    total_nodes+=len(communities[c].nodes)\n",
    "    total_edges+=len(communities[c].edges)\n",
    "    \n",
    "    subgraph_loss_sum=0\n",
    "    attack_node_count=0\n",
    "    attack_edge_count=0\n",
    "    for e in communities[c].edges:\n",
    "        try:\n",
    "            temp_edge=gg.edges[e]\n",
    "            srcnode=e['srcnode']\n",
    "            dstnode=e['dstnode']\n",
    "        except:\n",
    "            pass        \n",
    "\n",
    "        if True:\n",
    "            subgraph_loss_sum+=temp_edge['loss']\n",
    "\n",
    "    #     g.add_edge(srcnode,dstnode,srcmsg=node2msg[indexid2nodeid[srcnode]],dstmsg=node2msg[indexid2nodeid[dstnode]],loss=df['loss'][i])\n",
    "\n",
    "\n",
    "    #         dot.node( name=str(srcnode),label=str(node_index_id2msg_mal[srcnode]), color='purple',shape = 'box')\n",
    "            if \"'subject': '\" in temp_edge['srcmsg']:\n",
    "                src_shape='box'\n",
    "            elif \"'file': '\" in temp_edge['srcmsg']:\n",
    "                src_shape='oval'\n",
    "            elif \"'netflow': '\" in temp_edge['srcmsg']:\n",
    "                src_shape='diamond'\n",
    "                \n",
    "            src_shape='box'\n",
    "            if attack_edge_flag(temp_edge['srcmsg']):\n",
    "                src_node_color='red'\n",
    "                total_nodes+=1\n",
    "            else:\n",
    "                src_node_color='blue'\n",
    "                \n",
    "            src_node_color='blue'\n",
    "            \n",
    "            dot.node( name=str(hashgen(replace_path_name(temp_edge['srcmsg']))),\n",
    "                     label=str(replace_path_name(temp_edge['srcmsg'])+'\\t partition:'+str(partition[str(hashgen(replace_path_name(temp_edge['srcmsg'])))])), \n",
    "                     color=src_node_color,\n",
    "                     shape = src_shape)\n",
    "\n",
    "\n",
    "            if \"'subject': '\" in temp_edge['dstmsg']:\n",
    "                dst_shape='box'\n",
    "            elif \"'file': '\" in temp_edge['dstmsg']:\n",
    "                dst_shape='oval'\n",
    "            elif \"'netflow': '\" in temp_edge['dstmsg']:\n",
    "                dst_shape='diamond'\n",
    "\n",
    "                    \n",
    "            if \"->\" in temp_edge['dstmsg']:\n",
    "                dst_shape='diamond'\n",
    "            else:\n",
    "                dst_shape='oval'\n",
    "                \n",
    "                \n",
    "            if attack_edge_flag(temp_edge['dstmsg']):\n",
    "                dst_node_color='red'\n",
    "                total_nodes+=1\n",
    "            else:\n",
    "                dst_node_color='blue'\n",
    "            dst_node_color='blue'\n",
    "            \n",
    "            \n",
    "            dot.node( name=str(hashgen(replace_path_name(temp_edge['dstmsg']))),\n",
    "                     label=str(replace_path_name(temp_edge['dstmsg'])+'\\t partition:'+str(partition[str(hashgen(replace_path_name(temp_edge['dstmsg'])))])), \n",
    "                     color=dst_node_color,\n",
    "                     shape = dst_shape)\n",
    "\n",
    "\n",
    "    #         edgeindex=tensor_find(test_data.msg[i][16:-16],1)\n",
    "    \n",
    "            temp_edge_visual=replace_path_name(temp_edge['srcmsg'])+','+replace_path_name(temp_edge['dstmsg'])+','+str(temp_edge['time'])\n",
    "            temp_edge_hash_val=hashgen(temp_edge_visual)\n",
    "            \n",
    "            if temp_edge_hash_val in edges_attack_hashset_h051:\n",
    "                edge_color='red'\n",
    "                attck_nodes_set_visual.add(replace_path_name(temp_edge['srcmsg']))\n",
    "                attck_nodes_set_visual.add(replace_path_name(temp_edge['dstmsg']))\n",
    "                attack_edge_count+=1\n",
    "            else:\n",
    "                edge_color='blue'\n",
    "            \n",
    "#             if attack_edge_flag(temp_edge['srcmsg']) and attack_edge_flag(temp_edge['dstmsg']):\n",
    "#                 edge_color='red'\n",
    "#                 attack_edge_count+=1\n",
    "#             else:\n",
    "#                 edge_color='blue'\n",
    "                \n",
    "                \n",
    "            dot.edge(str(hashgen(replace_path_name(temp_edge['srcmsg']))),str(hashgen(replace_path_name(temp_edge['dstmsg']))), label= temp_edge['edge_type'] , color=edge_color)#+ \"  loss: \"+str(temp_edge['loss']) + \"  time: \"+str(temp_edge['time'])\n",
    "\n",
    "    #         dot.edge(str(srcnode), str(dstnode), label= temp_edge['edge_type']+ \"  loss: \"+str((temp_edge['loss'])) + \"  time: \"+str(temp_edge['time']) , color='red')\n",
    "\n",
    "\n",
    "#     if len(communities[c].edges)<2:\n",
    "#         continue\n",
    "#     if len(communities[c].edges)>1000:\n",
    "#         continue\n",
    "#         print(f\"edge num:{len(communities[c].edges)}，skip rendering\")\n",
    "        \n",
    "    print(\"Start to render the figures···\")\n",
    "    \n",
    "    dot.render('./graph_visual_h051/subgraph_'+str(graph_index), view=False)\n",
    "    print(\"subgraph loss:\",(subgraph_loss_sum/len(communities[c].edges)))\n",
    "    print(\"graph_index:\",graph_index)\n",
    "    print(\"edge_count:\",len(communities[c].edges))\n",
    "    print(\"node_count:\",len(communities[c].nodes))\n",
    "    print(f\"{attack_node_count=}\")\n",
    "    print(f\"{attack_edge_count=}\")\n",
    "    graph_index+=1\n",
    "\n",
    "print(f\"avg edges:{total_edges/len(communities)}\")\n",
    "print(f\"avg nodes:{total_nodes/len(communities)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kairos",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "236.766px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

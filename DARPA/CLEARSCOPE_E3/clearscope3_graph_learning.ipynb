{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cs/grad/opumni/Fall2024/COMP7860_Project/.conda/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# encoding=utf-8\n",
    "import os.path as osp\n",
    "import os\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch.nn import Linear\n",
    "from sklearn.metrics import average_precision_score, roc_auc_score\n",
    "from torch_geometric.data import TemporalData\n",
    "from torch_geometric.datasets import JODIEDataset\n",
    "from torch_geometric.datasets import ICEWS18\n",
    "from torch_geometric.nn import TGNMemory, TransformerConv\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn.models.tgn import (LastNeighborLoader, IdentityMessage, MeanAggregator,\n",
    "                                           LastAggregator)\n",
    "from torch_geometric import *\n",
    "from torch_geometric.utils import negative_sampling\n",
    "from tqdm import tqdm\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import math\n",
    "import copy\n",
    "import re\n",
    "import time\n",
    "import json\n",
    "import pandas as pd\n",
    "from random import choice\n",
    "import gc\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# device = 'cpu'\n",
    "# msg structure:      [src_node_feature,edge_attr,dst_node_feature]\n",
    "\n",
    "# compute the best partition\n",
    "import datetime\n",
    "# import community as community_louvain\n",
    "\n",
    "import xxhash\n",
    "\n",
    "# Find the edge index which the edge vector is corresponding to\n",
    "def tensor_find(t,x):\n",
    "    \"\"\"\n",
    "    Find the 1-based row index of the first occurrence of a value in a PyTorch tensor.\n",
    "\n",
    "    Parameters:\n",
    "    t (torch.Tensor): The PyTorch tensor to search.\n",
    "    x (Any): The value to locate within the tensor.\n",
    "\n",
    "    Returns:\n",
    "    int: The 1-based row index of the first occurrence of the value in the tensor.\n",
    "    \"\"\"\n",
    "    t_np=t.cpu().numpy()\n",
    "    idx=np.argwhere(t_np==x)\n",
    "    return idx[0][0]+1\n",
    "\n",
    "\n",
    "def std(t):\n",
    "    \"\"\"\n",
    "    Calculate the standard deviation of elements in the input data.\n",
    "\n",
    "    Parameters:\n",
    "    t (list, tuple, or ndarray): The input data to compute the standard deviation.\n",
    "                                 This can be a Python list, tuple, or a NumPy ndarray.\n",
    "\n",
    "    Returns:\n",
    "    float: The standard deviation of the input data.\n",
    "    \"\"\"\n",
    "    t = np.array(t)\n",
    "    return np.std(t)\n",
    "\n",
    "\n",
    "def var(t):\n",
    "    \"\"\"\n",
    "    Calculate the variance of elements in the input data.\n",
    "\n",
    "    Parameters:\n",
    "    t (list, tuple, or ndarray): The input data to compute the variance.\n",
    "                                 This can be a Python list, tuple, or a NumPy ndarray.\n",
    "\n",
    "    Returns:\n",
    "    float: The variance of the input data.\n",
    "    \"\"\"\n",
    "    t = np.array(t)\n",
    "    return np.var(t)\n",
    "\n",
    "\n",
    "def mean(t):\n",
    "    \"\"\"\n",
    "    Calculate the mean of elements in the input data.\n",
    "\n",
    "    Parameters:\n",
    "    t (list, tuple, or ndarray): The input data to compute the mean.\n",
    "                                 This can be a Python list, tuple, or a NumPy ndarray.\n",
    "\n",
    "    Returns:\n",
    "    float: The mean of the input data.\n",
    "    \"\"\"\n",
    "    t = np.array(t)\n",
    "    return np.mean(t)\n",
    "\n",
    "def hashgen(l):\n",
    "    \"\"\"\n",
    "    Generate a single hash value from a list of string values.\n",
    "\n",
    "    Parameters:\n",
    "    l (list of str): A list of string values, which can represent properties of a node or edge.\n",
    "\n",
    "    Returns:\n",
    "    int: A single hashed integer value generated from the input list.\n",
    "    \"\"\"\n",
    "    hasher = xxhash.xxh64()\n",
    "    for e in l:\n",
    "        hasher.update(e)\n",
    "    return hasher.intdigest()\n",
    "\n",
    "\n",
    "def cal_pos_edges_loss(link_pred_ratio):\n",
    "    \"\"\"\n",
    "    Calculate the loss for positive edges using a binary classification approach.\n",
    "\n",
    "    Parameters:\n",
    "    link_pred_ratio (list or Tensor): A list or tensor containing predicted link probabilities.\n",
    "\n",
    "    Returns:\n",
    "    Tensor: A tensor containing the computed loss for each prediction.\n",
    "    \"\"\"\n",
    "    loss = []\n",
    "    for i in link_pred_ratio:\n",
    "        # Compare predicted values with a target tensor of ones (positive class)\n",
    "        loss.append(criterion(i, torch.ones(1)))\n",
    "    return torch.tensor(loss)\n",
    "\n",
    "\n",
    "def cal_pos_edges_loss_multiclass(link_pred_ratio, labels):\n",
    "    \"\"\"\n",
    "    Calculate the loss for positive edges in a multi-class classification setting.\n",
    "\n",
    "    Parameters:\n",
    "    link_pred_ratio (list of Tensors): A list of tensors containing predicted class probabilities for links.\n",
    "    labels (list of Tensors): A list of tensors containing the ground truth labels for the links.\n",
    "\n",
    "    Returns:\n",
    "    Tensor: A tensor containing the computed loss for each prediction.\n",
    "    \"\"\"\n",
    "    loss = []\n",
    "    for i in range(len(link_pred_ratio)):\n",
    "        # Compare predicted values with ground truth labels for multi-class classification\n",
    "        loss.append(criterion(link_pred_ratio[i].reshape(1, -1), labels[i].reshape(-1)))\n",
    "    return torch.tensor(loss)\n",
    "\n",
    "\n",
    "def cal_pos_edges_loss_autoencoder(decoded, msg):\n",
    "    \"\"\"\n",
    "    Calculate the loss for positive edges in an autoencoder setting.\n",
    "\n",
    "    Parameters:\n",
    "    decoded (list of Tensors): A list of tensors representing the decoded outputs.\n",
    "    msg (list of Tensors): A list of tensors representing the original messages (inputs to the autoencoder).\n",
    "\n",
    "    Returns:\n",
    "    Tensor: A tensor containing the computed loss for each decoded message.\n",
    "    \"\"\"\n",
    "    loss = []\n",
    "    for i in range(len(decoded)):\n",
    "        # Compare the decoded outputs with the original messages\n",
    "        loss.append(criterion(decoded[i].reshape(1, -1), msg[i].reshape(-1)))\n",
    "    return torch.tensor(loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": "IPython.notebook.set_autosave_interval(120000)"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autosaving every 120 seconds\n"
     ]
    }
   ],
   "source": [
    "%autosave 120  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timezone\n",
    "import time\n",
    "import pytz\n",
    "from time import mktime\n",
    "from datetime import datetime\n",
    "import time\n",
    "def ns_time_to_datetime(ns):\n",
    "    \"\"\"\n",
    "    :param ns: int nano timestamp\n",
    "    :return: datetime   format: 2013-10-10 23:40:00.000000000\n",
    "    \"\"\"\n",
    "    dt = datetime.fromtimestamp(int(ns) // 1000000000)\n",
    "    s = dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "    s += '.' + str(int(int(ns) % 1000000000)).zfill(9)\n",
    "    return s\n",
    "\n",
    "def ns_time_to_datetime_US(ns):\n",
    "    \"\"\"\n",
    "    :param ns: int nano timestamp\n",
    "    :return: datetime   format: 2013-10-10 23:40:00.000000000\n",
    "    \"\"\"\n",
    "    tz = pytz.timezone('US/Eastern')\n",
    "    dt = pytz.datetime.datetime.fromtimestamp(int(ns) // 1000000000, tz)\n",
    "    s = dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "    s += '.' + str(int(int(ns) % 1000000000)).zfill(9)\n",
    "    return s\n",
    "\n",
    "def time_to_datetime_US(s):\n",
    "    \"\"\"\n",
    "    :param ns: int nano timestamp\n",
    "    :return: datetime   format: 2013-10-10 23:40:00\n",
    "    \"\"\"\n",
    "    tz = pytz.timezone('US/Eastern')\n",
    "    dt = pytz.datetime.datetime.fromtimestamp(int(s), tz)\n",
    "    s = dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "    return s\n",
    "\n",
    "def datetime_to_ns_time(date):\n",
    "    \"\"\"\n",
    "    :param date: str   format: %Y-%m-%d %H:%M:%S   e.g. 2013-10-10 23:40:00\n",
    "    :return: nano timestamp\n",
    "    \"\"\"\n",
    "    timeArray = time.strptime(date, \"%Y-%m-%d %H:%M:%S\")\n",
    "    timeStamp = int(time.mktime(timeArray))\n",
    "    timeStamp = timeStamp * 1000000000\n",
    "    return timeStamp\n",
    "\n",
    "def datetime_to_ns_time_US(date):\n",
    "    \"\"\"\n",
    "    :param date: str   format: %Y-%m-%d %H:%M:%S   e.g. 2013-10-10 23:40:00\n",
    "    :return: nano timestamp\n",
    "    \"\"\"\n",
    "    tz = pytz.timezone('US/Eastern')\n",
    "    timeArray = time.strptime(date, \"%Y-%m-%d %H:%M:%S\")\n",
    "    dt = datetime.fromtimestamp(mktime(timeArray))\n",
    "    timestamp = tz.localize(dt)\n",
    "    timestamp = timestamp.timestamp()\n",
    "    timeStamp = timestamp * 1000000000\n",
    "    return int(timeStamp)\n",
    "\n",
    "def datetime_to_timestamp_US(date):\n",
    "    \"\"\"\n",
    "    :param date: str   format: %Y-%m-%d %H:%M:%S   e.g. 2013-10-10 23:40:00\n",
    "    :return: nano timestamp\n",
    "    \"\"\"\n",
    "    tz = pytz.timezone('US/Eastern')\n",
    "    timeArray = time.strptime(date, \"%Y-%m-%d %H:%M:%S\")\n",
    "    dt = datetime.fromtimestamp(mktime(timeArray))\n",
    "    timestamp = tz.localize(dt)\n",
    "    timestamp = timestamp.timestamp()\n",
    "    timeStamp = timestamp\n",
    "    return int(timeStamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "\n",
    "from psycopg2 import extras as ex\n",
    "connect = psycopg2.connect(database = 'tc_clearscope3_dataset_db',\n",
    "                           host = 'localhost',\n",
    "                           user = 'postgres',\n",
    "                           password = '123456',\n",
    "                           port = '5432'\n",
    "                          )\n",
    "\n",
    "cur = connect.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_4_4=torch.load(\"./train_graphs/graph_4_4.TemporalData.simple\").to(device=device)\n",
    "graph_4_5=torch.load(\"./train_graphs/graph_4_5.TemporalData.simple\").to(device=device)\n",
    "graph_4_6=torch.load(\"./train_graphs/graph_4_6.TemporalData.simple\").to(device=device)\n",
    "\n",
    "train_data=graph_4_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constructing the map for nodeid to msg\n",
    "sql=\"select * from node2id ORDER BY index_id;\"\n",
    "cur.execute(sql)\n",
    "rows = cur.fetchall()\n",
    "\n",
    "# node hash => nodeid (index) and nodeid (index) => msg (dictionary)\n",
    "nodeid2msg={}\n",
    "for i in rows:\n",
    "    nodeid2msg[i[0]]=i[-1]\n",
    "    nodeid2msg[i[-1]]={i[1]:i[2]}  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "rel2id={1: 'EVENT_CLOSE',\n",
    " 'EVENT_CLOSE': 1,\n",
    " 2: 'EVENT_OPEN',\n",
    " 'EVENT_OPEN': 2,\n",
    " 3: 'EVENT_READ',\n",
    " 'EVENT_READ': 3,\n",
    " 4: 'EVENT_WRITE',\n",
    " 'EVENT_WRITE': 4,\n",
    " 5: 'EVENT_RECVFROM',\n",
    " 'EVENT_RECVFROM': 5,\n",
    " 6: 'EVENT_RECVMSG',\n",
    " 'EVENT_RECVMSG': 6,\n",
    " 7: 'EVENT_SENDMSG',\n",
    " 'EVENT_SENDMSG': 7,\n",
    " 8: 'EVENT_SENDTO',\n",
    " 'EVENT_SENDTO': 8}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data, val_data, test_data = data.train_val_test_split(val_ratio=0.15, test_ratio=0.15)\n",
    "# max_node_num = max(torch.cat([data.dst,data.src]))+1\n",
    "# max_node_num = data.num_nodes+1\n",
    "max_node_num = 172724  # +1\n",
    "# min_dst_idx, max_dst_idx = int(data.dst.min()), int(data.dst.max())\n",
    "min_dst_idx, max_dst_idx = 0, max_node_num\n",
    "\n",
    "# The LastNeighborLoader is used in TGN models to handle neighbor sampling dynamically by storing and retrieving the most recent neighbors of nodes in the graph. \n",
    "# it receives total number of nodes to store, and max number of neighbors to store for each node\n",
    "neighbor_loader = LastNeighborLoader(max_node_num, size=20, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphAttentionEmbedding(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    The GraphAttentionEmbedding class implements a graph attention-based \n",
    "    embedding model using a two-layer TransformerConv. \n",
    "    It is designed for temporal graph networks, where the embedding considers \n",
    "    both node features and edge attributes, including temporal information.\n",
    "\n",
    "    Parameters:\n",
    "    - in_channels (int): Input feature dimensionality for nodes.\n",
    "    - out_channels (int): Output feature dimensionality for nodes.\n",
    "    - msg_dim (int): Dimensionality of edge message features.\n",
    "    - time_enc (TimeEncoder): Time encoding module.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels, msg_dim, time_enc):\n",
    "        super(GraphAttentionEmbedding, self).__init__()\n",
    "        self.time_enc = time_enc\n",
    "        edge_dim = msg_dim + time_enc.out_channels\n",
    "\n",
    "        # First TransformerConv layer with 8 heads\n",
    "        self.conv = TransformerConv(in_channels, out_channels, heads=8,\n",
    "                                    dropout=0.0, edge_dim=edge_dim)\n",
    "        \n",
    "        # Second TransformerConv layer with 1 head, no concatenation\n",
    "        self.conv2 = TransformerConv(out_channels*8, out_channels,heads=1, concat=False,\n",
    "                             dropout=0.0, edge_dim=edge_dim)\n",
    "\n",
    "    def forward(self, x, last_update, edge_index, t, msg):\n",
    "        \"\"\"\n",
    "        Forward pass of the GraphAttentionEmbedding model.\n",
    "\n",
    "        Parameters:\n",
    "        - x (torch.Tensor): Node features of shape (num_nodes, in_channels).\n",
    "        - last_update (torch.Tensor): Timestamps of last updates for each node.\n",
    "        - edge_index (torch.Tensor): Edge index in COO format of shape (2, num_edges).\n",
    "        - t (torch.Tensor): Edge timestamps of shape (num_edges,).\n",
    "        - msg (torch.Tensor): Edge message features of shape (num_edges, msg_dim).\n",
    "\n",
    "        Returns:\n",
    "        - torch.Tensor: Updated node embeddings of shape (num_nodes, out_channels).\n",
    "        \"\"\"\n",
    "        last_update.to(device)\n",
    "        x = x.to(device)\n",
    "        t = t.to(device)\n",
    "\n",
    "        # Compute relative time differences and encode them\n",
    "        rel_t = last_update[edge_index[0]] - t\n",
    "        rel_t_enc = self.time_enc(rel_t.to(x.dtype))\n",
    "\n",
    "        # Concatenate relative time encoding and message features as edge attributes\n",
    "        edge_attr = torch.cat([rel_t_enc, msg], dim=-1)\n",
    "\n",
    "        # Apply the first TransformerConv layer with ReLU activation\n",
    "        x = F.relu(self.conv(x, edge_index, edge_attr))\n",
    "\n",
    "        # Apply the second TransformerConv layer with ReLU activation\n",
    "        x = F.relu(self.conv2(x, edge_index, edge_attr))\n",
    "        return x\n",
    "\n",
    "\n",
    "class LinkPredictor(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    A neural network for Event Type pediction tasks, transforming node embeddings\n",
    "    and predicting link properties.\n",
    "\n",
    "    Parameters:\n",
    "    - in_channels (int): The dimensionality of the input node embeddings.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels):\n",
    "        super(LinkPredictor, self).__init__()\n",
    "        \n",
    "        # Linear transformations for source and destination embeddings\n",
    "        self.lin_src = Linear(in_channels, in_channels*2)\n",
    "        self.lin_dst = Linear(in_channels, in_channels*2)\n",
    "        \n",
    "        # Sequential feedforward network for link prediction\n",
    "        self.lin_seq = nn.Sequential(\n",
    "            Linear(in_channels*4, in_channels*8),\n",
    "            torch.nn.Dropout(0.5),\n",
    "            nn.Tanh(),\n",
    "            Linear(in_channels*8, in_channels*2),\n",
    "            torch.nn.Dropout(0.5),\n",
    "            nn.Tanh(),\n",
    "            Linear(in_channels*2, int(in_channels//2)),\n",
    "            torch.nn.Dropout(0.5),\n",
    "            nn.Tanh(),\n",
    "\n",
    "            # Final prediction layer\n",
    "            # train_data.msg contains feature vector of src and dest and edge (event) information.\n",
    "            # train_data.msg.shape[1]-32 is the size of the edge (event) info\n",
    "            Linear(int(in_channels//2), train_data.msg.shape[1]-32)                   \n",
    "        )\n",
    "        \n",
    "\n",
    "    def forward(self, z_src, z_dst):\n",
    "        \"\"\"\n",
    "        Forward pass for the LinkPredictor.\n",
    "\n",
    "        Parameters:\n",
    "        - z_src (torch.Tensor): Source node embeddings, shape (batch_size, in_channels).\n",
    "        - z_dst (torch.Tensor): Destination node embeddings, shape (batch_size, in_channels).\n",
    "\n",
    "        Returns:\n",
    "        - torch.Tensor: Predicted Edge (event) properties, shape (batch_size, train_data.msg.shape[1] - 32).\n",
    "        \"\"\"\n",
    "        h = torch.cat([self.lin_src(z_src) , self.lin_dst(z_dst)],dim=-1)      \n",
    "         \n",
    "        h = self.lin_seq (h)\n",
    "        \n",
    "        return h\n",
    "\n",
    "memory_dim = 100         # node state\n",
    "time_dim = 100\n",
    "embedding_dim = 100      # edge embedding\n",
    "\n",
    "# create the memory\n",
    "memory = TGNMemory(\n",
    "    max_node_num,\n",
    "    train_data.msg.size(-1),\n",
    "    memory_dim,\n",
    "    time_dim,\n",
    "    message_module=IdentityMessage(train_data.msg.size(-1), memory_dim, time_dim),\n",
    "    aggregator_module=LastAggregator(),\n",
    ").to(device)\n",
    "\n",
    "# create the graph neural network with transformer\n",
    "gnn = GraphAttentionEmbedding(\n",
    "    in_channels=memory_dim,\n",
    "    out_channels=embedding_dim,\n",
    "    msg_dim=train_data.msg.size(-1),\n",
    "    time_enc=memory.time_enc,\n",
    ").to(device)\n",
    "\n",
    "# create the MLP link predictor\n",
    "link_pred = LinkPredictor(in_channels=embedding_dim).to(device)\n",
    "\n",
    "# set Adam optimizer for all 3 networks\n",
    "optimizer = torch.optim.Adam(\n",
    "    set(memory.parameters()) | set(gnn.parameters())\n",
    "    | set(link_pred.parameters()), lr=0.00005, eps=1e-08,weight_decay=0.01)\n",
    "\n",
    "\n",
    "# scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Helper vector to map global node indices to local ones.\n",
    "assoc = torch.empty(max_node_num, dtype=torch.long, device=device)\n",
    "\n",
    "saved_nodes=set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH=1024\n",
    "\n",
    "def train(train_data):\n",
    "    \"\"\"\n",
    "    Trains the Temporal Graph Network (TGN) using sequential batches of data.\n",
    "\n",
    "    Parameters:\n",
    "    - train_data: Dataset object with sequential batches, timestamps, and messages.\n",
    "\n",
    "    Returns:\n",
    "    - float: Average loss over all events in the training data.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Set networks to training mode\n",
    "    memory.train()\n",
    "    gnn.train()\n",
    "    link_pred.train()\n",
    "\n",
    "    memory.reset_state()  # Start with a fresh memory.\n",
    "    neighbor_loader.reset_state()  # Start with an empty graph.\n",
    "    saved_nodes=set()\n",
    "\n",
    "    # Tracks total loss over the training data\n",
    "    total_loss = 0\n",
    "    \n",
    "    # Process each batch in the training dataset\n",
    "    for batch in train_data.seq_batches(batch_size=BATCH):\n",
    "        # Reset gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Extract batch data: source, destination, timestamps, and messages\n",
    "        src, pos_dst, t, msg = batch.src, batch.dst, batch.t, batch.msg        \n",
    "        \n",
    "        # Retrieve unique nodes involved in the batch and their neighbors\n",
    "        n_id = torch.cat([src, pos_dst]).unique()\n",
    "        n_id, edge_index, e_id = neighbor_loader(n_id)\n",
    "\n",
    "        # Helper vector to map global node indices to local ones.\n",
    "        assoc[n_id] = torch.arange(n_id.size(0), device=device)\n",
    "\n",
    "        # Get updated memory of all nodes involved in the computation.\n",
    "        z, last_update = memory(n_id)\n",
    "        \n",
    "        # Pass embeddings through the GNN to update them\n",
    "        z = gnn(z, last_update, edge_index, train_data.t[e_id], train_data.msg[e_id])\n",
    "        \n",
    "        # Compute predictions for positive edges\n",
    "        pos_out = link_pred(z[assoc[src]], z[assoc[pos_dst]])       \n",
    "\n",
    "         # Concatenate predictions\n",
    "        y_pred = torch.cat([pos_out], dim=0)\n",
    "        \n",
    "        # Ground-truth label generation using tensor_find\n",
    "        # msg contains feature vector of src and dest and edge (event) information.\n",
    "        # extracting event info from them\n",
    "        y_true=[]\n",
    "        for m in msg:\n",
    "            l=tensor_find(m[16:-16],1)-1\n",
    "            y_true.append(l)           \n",
    "          \n",
    "        y_true = torch.tensor(y_true).to(device=device)\n",
    "        y_true=y_true.reshape(-1).to(torch.long).to(device=device)\n",
    "        \n",
    "        # Compute loss using predictions and ground-truth labels\n",
    "        loss = criterion(y_pred, y_true)\n",
    "        \n",
    "        # Update memory and neighbor loader with ground-truth state.\n",
    "        memory.update_state(src, pos_dst, t, msg)\n",
    "        neighbor_loader.insert(src, pos_dst)\n",
    "        \n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Detach memory to free computation graph\n",
    "        memory.detach()\n",
    "\n",
    "        # Accumulate total loss scaled by the number of events in the batch\n",
    "        total_loss += float(loss) * batch.num_events\n",
    "\n",
    "    # Return the average loss over all events\n",
    "    return total_loss / train_data.num_events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/30 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch: 01, Loss: 1.3058\n",
      "  Epoch: 01, Loss: 1.0811\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 1/30 [02:55<1:24:54, 175.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch: 01, Loss: 1.0880\n",
      "  Epoch: 02, Loss: 0.8895\n",
      "  Epoch: 02, Loss: 0.9006\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 2/30 [05:59<1:24:09, 180.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch: 02, Loss: 1.0012\n",
      "  Epoch: 03, Loss: 0.8326\n",
      "  Epoch: 03, Loss: 0.8404\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 3/30 [09:43<1:30:15, 200.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch: 03, Loss: 0.9658\n",
      "  Epoch: 04, Loss: 0.8042\n",
      "  Epoch: 04, Loss: 0.8003\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 4/30 [13:49<1:34:41, 218.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch: 04, Loss: 0.9366\n",
      "  Epoch: 05, Loss: 0.7862\n",
      "  Epoch: 05, Loss: 0.7731\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 5/30 [17:54<1:34:55, 227.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch: 05, Loss: 0.9189\n",
      "  Epoch: 06, Loss: 0.7727\n",
      "  Epoch: 06, Loss: 0.7500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 6/30 [21:54<1:32:46, 231.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch: 06, Loss: 0.9024\n",
      "  Epoch: 07, Loss: 0.7597\n",
      "  Epoch: 07, Loss: 0.7283\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 7/30 [25:50<1:29:27, 233.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch: 07, Loss: 0.8883\n",
      "  Epoch: 08, Loss: 0.7491\n",
      "  Epoch: 08, Loss: 0.7126\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 8/30 [30:08<1:28:24, 241.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch: 08, Loss: 0.8781\n",
      "  Epoch: 09, Loss: 0.7408\n",
      "  Epoch: 09, Loss: 0.6998\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 9/30 [34:51<1:29:01, 254.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch: 09, Loss: 0.8721\n",
      "  Epoch: 10, Loss: 0.7343\n",
      "  Epoch: 10, Loss: 0.6919\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 10/30 [39:45<1:28:53, 266.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch: 10, Loss: 0.8658\n",
      "  Epoch: 11, Loss: 0.7295\n",
      "  Epoch: 11, Loss: 0.6853\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|███▋      | 11/30 [45:21<1:31:07, 287.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch: 11, Loss: 0.8643\n",
      "  Epoch: 12, Loss: 0.7255\n",
      "  Epoch: 12, Loss: 0.6794\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 12/30 [51:00<1:31:00, 303.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch: 12, Loss: 0.8484\n",
      "  Epoch: 13, Loss: 0.7210\n",
      "  Epoch: 13, Loss: 0.6752\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 13/30 [56:40<1:29:06, 314.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch: 13, Loss: 0.8290\n",
      "  Epoch: 14, Loss: 0.7157\n",
      "  Epoch: 14, Loss: 0.6648\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████▋     | 14/30 [1:02:32<1:26:51, 325.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch: 14, Loss: 0.8190\n",
      "  Epoch: 15, Loss: 0.7116\n",
      "  Epoch: 15, Loss: 0.6609\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 15/30 [1:08:22<1:23:15, 333.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch: 15, Loss: 0.8141\n",
      "  Epoch: 16, Loss: 0.7095\n",
      "  Epoch: 16, Loss: 0.6582\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████▎    | 16/30 [1:14:25<1:19:50, 342.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch: 16, Loss: 0.8110\n",
      "  Epoch: 17, Loss: 0.7075\n",
      "  Epoch: 17, Loss: 0.6550\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████▋    | 17/30 [1:20:28<1:15:28, 348.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch: 17, Loss: 0.8071\n",
      "  Epoch: 18, Loss: 0.7055\n",
      "  Epoch: 18, Loss: 0.6536\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 18/30 [1:26:31<1:10:31, 352.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch: 18, Loss: 0.8060\n",
      "  Epoch: 19, Loss: 0.7047\n",
      "  Epoch: 19, Loss: 0.6511\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|██████▎   | 19/30 [1:32:44<1:05:46, 358.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch: 19, Loss: 0.8038\n",
      "  Epoch: 20, Loss: 0.7039\n",
      "  Epoch: 20, Loss: 0.6504\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 20/30 [1:38:50<1:00:11, 361.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch: 20, Loss: 0.8012\n",
      "  Epoch: 21, Loss: 0.7038\n",
      "  Epoch: 21, Loss: 0.6506\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 21/30 [1:45:03<54:40, 364.45s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch: 21, Loss: 0.8009\n",
      "  Epoch: 22, Loss: 0.7023\n",
      "  Epoch: 22, Loss: 0.6474\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|███████▎  | 22/30 [1:51:16<48:57, 367.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch: 22, Loss: 0.7991\n",
      "  Epoch: 23, Loss: 0.7012\n",
      "  Epoch: 23, Loss: 0.6481\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|███████▋  | 23/30 [1:57:24<42:51, 367.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch: 23, Loss: 0.7984\n",
      "  Epoch: 24, Loss: 0.7000\n",
      "  Epoch: 24, Loss: 0.6456\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 24/30 [2:03:38<36:55, 369.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch: 24, Loss: 0.7973\n",
      "  Epoch: 25, Loss: 0.6996\n",
      "  Epoch: 25, Loss: 0.6445\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 25/30 [2:09:54<30:57, 371.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch: 25, Loss: 0.7955\n",
      "  Epoch: 26, Loss: 0.6994\n",
      "  Epoch: 26, Loss: 0.6443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|████████▋ | 26/30 [2:15:54<24:32, 368.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch: 26, Loss: 0.7954\n",
      "  Epoch: 27, Loss: 0.6984\n",
      "  Epoch: 27, Loss: 0.6435\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 27/30 [2:22:05<18:26, 368.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch: 27, Loss: 0.7940\n",
      "  Epoch: 28, Loss: 0.6985\n",
      "  Epoch: 28, Loss: 0.6422\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|█████████▎| 28/30 [2:28:20<12:21, 370.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch: 28, Loss: 0.7935\n",
      "  Epoch: 29, Loss: 0.6973\n",
      "  Epoch: 29, Loss: 0.6418\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|█████████▋| 29/30 [2:34:37<06:12, 372.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch: 29, Loss: 0.7924\n",
      "  Epoch: 30, Loss: 0.6965\n",
      "  Epoch: 30, Loss: 0.6409\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [2:40:51<00:00, 321.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch: 30, Loss: 0.7918\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# train on benign graphs\n",
    "train_graphs=[graph_4_4, graph_4_5, graph_4_6]\n",
    "\n",
    "# train fro 30 epochs\n",
    "for epoch in tqdm(range(1, 31)):\n",
    "    for g in train_graphs:\n",
    "        loss = train(g)\n",
    "        print(f'  Epoch: {epoch:02d}, Loss: {loss:.4f}')\n",
    "\n",
    "# store the models in file\n",
    "model=[memory,gnn, link_pred,neighbor_loader]\n",
    "os.system(\"mkdir -p ./models/\")\n",
    "torch.save(model,\"./models/model_saved_share.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time \n",
    "\n",
    "# Disable gradient computation for inference\n",
    "@torch.no_grad()\n",
    "\n",
    "def test_day_new(inference_data, path):\n",
    "    \"\"\"\n",
    "    Evaluate the Temporal Graph Network (TGN) on inference data.\n",
    "    Stores result for each time window in txt file.\n",
    "    Each text file contains event information. \n",
    "    Example: {'loss': 2.8073678016662598, 'srcnode': 76938, 'dstnode': 868, 'srcmsg': \"{'subject': 'system_server'}\", 'dstmsg': \"{'file': '/data/system/sync/pending.xml'}\", 'edge_type': 'EVENT_OPEN', 'time': 1522814400030000000}\n",
    "\n",
    "    Parameters:\n",
    "    - inference_data: Dataset containing sequential batches for temporal graph inference.\n",
    "    - path (str): Path to save logs and results.\n",
    "\n",
    "    Returns:\n",
    "    - dict: Summary of results, including loss and timing for each time window.\n",
    "    \"\"\"\n",
    "\n",
    "    if os.path.exists(path):\n",
    "        pass\n",
    "    else:\n",
    "        os.mkdir(path)\n",
    "    \n",
    "    # Set networks to evaluation mode\n",
    "    memory.eval()\n",
    "    gnn.eval()\n",
    "    link_pred.eval()\n",
    "    \n",
    "    memory.reset_state()  # Start with a fresh memory.  \n",
    "    neighbor_loader.reset_state()  # Start with an empty graph.\n",
    "    \n",
    "    # Initialize tracking variables\n",
    "    time_with_loss={}\n",
    "    total_loss = 0    \n",
    "    edge_list=[]\n",
    "    \n",
    "    unique_nodes=torch.tensor([]).to(device=device)\n",
    "    total_edges=0\n",
    "\n",
    "\n",
    "    start_time=inference_data.t[0]\n",
    "    event_count=0\n",
    "    \n",
    "    pos_o=[]\n",
    "    loss_list=[]\n",
    "\n",
    "    print(\"after merge:\",inference_data)\n",
    "    \n",
    "    # Record the running time to evaluate the performance\n",
    "    start = time.perf_counter()\n",
    "\n",
    "    for batch in inference_data.seq_batches(batch_size=BATCH):\n",
    "        \n",
    "        src, pos_dst, t, msg = batch.src, batch.dst, batch.t, batch.msg\n",
    "        unique_nodes=torch.cat([unique_nodes,src,pos_dst]).unique()\n",
    "        total_edges+=BATCH\n",
    "        \n",
    "        # Retrieve embeddings and neighbors\n",
    "        n_id = torch.cat([src, pos_dst]).unique()       \n",
    "        n_id, edge_index, e_id = neighbor_loader(n_id)\n",
    "        assoc[n_id] = torch.arange(n_id.size(0), device=device)\n",
    "\n",
    "        # Get memory and update embeddings via GNN\n",
    "        z, last_update = memory(n_id)\n",
    "        z = gnn(z, last_update, edge_index, inference_data.t[e_id], inference_data.msg[e_id])\n",
    "\n",
    "        # Predict edge properties\n",
    "        pos_out = link_pred(z[assoc[src]], z[assoc[pos_dst]])\n",
    "        pos_o.append(pos_out)\n",
    "        y_pred = torch.cat([pos_out], dim=0)\n",
    "\n",
    "        # Generate ground-truth labels\n",
    "        y_true=[]\n",
    "        for m in msg:\n",
    "            # Extract label index from the message\n",
    "            l=tensor_find(m[16:-16],1)-1\n",
    "            y_true.append(l) \n",
    "        y_true = torch.tensor(y_true).to(device=device)\n",
    "        y_true=y_true.reshape(-1).to(torch.long).to(device=device)\n",
    "\n",
    "        # Only consider which edge hasn't been correctly predicted.\n",
    "        # For benign graphs, the behaviors patterns are similar and therefore their losses are small\n",
    "        # For anoamlous behaviors, some behaviors might not be seen before, so the probability of predicting those edges are low. Thus their losses are high.\n",
    "        loss = criterion(y_pred, y_true)\n",
    "        total_loss += float(loss) * batch.num_events\n",
    "        \n",
    "        # update the edges in the batch to the memory and neighbor_loader\n",
    "        memory.update_state(src, pos_dst, t, msg)\n",
    "        neighbor_loader.insert(src, pos_dst)\n",
    "        \n",
    "        # compute the loss for each edge\n",
    "        each_edge_loss= cal_pos_edges_loss_multiclass(pos_out,y_true)\n",
    "        \n",
    "        # Process edges for logging\n",
    "        for i in range(len(pos_out)):\n",
    "            srcnode=int(src[i])\n",
    "            dstnode=int(pos_dst[i])  \n",
    "            \n",
    "            srcmsg=str(nodeid2msg[srcnode]) \n",
    "            dstmsg=str(nodeid2msg[dstnode])\n",
    "            t_var=int(t[i])\n",
    "            edgeindex=tensor_find(msg[i][16:-16],1)   \n",
    "            edge_type=rel2id[edgeindex]\n",
    "            loss=each_edge_loss[i]    \n",
    "\n",
    "            temp_dic={}\n",
    "            temp_dic['loss']=float(loss)\n",
    "            temp_dic['srcnode']=srcnode\n",
    "            temp_dic['dstnode']=dstnode\n",
    "            temp_dic['srcmsg']=srcmsg\n",
    "            temp_dic['dstmsg']=dstmsg\n",
    "            temp_dic['edge_type']=edge_type\n",
    "            temp_dic['time']=t_var\n",
    "            \n",
    "            edge_list.append(temp_dic)\n",
    "        \n",
    "        # Check if the time interval is over (15 minutes - Window Size)\n",
    "        event_count+=len(batch.src)\n",
    "        if t[-1]>start_time+60000000000*15:\n",
    "            # Here is a checkpoint, which records all edge losses in the current time window\n",
    "            time_interval=ns_time_to_datetime_US(start_time)+\"~\"+ns_time_to_datetime_US(t[-1])\n",
    "\n",
    "            end = time.perf_counter()\n",
    "            time_with_loss[time_interval]={'loss':loss,\n",
    "                                          'nodes_count':len(unique_nodes),\n",
    "                                          'total_edges':total_edges,\n",
    "                                          'costed_time':(end-start)}\n",
    "            \n",
    "            \n",
    "            log=open(path+\"/\"+time_interval+\".txt\",'w')\n",
    "            \n",
    "            # Compute average loss for the interval\n",
    "            for e in edge_list:\n",
    "                loss+=e['loss']\n",
    "            loss=loss/event_count   \n",
    "\n",
    "            # Save results to log file\n",
    "            print(f'Time: {time_interval}, Loss: {loss:.4f}, Nodes_count: {len(unique_nodes)}, Cost Time: {(end-start):.2f}s')\n",
    "            edge_list = sorted(edge_list, key=lambda x:x['loss'],reverse=True)  # Rank the results based on edge losses\n",
    "            for e in edge_list: \n",
    "                log.write(str(e))\n",
    "                log.write(\"\\n\") \n",
    "            \n",
    "            # Reset tracking variables for the next interval\n",
    "            event_count=0\n",
    "            total_loss=0\n",
    "            loss=0\n",
    "            start_time=t[-1]\n",
    "            log.close()\n",
    "            edge_list.clear()\n",
    "\n",
    "    return time_with_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_4_7=torch.load(\"./train_graphs/graph_4_7.TemporalData.simple\").to(device=device)\n",
    "graph_4_10=torch.load(\"./train_graphs/graph_4_10.TemporalData.simple\").to(device=device)\n",
    "graph_4_11=torch.load(\"./train_graphs/graph_4_11.TemporalData.simple\").to(device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load(\"./models/model_saved_share.pt\", map_location=device)\n",
    "memory,gnn, link_pred, neighbor_loader = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after merge: TemporalData(dst=[1357851], msg=[1357851, 40], src=[1357851], t=[1357851])\n",
      "Time: 2018-04-04 00:00:00.030000000~2018-04-04 00:18:00.409000000, Loss: 2.1723, Nodes_count: 45, Cost Time: 0.11s\n",
      "Time: 2018-04-04 00:18:00.409000000~2018-04-04 00:44:00.541000000, Loss: 0.2415, Nodes_count: 81, Cost Time: 0.62s\n",
      "Time: 2018-04-04 00:44:00.541000000~2018-04-04 01:01:40.901000000, Loss: 0.4488, Nodes_count: 103, Cost Time: 0.89s\n",
      "Time: 2018-04-04 01:01:40.901000000~2018-04-04 01:28:00.498000000, Loss: 0.3832, Nodes_count: 129, Cost Time: 0.98s\n",
      "Time: 2018-04-04 01:28:00.498000000~2018-04-04 01:46:29.679000000, Loss: 0.4371, Nodes_count: 145, Cost Time: 1.06s\n",
      "Time: 2018-04-04 01:46:29.679000000~2018-04-04 02:13:00.519000000, Loss: 0.3564, Nodes_count: 160, Cost Time: 1.15s\n",
      "Time: 2018-04-04 02:13:00.519000000~2018-04-04 02:30:02.355000000, Loss: 0.5438, Nodes_count: 195, Cost Time: 1.24s\n",
      "Time: 2018-04-04 02:30:02.355000000~2018-04-04 02:57:00.339000000, Loss: 0.3305, Nodes_count: 215, Cost Time: 1.35s\n",
      "Time: 2018-04-04 02:57:00.339000000~2018-04-04 03:19:23.600000000, Loss: 0.3809, Nodes_count: 233, Cost Time: 1.44s\n",
      "Time: 2018-04-04 03:19:23.600000000~2018-04-04 03:41:40.895000000, Loss: 0.2711, Nodes_count: 250, Cost Time: 1.54s\n",
      "Time: 2018-04-04 03:41:40.895000000~2018-04-04 04:02:12.886000000, Loss: 0.4690, Nodes_count: 281, Cost Time: 1.63s\n",
      "Time: 2018-04-04 04:02:12.886000000~2018-04-04 04:28:26.253000000, Loss: 0.3866, Nodes_count: 304, Cost Time: 1.72s\n",
      "Time: 2018-04-04 04:28:26.253000000~2018-04-04 04:47:03.432000000, Loss: 0.3898, Nodes_count: 325, Cost Time: 1.80s\n",
      "Time: 2018-04-04 04:47:03.432000000~2018-04-04 05:13:47.869000000, Loss: 0.3614, Nodes_count: 346, Cost Time: 1.89s\n",
      "Time: 2018-04-04 05:13:47.869000000~2018-04-04 05:35:28.922000000, Loss: 0.4059, Nodes_count: 365, Cost Time: 1.99s\n",
      "Time: 2018-04-04 05:35:28.922000000~2018-04-04 05:57:55.470000000, Loss: 0.2816, Nodes_count: 390, Cost Time: 2.08s\n",
      "Time: 2018-04-04 05:57:55.470000000~2018-04-04 06:20:00.400000000, Loss: 0.3815, Nodes_count: 408, Cost Time: 2.17s\n",
      "Time: 2018-04-04 06:20:00.400000000~2018-04-04 06:42:05.998000000, Loss: 0.4116, Nodes_count: 425, Cost Time: 2.26s\n",
      "Time: 2018-04-04 06:42:05.998000000~2018-04-04 07:00:01.112000000, Loss: 0.4906, Nodes_count: 456, Cost Time: 2.36s\n",
      "Time: 2018-04-04 07:00:01.112000000~2018-04-04 07:25:31.328000000, Loss: 0.3428, Nodes_count: 476, Cost Time: 2.46s\n",
      "Time: 2018-04-04 07:25:31.328000000~2018-04-04 07:45:00.920000000, Loss: 0.4246, Nodes_count: 499, Cost Time: 2.55s\n",
      "Time: 2018-04-04 07:45:00.920000000~2018-04-04 08:05:00.337000000, Loss: 0.4054, Nodes_count: 521, Cost Time: 2.65s\n",
      "Time: 2018-04-04 08:05:00.337000000~2018-04-04 08:30:00.340000000, Loss: 0.3982, Nodes_count: 549, Cost Time: 2.74s\n",
      "Time: 2018-04-04 08:30:00.340000000~2018-04-04 08:53:44.098000000, Loss: 0.3493, Nodes_count: 566, Cost Time: 2.83s\n",
      "Time: 2018-04-04 08:53:44.098000000~2018-04-04 09:08:54.146000000, Loss: 0.6867, Nodes_count: 682, Cost Time: 5.24s\n",
      "Time: 2018-04-04 09:08:54.146000000~2018-04-04 09:24:14.060000000, Loss: 0.7053, Nodes_count: 742, Cost Time: 5.86s\n",
      "Time: 2018-04-04 09:24:14.060000000~2018-04-04 09:46:59.997000000, Loss: 0.5188, Nodes_count: 784, Cost Time: 7.44s\n",
      "Time: 2018-04-04 09:46:59.997000000~2018-04-04 10:07:04.753000000, Loss: 0.7141, Nodes_count: 1076, Cost Time: 8.72s\n",
      "Time: 2018-04-04 10:07:04.753000000~2018-04-04 10:24:56.390000000, Loss: 0.5980, Nodes_count: 1202, Cost Time: 11.29s\n",
      "Time: 2018-04-04 10:24:56.390000000~2018-04-04 10:43:20.118000000, Loss: 0.5921, Nodes_count: 1272, Cost Time: 14.35s\n",
      "Time: 2018-04-04 10:43:20.118000000~2018-04-04 11:00:02.549000000, Loss: 0.6522, Nodes_count: 1476, Cost Time: 21.11s\n",
      "Time: 2018-04-04 11:00:02.549000000~2018-04-04 11:15:17.515000000, Loss: 0.3146, Nodes_count: 1551, Cost Time: 24.22s\n",
      "Time: 2018-04-04 11:15:17.515000000~2018-04-04 11:31:03.856000000, Loss: 0.6101, Nodes_count: 1667, Cost Time: 25.60s\n",
      "Time: 2018-04-04 11:31:03.856000000~2018-04-04 11:46:32.145000000, Loss: 0.8436, Nodes_count: 2097, Cost Time: 27.90s\n",
      "Time: 2018-04-04 11:46:32.145000000~2018-04-04 12:01:38.539000000, Loss: 0.6322, Nodes_count: 2208, Cost Time: 32.75s\n",
      "Time: 2018-04-04 12:01:38.539000000~2018-04-04 12:16:41.081000000, Loss: 0.9651, Nodes_count: 3487, Cost Time: 37.16s\n",
      "Time: 2018-04-04 12:16:41.081000000~2018-04-04 12:34:19.710000000, Loss: 0.7826, Nodes_count: 4046, Cost Time: 40.00s\n",
      "Time: 2018-04-04 12:34:19.710000000~2018-04-04 12:49:22.540000000, Loss: 1.0924, Nodes_count: 4993, Cost Time: 43.17s\n",
      "Time: 2018-04-04 12:49:22.540000000~2018-04-04 13:08:07.223000000, Loss: 0.8550, Nodes_count: 5326, Cost Time: 45.43s\n",
      "Time: 2018-04-04 13:08:07.223000000~2018-04-04 13:30:01.377000000, Loss: 0.5249, Nodes_count: 5386, Cost Time: 46.13s\n",
      "Time: 2018-04-04 13:30:01.377000000~2018-04-04 13:45:50.641000000, Loss: 0.7786, Nodes_count: 5543, Cost Time: 47.94s\n",
      "Time: 2018-04-04 13:45:50.641000000~2018-04-04 14:01:08.905000000, Loss: 0.8350, Nodes_count: 5895, Cost Time: 50.57s\n",
      "Time: 2018-04-04 14:01:08.905000000~2018-04-04 14:17:40.088000000, Loss: 0.5714, Nodes_count: 5944, Cost Time: 54.36s\n",
      "Time: 2018-04-04 14:17:40.088000000~2018-04-04 14:37:36.937000000, Loss: 0.3321, Nodes_count: 5980, Cost Time: 56.01s\n",
      "Time: 2018-04-04 14:37:36.937000000~2018-04-04 14:56:53.830000000, Loss: 0.7448, Nodes_count: 6235, Cost Time: 60.30s\n",
      "Time: 2018-04-04 14:56:53.830000000~2018-04-04 15:11:59.827000000, Loss: 0.6125, Nodes_count: 6276, Cost Time: 62.63s\n",
      "Time: 2018-04-04 15:11:59.827000000~2018-04-04 15:30:59.833000000, Loss: 0.6088, Nodes_count: 6503, Cost Time: 66.35s\n",
      "Time: 2018-04-04 15:30:59.833000000~2018-04-04 15:46:27.669000000, Loss: 0.5823, Nodes_count: 6619, Cost Time: 68.02s\n",
      "Time: 2018-04-04 15:46:27.669000000~2018-04-04 16:01:28.278000000, Loss: 0.6528, Nodes_count: 6824, Cost Time: 91.82s\n",
      "Time: 2018-04-04 16:01:28.278000000~2018-04-04 16:19:10.228000000, Loss: 0.7100, Nodes_count: 6953, Cost Time: 102.53s\n",
      "Time: 2018-04-04 16:19:10.228000000~2018-04-04 16:34:10.523000000, Loss: 0.5075, Nodes_count: 6992, Cost Time: 103.88s\n",
      "Time: 2018-04-04 16:34:10.523000000~2018-04-04 16:49:13.914000000, Loss: 0.7381, Nodes_count: 7254, Cost Time: 109.85s\n",
      "Time: 2018-04-04 16:49:13.914000000~2018-04-04 17:04:39.724000000, Loss: 0.5186, Nodes_count: 7346, Cost Time: 111.62s\n",
      "Time: 2018-04-04 17:04:39.724000000~2018-04-04 17:28:27.768000000, Loss: 0.4166, Nodes_count: 7375, Cost Time: 111.92s\n",
      "Time: 2018-04-04 17:28:27.768000000~2018-04-04 17:50:34.864000000, Loss: 0.6461, Nodes_count: 7388, Cost Time: 112.01s\n",
      "Time: 2018-04-04 17:50:34.864000000~2018-04-04 18:12:26.456000000, Loss: 0.2419, Nodes_count: 7404, Cost Time: 112.11s\n",
      "Time: 2018-04-04 18:12:26.456000000~2018-04-04 18:30:37.518000000, Loss: 0.6766, Nodes_count: 7430, Cost Time: 112.21s\n",
      "Time: 2018-04-04 18:30:37.518000000~2018-04-04 18:55:41.922000000, Loss: 0.1852, Nodes_count: 7446, Cost Time: 112.30s\n",
      "Time: 2018-04-04 18:55:41.922000000~2018-04-04 19:15:01.002000000, Loss: 0.3011, Nodes_count: 7468, Cost Time: 112.40s\n",
      "Time: 2018-04-04 19:15:01.002000000~2018-04-04 19:34:59.220000000, Loss: 0.6691, Nodes_count: 7484, Cost Time: 112.50s\n",
      "Time: 2018-04-04 19:34:59.220000000~2018-04-04 20:00:00.180000000, Loss: 0.6937, Nodes_count: 7510, Cost Time: 112.59s\n",
      "Time: 2018-04-04 20:00:00.180000000~2018-04-04 20:21:35.285000000, Loss: 0.1903, Nodes_count: 7521, Cost Time: 112.68s\n",
      "Time: 2018-04-04 20:21:35.285000000~2018-04-04 20:43:53.871000000, Loss: 0.3159, Nodes_count: 7545, Cost Time: 112.78s\n",
      "Time: 2018-04-04 20:43:53.871000000~2018-04-04 21:05:30.588000000, Loss: 0.2883, Nodes_count: 7568, Cost Time: 112.88s\n",
      "Time: 2018-04-04 21:05:30.588000000~2018-04-04 21:27:32.722000000, Loss: 0.3068, Nodes_count: 7594, Cost Time: 112.97s\n",
      "Time: 2018-04-04 21:27:32.722000000~2018-04-04 21:48:59.197000000, Loss: 0.6762, Nodes_count: 7610, Cost Time: 113.07s\n",
      "Time: 2018-04-04 21:48:59.197000000~2018-04-04 22:09:45.403000000, Loss: 0.7247, Nodes_count: 7628, Cost Time: 113.17s\n",
      "Time: 2018-04-04 22:09:45.403000000~2018-04-04 22:30:16.698000000, Loss: 0.3345, Nodes_count: 7655, Cost Time: 113.26s\n",
      "Time: 2018-04-04 22:30:16.698000000~2018-04-04 22:52:49.267000000, Loss: 0.5029, Nodes_count: 7669, Cost Time: 113.36s\n",
      "Time: 2018-04-04 22:52:49.267000000~2018-04-04 23:13:54.971000000, Loss: 0.6879, Nodes_count: 7695, Cost Time: 113.45s\n",
      "Time: 2018-04-04 23:13:54.971000000~2018-04-04 23:35:49.374000000, Loss: 0.2779, Nodes_count: 7712, Cost Time: 113.55s\n",
      "Time: 2018-04-04 23:35:49.374000000~2018-04-04 23:59:02.265000000, Loss: 0.2547, Nodes_count: 7740, Cost Time: 113.64s\n"
     ]
    }
   ],
   "source": [
    "ans_4_4=test_day_new(graph_4_4,\"graph_4_4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after merge: TemporalData(dst=[840914], msg=[840914, 40], src=[840914], t=[840914])\n",
      "Time: 2018-04-05 00:00:00.041000000~2018-04-05 00:19:19.331000000, Loss: 2.0655, Nodes_count: 42, Cost Time: 0.07s\n",
      "Time: 2018-04-05 00:19:19.331000000~2018-04-05 00:44:22.644000000, Loss: 0.2333, Nodes_count: 75, Cost Time: 0.15s\n",
      "Time: 2018-04-05 00:44:22.644000000~2018-04-05 01:05:24.697000000, Loss: 0.6920, Nodes_count: 93, Cost Time: 0.24s\n",
      "Time: 2018-04-05 01:05:24.697000000~2018-04-05 01:27:27.947000000, Loss: 0.6294, Nodes_count: 118, Cost Time: 0.33s\n",
      "Time: 2018-04-05 01:27:27.947000000~2018-04-05 01:50:05.181000000, Loss: 0.5873, Nodes_count: 138, Cost Time: 0.41s\n",
      "Time: 2018-04-05 01:50:05.181000000~2018-04-05 02:15:00.642000000, Loss: 0.6035, Nodes_count: 168, Cost Time: 0.50s\n",
      "Time: 2018-04-05 02:15:00.642000000~2018-04-05 02:37:19.553000000, Loss: 0.3142, Nodes_count: 186, Cost Time: 0.59s\n",
      "Time: 2018-04-05 02:37:19.553000000~2018-04-05 02:58:26.087000000, Loss: 0.3182, Nodes_count: 216, Cost Time: 0.69s\n",
      "Time: 2018-04-05 02:58:26.087000000~2018-04-05 03:22:10.102000000, Loss: 0.2267, Nodes_count: 231, Cost Time: 0.78s\n",
      "Time: 2018-04-05 03:22:10.102000000~2018-04-05 03:45:00.640000000, Loss: 0.7119, Nodes_count: 259, Cost Time: 0.87s\n",
      "Time: 2018-04-05 03:45:00.640000000~2018-04-05 04:05:13.730000000, Loss: 0.3388, Nodes_count: 283, Cost Time: 0.96s\n",
      "Time: 2018-04-05 04:05:13.730000000~2018-04-05 04:30:00.519000000, Loss: 0.6235, Nodes_count: 309, Cost Time: 1.05s\n",
      "Time: 2018-04-05 04:30:00.519000000~2018-04-05 04:50:19.505000000, Loss: 0.2890, Nodes_count: 323, Cost Time: 1.14s\n",
      "Time: 2018-04-05 04:50:19.505000000~2018-04-05 05:12:47.557000000, Loss: 0.6308, Nodes_count: 341, Cost Time: 1.24s\n",
      "Time: 2018-04-05 05:12:47.557000000~2018-04-05 05:34:28.245000000, Loss: 0.6517, Nodes_count: 366, Cost Time: 1.33s\n",
      "Time: 2018-04-05 05:34:28.245000000~2018-04-05 05:56:18.371000000, Loss: 0.2688, Nodes_count: 390, Cost Time: 1.42s\n",
      "Time: 2018-04-05 05:56:18.371000000~2018-04-05 06:18:38.888000000, Loss: 0.6196, Nodes_count: 409, Cost Time: 1.51s\n",
      "Time: 2018-04-05 06:18:38.888000000~2018-04-05 06:39:58.601000000, Loss: 0.6909, Nodes_count: 428, Cost Time: 1.60s\n",
      "Time: 2018-04-05 06:39:58.601000000~2018-04-05 07:01:00.582000000, Loss: 0.7123, Nodes_count: 453, Cost Time: 1.70s\n",
      "Time: 2018-04-05 07:01:00.582000000~2018-04-05 07:23:23.952000000, Loss: 0.2461, Nodes_count: 466, Cost Time: 1.79s\n",
      "Time: 2018-04-05 07:23:23.952000000~2018-04-05 07:44:31.912000000, Loss: 0.3038, Nodes_count: 491, Cost Time: 1.88s\n",
      "Time: 2018-04-05 07:44:31.912000000~2018-04-05 08:01:00.827000000, Loss: 0.7641, Nodes_count: 516, Cost Time: 1.98s\n",
      "Time: 2018-04-05 08:01:00.827000000~2018-04-05 08:27:13.911000000, Loss: 0.2037, Nodes_count: 540, Cost Time: 2.07s\n",
      "Time: 2018-04-05 08:27:13.911000000~2018-04-05 08:49:43.480000000, Loss: 0.2306, Nodes_count: 558, Cost Time: 2.16s\n",
      "Time: 2018-04-05 08:49:43.480000000~2018-04-05 09:07:37.850000000, Loss: 0.7649, Nodes_count: 640, Cost Time: 2.80s\n",
      "Time: 2018-04-05 09:07:37.850000000~2018-04-05 09:23:08.586000000, Loss: 0.5841, Nodes_count: 725, Cost Time: 3.38s\n",
      "Time: 2018-04-05 09:23:08.586000000~2018-04-05 09:38:53.652000000, Loss: 0.8264, Nodes_count: 1444, Cost Time: 6.86s\n",
      "Time: 2018-04-05 09:38:53.652000000~2018-04-05 09:53:59.568000000, Loss: 0.6742, Nodes_count: 1604, Cost Time: 9.98s\n",
      "Time: 2018-04-05 09:53:59.568000000~2018-04-05 10:09:49.956000000, Loss: 0.6677, Nodes_count: 1723, Cost Time: 11.40s\n",
      "Time: 2018-04-05 10:09:49.956000000~2018-04-05 10:25:38.986000000, Loss: 0.4422, Nodes_count: 1756, Cost Time: 11.85s\n",
      "Time: 2018-04-05 10:25:38.986000000~2018-04-05 10:41:04.361000000, Loss: 0.6928, Nodes_count: 1917, Cost Time: 14.04s\n",
      "Time: 2018-04-05 10:41:04.361000000~2018-04-05 10:57:00.432000000, Loss: 0.6358, Nodes_count: 2079, Cost Time: 15.64s\n",
      "Time: 2018-04-05 10:57:00.432000000~2018-04-05 11:13:38.196000000, Loss: 0.2733, Nodes_count: 2111, Cost Time: 16.58s\n",
      "Time: 2018-04-05 11:13:38.196000000~2018-04-05 11:28:40.147000000, Loss: 0.7148, Nodes_count: 2313, Cost Time: 18.38s\n",
      "Time: 2018-04-05 11:28:40.147000000~2018-04-05 11:43:41.792000000, Loss: 0.6568, Nodes_count: 2392, Cost Time: 21.80s\n",
      "Time: 2018-04-05 11:43:41.792000000~2018-04-05 11:59:09.930000000, Loss: 0.2666, Nodes_count: 2449, Cost Time: 23.17s\n",
      "Time: 2018-04-05 11:59:09.930000000~2018-04-05 12:14:12.827000000, Loss: 0.9330, Nodes_count: 3626, Cost Time: 29.24s\n",
      "Time: 2018-04-05 12:14:12.827000000~2018-04-05 12:32:50.607000000, Loss: 0.7038, Nodes_count: 3659, Cost Time: 32.85s\n",
      "Time: 2018-04-05 12:32:50.607000000~2018-04-05 12:56:48.191000000, Loss: 0.4965, Nodes_count: 3769, Cost Time: 34.29s\n",
      "Time: 2018-04-05 12:56:48.191000000~2018-04-05 13:14:01.066000000, Loss: 0.3000, Nodes_count: 3793, Cost Time: 34.69s\n",
      "Time: 2018-04-05 13:14:01.066000000~2018-04-05 13:29:05.129000000, Loss: 0.8257, Nodes_count: 3935, Cost Time: 37.24s\n",
      "Time: 2018-04-05 13:29:05.129000000~2018-04-05 13:45:20.418000000, Loss: 0.4771, Nodes_count: 3998, Cost Time: 37.91s\n",
      "Time: 2018-04-05 13:45:20.418000000~2018-04-05 14:00:32.997000000, Loss: 0.5842, Nodes_count: 4137, Cost Time: 39.68s\n",
      "Time: 2018-04-05 14:00:32.997000000~2018-04-05 14:20:15.390000000, Loss: 0.4028, Nodes_count: 4208, Cost Time: 40.67s\n",
      "Time: 2018-04-05 14:20:15.390000000~2018-04-05 14:36:58.088000000, Loss: 0.6483, Nodes_count: 4310, Cost Time: 46.86s\n",
      "Time: 2018-04-05 14:36:58.088000000~2018-04-05 14:51:59.209000000, Loss: 0.3946, Nodes_count: 4369, Cost Time: 49.72s\n",
      "Time: 2018-04-05 14:51:59.209000000~2018-04-05 15:06:59.385000000, Loss: 0.6847, Nodes_count: 4722, Cost Time: 52.11s\n",
      "Time: 2018-04-05 15:06:59.385000000~2018-04-05 15:22:31.261000000, Loss: 0.5885, Nodes_count: 4906, Cost Time: 56.43s\n",
      "Time: 2018-04-05 15:22:31.261000000~2018-04-05 15:37:54.348000000, Loss: 0.8177, Nodes_count: 5616, Cost Time: 60.95s\n",
      "Time: 2018-04-05 15:37:54.348000000~2018-04-05 15:53:34.793000000, Loss: 0.4999, Nodes_count: 5759, Cost Time: 62.94s\n",
      "Time: 2018-04-05 15:53:34.793000000~2018-04-05 16:09:23.048000000, Loss: 0.3490, Nodes_count: 5780, Cost Time: 63.85s\n",
      "Time: 2018-04-05 16:09:23.048000000~2018-04-05 16:25:55.195000000, Loss: 0.4449, Nodes_count: 5928, Cost Time: 65.54s\n",
      "Time: 2018-04-05 16:25:55.195000000~2018-04-05 16:41:03.167000000, Loss: 0.3500, Nodes_count: 5989, Cost Time: 68.03s\n",
      "Time: 2018-04-05 16:41:03.167000000~2018-04-05 16:59:07.920000000, Loss: 0.6660, Nodes_count: 6055, Cost Time: 69.07s\n",
      "Time: 2018-04-05 16:59:07.920000000~2018-04-05 17:15:19.588000000, Loss: 0.2343, Nodes_count: 6067, Cost Time: 69.30s\n",
      "Time: 2018-04-05 17:15:19.588000000~2018-04-05 17:35:14.078000000, Loss: 0.5915, Nodes_count: 6081, Cost Time: 69.49s\n",
      "Time: 2018-04-05 17:35:14.078000000~2018-04-05 17:55:16.875000000, Loss: 0.2026, Nodes_count: 6100, Cost Time: 69.68s\n",
      "Time: 2018-04-05 17:55:16.875000000~2018-04-05 18:15:42.341000000, Loss: 0.1978, Nodes_count: 6121, Cost Time: 69.86s\n",
      "Time: 2018-04-05 18:15:42.341000000~2018-04-05 18:37:14.681000000, Loss: 0.2877, Nodes_count: 6142, Cost Time: 70.13s\n",
      "Time: 2018-04-05 18:37:14.681000000~2018-04-05 18:58:00.402000000, Loss: 0.1856, Nodes_count: 6162, Cost Time: 70.32s\n",
      "Time: 2018-04-05 18:58:00.402000000~2018-04-05 19:17:05.754000000, Loss: 0.2295, Nodes_count: 6180, Cost Time: 70.52s\n",
      "Time: 2018-04-05 19:17:05.754000000~2018-04-05 19:37:53.928000000, Loss: 0.1849, Nodes_count: 6198, Cost Time: 70.70s\n",
      "Time: 2018-04-05 19:37:53.928000000~2018-04-05 19:59:18.826000000, Loss: 0.1692, Nodes_count: 6220, Cost Time: 70.88s\n",
      "Time: 2018-04-05 19:59:18.826000000~2018-04-05 20:19:00.511000000, Loss: 0.1477, Nodes_count: 6229, Cost Time: 71.07s\n",
      "Time: 2018-04-05 20:19:00.511000000~2018-04-05 20:39:00.399000000, Loss: 0.1899, Nodes_count: 6250, Cost Time: 71.25s\n",
      "Time: 2018-04-05 20:39:00.399000000~2018-04-05 21:00:00.321000000, Loss: 0.2199, Nodes_count: 6277, Cost Time: 71.43s\n",
      "Time: 2018-04-05 21:00:00.321000000~2018-04-05 21:19:46.983000000, Loss: 0.3392, Nodes_count: 6293, Cost Time: 71.70s\n",
      "Time: 2018-04-05 21:19:46.983000000~2018-04-05 21:38:46.760000000, Loss: 0.3495, Nodes_count: 6309, Cost Time: 71.99s\n",
      "Time: 2018-04-05 21:38:46.760000000~2018-04-05 22:00:00.344000000, Loss: 0.2003, Nodes_count: 6331, Cost Time: 72.20s\n",
      "Time: 2018-04-05 22:00:00.344000000~2018-04-05 22:19:18.475000000, Loss: 0.3532, Nodes_count: 6343, Cost Time: 72.47s\n",
      "Time: 2018-04-05 22:19:18.475000000~2018-04-05 22:39:21.047000000, Loss: 0.1915, Nodes_count: 6357, Cost Time: 72.67s\n",
      "Time: 2018-04-05 22:39:21.047000000~2018-04-05 22:57:00.416000000, Loss: 0.4075, Nodes_count: 6387, Cost Time: 72.94s\n",
      "Time: 2018-04-05 22:57:00.416000000~2018-04-05 23:15:02.754000000, Loss: 0.2270, Nodes_count: 6403, Cost Time: 73.15s\n",
      "Time: 2018-04-05 23:15:02.754000000~2018-04-05 23:35:48.736000000, Loss: 0.1794, Nodes_count: 6418, Cost Time: 73.33s\n",
      "Time: 2018-04-05 23:35:48.736000000~2018-04-05 23:58:11.523000000, Loss: 0.1510, Nodes_count: 6439, Cost Time: 73.52s\n"
     ]
    }
   ],
   "source": [
    "ans_4_5=test_day_new(graph_4_5,\"graph_4_5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after merge: TemporalData(dst=[1134670], msg=[1134670, 40], src=[1134670], t=[1134670])\n",
      "Time: 2018-04-06 00:00:00.050000000~2018-04-06 00:19:16.992000000, Loss: 1.0684, Nodes_count: 43, Cost Time: 0.14s\n",
      "Time: 2018-04-06 00:19:16.992000000~2018-04-06 00:45:05.853000000, Loss: 0.3859, Nodes_count: 87, Cost Time: 0.48s\n",
      "Time: 2018-04-06 00:45:05.853000000~2018-04-06 01:00:11.671000000, Loss: 0.3000, Nodes_count: 114, Cost Time: 0.67s\n",
      "Time: 2018-04-06 01:00:11.671000000~2018-04-06 01:18:57.427000000, Loss: 0.4082, Nodes_count: 126, Cost Time: 0.92s\n",
      "Time: 2018-04-06 01:18:57.427000000~2018-04-06 01:42:40.184000000, Loss: 0.1656, Nodes_count: 141, Cost Time: 1.10s\n",
      "Time: 2018-04-06 01:42:40.184000000~2018-04-06 02:05:25.221000000, Loss: 0.4362, Nodes_count: 176, Cost Time: 1.45s\n",
      "Time: 2018-04-06 02:05:25.221000000~2018-04-06 02:22:54.336000000, Loss: 0.3202, Nodes_count: 195, Cost Time: 1.73s\n",
      "Time: 2018-04-06 02:22:54.336000000~2018-04-06 02:44:45.281000000, Loss: 0.1943, Nodes_count: 225, Cost Time: 1.91s\n",
      "Time: 2018-04-06 02:44:45.281000000~2018-04-06 03:03:00.573000000, Loss: 0.2248, Nodes_count: 248, Cost Time: 2.10s\n",
      "Time: 2018-04-06 03:03:00.573000000~2018-04-06 03:25:24.762000000, Loss: 0.1520, Nodes_count: 266, Cost Time: 2.30s\n",
      "Time: 2018-04-06 03:25:24.762000000~2018-04-06 03:45:28.902000000, Loss: 0.1881, Nodes_count: 287, Cost Time: 2.48s\n",
      "Time: 2018-04-06 03:45:28.902000000~2018-04-06 04:05:31.388000000, Loss: 0.1977, Nodes_count: 310, Cost Time: 2.66s\n",
      "Time: 2018-04-06 04:05:31.388000000~2018-04-06 04:27:35.079000000, Loss: 0.1615, Nodes_count: 331, Cost Time: 2.84s\n",
      "Time: 2018-04-06 04:27:35.079000000~2018-04-06 04:45:32.422000000, Loss: 0.2887, Nodes_count: 353, Cost Time: 3.11s\n",
      "Time: 2018-04-06 04:45:32.422000000~2018-04-06 05:07:27.473000000, Loss: 0.1696, Nodes_count: 367, Cost Time: 3.30s\n",
      "Time: 2018-04-06 05:07:27.473000000~2018-04-06 05:27:24.984000000, Loss: 0.1845, Nodes_count: 391, Cost Time: 3.48s\n",
      "Time: 2018-04-06 05:27:24.984000000~2018-04-06 05:47:11.800000000, Loss: 0.1754, Nodes_count: 411, Cost Time: 3.66s\n",
      "Time: 2018-04-06 05:47:11.800000000~2018-04-06 06:05:34.537000000, Loss: 0.4371, Nodes_count: 430, Cost Time: 3.93s\n",
      "Time: 2018-04-06 06:05:34.537000000~2018-04-06 06:28:00.121000000, Loss: 0.1657, Nodes_count: 450, Cost Time: 4.11s\n",
      "Time: 2018-04-06 06:28:00.121000000~2018-04-06 06:45:02.339000000, Loss: 0.2844, Nodes_count: 469, Cost Time: 4.39s\n",
      "Time: 2018-04-06 06:45:02.339000000~2018-04-06 07:05:33.094000000, Loss: 0.2175, Nodes_count: 487, Cost Time: 4.58s\n",
      "Time: 2018-04-06 07:05:33.094000000~2018-04-06 07:27:38.006000000, Loss: 0.1547, Nodes_count: 511, Cost Time: 4.75s\n",
      "Time: 2018-04-06 07:27:38.006000000~2018-04-06 07:47:00.431000000, Loss: 0.1716, Nodes_count: 526, Cost Time: 4.93s\n",
      "Time: 2018-04-06 07:47:00.431000000~2018-04-06 08:07:33.934000000, Loss: 0.1817, Nodes_count: 546, Cost Time: 5.11s\n",
      "Time: 2018-04-06 08:07:33.934000000~2018-04-06 08:29:46.554000000, Loss: 0.1854, Nodes_count: 573, Cost Time: 5.28s\n",
      "Time: 2018-04-06 08:29:46.554000000~2018-04-06 08:45:16.620000000, Loss: 0.2869, Nodes_count: 594, Cost Time: 5.55s\n",
      "Time: 2018-04-06 08:45:16.620000000~2018-04-06 09:04:00.469000000, Loss: 0.2308, Nodes_count: 611, Cost Time: 5.75s\n",
      "Time: 2018-04-06 09:04:00.469000000~2018-04-06 09:29:00.411000000, Loss: 0.1787, Nodes_count: 640, Cost Time: 5.92s\n",
      "Time: 2018-04-06 09:29:00.411000000~2018-04-06 09:49:36.361000000, Loss: 0.2711, Nodes_count: 656, Cost Time: 6.19s\n",
      "Time: 2018-04-06 09:49:36.361000000~2018-04-06 10:11:25.827000000, Loss: 0.2144, Nodes_count: 679, Cost Time: 6.38s\n",
      "Time: 2018-04-06 10:11:25.827000000~2018-04-06 10:33:37.548000000, Loss: 0.2154, Nodes_count: 707, Cost Time: 6.57s\n",
      "Time: 2018-04-06 10:33:37.548000000~2018-04-06 10:51:59.967000000, Loss: 0.2593, Nodes_count: 725, Cost Time: 6.84s\n",
      "Time: 2018-04-06 10:51:59.967000000~2018-04-06 11:14:09.465000000, Loss: 0.1797, Nodes_count: 742, Cost Time: 7.02s\n",
      "Time: 2018-04-06 11:14:09.465000000~2018-04-06 11:30:58.081000000, Loss: 0.5015, Nodes_count: 772, Cost Time: 7.30s\n",
      "Time: 2018-04-06 11:30:58.081000000~2018-04-06 11:51:20.194000000, Loss: 0.1649, Nodes_count: 787, Cost Time: 7.49s\n",
      "Time: 2018-04-06 11:51:20.194000000~2018-04-06 12:09:43.911000000, Loss: 0.1918, Nodes_count: 806, Cost Time: 7.67s\n",
      "Time: 2018-04-06 12:09:43.911000000~2018-04-06 12:25:54.685000000, Loss: 0.2317, Nodes_count: 863, Cost Time: 8.01s\n",
      "Time: 2018-04-06 12:25:54.685000000~2018-04-06 12:41:20.717000000, Loss: 0.2072, Nodes_count: 919, Cost Time: 8.43s\n",
      "Time: 2018-04-06 12:41:20.717000000~2018-04-06 12:56:55.757000000, Loss: 0.2370, Nodes_count: 991, Cost Time: 8.87s\n",
      "Time: 2018-04-06 12:56:55.757000000~2018-04-06 13:12:04.238000000, Loss: 0.2477, Nodes_count: 1045, Cost Time: 9.30s\n",
      "Time: 2018-04-06 13:12:04.238000000~2018-04-06 13:30:04.328000000, Loss: 0.2443, Nodes_count: 1083, Cost Time: 9.59s\n",
      "Time: 2018-04-06 13:30:04.328000000~2018-04-06 13:45:17.542000000, Loss: 0.3651, Nodes_count: 1184, Cost Time: 10.18s\n",
      "Time: 2018-04-06 13:45:17.542000000~2018-04-06 14:00:35.220000000, Loss: 1.0121, Nodes_count: 1964, Cost Time: 11.97s\n",
      "Time: 2018-04-06 14:00:35.220000000~2018-04-06 14:19:19.282000000, Loss: 0.8258, Nodes_count: 2212, Cost Time: 12.96s\n",
      "Time: 2018-04-06 14:19:19.282000000~2018-04-06 14:36:21.009000000, Loss: 0.6939, Nodes_count: 2284, Cost Time: 13.68s\n",
      "Time: 2018-04-06 14:36:21.009000000~2018-04-06 14:52:35.840000000, Loss: 0.9466, Nodes_count: 2632, Cost Time: 15.09s\n",
      "Time: 2018-04-06 14:52:35.840000000~2018-04-06 15:08:30.414000000, Loss: 0.9572, Nodes_count: 2883, Cost Time: 16.31s\n",
      "Time: 2018-04-06 15:08:30.414000000~2018-04-06 15:29:50.869000000, Loss: 0.4180, Nodes_count: 2923, Cost Time: 16.75s\n",
      "Time: 2018-04-06 15:29:50.869000000~2018-04-06 15:49:26.015000000, Loss: 0.8312, Nodes_count: 3042, Cost Time: 17.72s\n",
      "Time: 2018-04-06 15:49:26.015000000~2018-04-06 16:05:10.837000000, Loss: 0.6019, Nodes_count: 3231, Cost Time: 24.29s\n",
      "Time: 2018-04-06 16:05:10.837000000~2018-04-06 16:20:11.626000000, Loss: 0.5986, Nodes_count: 3254, Cost Time: 25.24s\n",
      "Time: 2018-04-06 16:20:11.626000000~2018-04-06 16:35:25.587000000, Loss: 0.9232, Nodes_count: 4573, Cost Time: 32.12s\n",
      "Time: 2018-04-06 16:35:25.587000000~2018-04-06 16:50:32.945000000, Loss: 0.7826, Nodes_count: 4943, Cost Time: 36.57s\n",
      "Time: 2018-04-06 16:50:32.945000000~2018-04-06 17:05:39.288000000, Loss: 0.9220, Nodes_count: 5490, Cost Time: 38.78s\n",
      "Time: 2018-04-06 17:05:39.288000000~2018-04-06 17:20:54.977000000, Loss: 0.8326, Nodes_count: 6203, Cost Time: 45.16s\n",
      "Time: 2018-04-06 17:20:54.977000000~2018-04-06 17:36:52.502000000, Loss: 0.6557, Nodes_count: 7037, Cost Time: 51.59s\n",
      "Time: 2018-04-06 17:36:52.502000000~2018-04-06 17:52:05.798000000, Loss: 0.8477, Nodes_count: 7783, Cost Time: 55.01s\n",
      "Time: 2018-04-06 17:52:05.798000000~2018-04-06 18:12:57.421000000, Loss: 0.5929, Nodes_count: 7880, Cost Time: 59.66s\n",
      "Time: 2018-04-06 18:12:57.421000000~2018-04-06 18:30:22.483000000, Loss: 0.9908, Nodes_count: 8569, Cost Time: 62.26s\n",
      "Time: 2018-04-06 18:30:22.483000000~2018-04-06 18:45:23.209000000, Loss: 0.9828, Nodes_count: 10489, Cost Time: 69.07s\n",
      "Time: 2018-04-06 18:45:23.209000000~2018-04-06 19:04:54.592000000, Loss: 0.8394, Nodes_count: 11175, Cost Time: 79.39s\n",
      "Time: 2018-04-06 19:04:54.592000000~2018-04-06 19:23:03.611000000, Loss: 0.9736, Nodes_count: 12119, Cost Time: 87.55s\n",
      "Time: 2018-04-06 19:23:03.611000000~2018-04-06 19:44:32.667000000, Loss: 1.0134, Nodes_count: 12557, Cost Time: 89.82s\n",
      "Time: 2018-04-06 19:44:32.667000000~2018-04-06 20:00:39.987000000, Loss: 0.9432, Nodes_count: 12716, Cost Time: 90.87s\n",
      "Time: 2018-04-06 20:00:39.987000000~2018-04-06 20:19:42.681000000, Loss: 0.4279, Nodes_count: 12727, Cost Time: 91.27s\n",
      "Time: 2018-04-06 20:19:42.681000000~2018-04-06 20:42:46.845000000, Loss: 0.2451, Nodes_count: 12750, Cost Time: 92.13s\n",
      "Time: 2018-04-06 20:42:46.845000000~2018-04-06 20:59:25.771000000, Loss: 1.1028, Nodes_count: 13402, Cost Time: 93.41s\n",
      "Time: 2018-04-06 20:59:25.771000000~2018-04-06 21:28:30.420000000, Loss: 0.7170, Nodes_count: 13464, Cost Time: 93.80s\n",
      "Time: 2018-04-06 21:28:30.420000000~2018-04-06 21:53:32.222000000, Loss: 0.3443, Nodes_count: 13496, Cost Time: 94.02s\n",
      "Time: 2018-04-06 21:53:32.222000000~2018-04-06 22:10:33.999000000, Loss: 0.7599, Nodes_count: 14116, Cost Time: 99.00s\n",
      "Time: 2018-04-06 22:10:33.999000000~2018-04-06 22:39:24.718000000, Loss: 0.5810, Nodes_count: 14158, Cost Time: 99.56s\n",
      "Time: 2018-04-06 22:39:24.718000000~2018-04-06 23:10:53.922000000, Loss: 0.9511, Nodes_count: 14572, Cost Time: 102.55s\n",
      "Time: 2018-04-06 23:10:53.922000000~2018-04-06 23:31:29.855000000, Loss: 0.5801, Nodes_count: 14605, Cost Time: 102.82s\n",
      "Time: 2018-04-06 23:31:29.855000000~2018-04-06 23:58:03.573000000, Loss: 0.4638, Nodes_count: 14628, Cost Time: 102.91s\n"
     ]
    }
   ],
   "source": [
    "ans_4_6=test_day_new(graph_4_6,\"graph_4_6\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after merge: TemporalData(dst=[1847921], msg=[1847921, 40], src=[1847921], t=[1847921])\n",
      "Time: 2018-04-07 00:00:00.040000000~2018-04-07 00:30:11.169000000, Loss: 1.4250, Nodes_count: 75, Cost Time: 0.15s\n",
      "Time: 2018-04-07 00:30:11.169000000~2018-04-07 00:55:07.465000000, Loss: 0.6400, Nodes_count: 99, Cost Time: 0.32s\n",
      "Time: 2018-04-07 00:55:07.465000000~2018-04-07 01:18:15.361000000, Loss: 0.4791, Nodes_count: 119, Cost Time: 0.41s\n",
      "Time: 2018-04-07 01:18:15.361000000~2018-04-07 01:44:15.385000000, Loss: 0.5063, Nodes_count: 142, Cost Time: 0.49s\n",
      "Time: 2018-04-07 01:44:15.385000000~2018-04-07 02:15:00.653000000, Loss: 0.5642, Nodes_count: 176, Cost Time: 0.67s\n",
      "Time: 2018-04-07 02:15:00.653000000~2018-04-07 02:30:22.992000000, Loss: 0.8789, Nodes_count: 201, Cost Time: 0.85s\n",
      "Time: 2018-04-07 02:30:22.992000000~2018-04-07 02:45:49.212000000, Loss: 0.7982, Nodes_count: 210, Cost Time: 0.96s\n",
      "Time: 2018-04-07 02:45:49.212000000~2018-04-07 03:11:00.395000000, Loss: 0.4518, Nodes_count: 227, Cost Time: 1.05s\n",
      "Time: 2018-04-07 03:11:00.395000000~2018-04-07 03:32:00.300000000, Loss: 0.5794, Nodes_count: 256, Cost Time: 1.15s\n",
      "Time: 2018-04-07 03:32:00.300000000~2018-04-07 03:58:51.900000000, Loss: 0.4653, Nodes_count: 277, Cost Time: 1.24s\n",
      "Time: 2018-04-07 03:58:51.900000000~2018-04-07 04:23:34.116000000, Loss: 0.4771, Nodes_count: 300, Cost Time: 1.33s\n",
      "Time: 2018-04-07 04:23:34.116000000~2018-04-07 04:46:00.626000000, Loss: 0.4750, Nodes_count: 319, Cost Time: 1.42s\n",
      "Time: 2018-04-07 04:46:00.626000000~2018-04-07 05:10:42.317000000, Loss: 0.4687, Nodes_count: 333, Cost Time: 1.52s\n",
      "Time: 2018-04-07 05:10:42.317000000~2018-04-07 05:30:07.852000000, Loss: 0.6108, Nodes_count: 364, Cost Time: 1.61s\n",
      "Time: 2018-04-07 05:30:07.852000000~2018-04-07 05:57:05.858000000, Loss: 0.7844, Nodes_count: 378, Cost Time: 1.79s\n",
      "Time: 2018-04-07 05:57:05.858000000~2018-04-07 06:20:15.841000000, Loss: 0.6407, Nodes_count: 398, Cost Time: 1.89s\n",
      "Time: 2018-04-07 06:20:15.841000000~2018-04-07 06:44:17.080000000, Loss: 0.5453, Nodes_count: 419, Cost Time: 1.98s\n",
      "Time: 2018-04-07 06:44:17.080000000~2018-04-07 07:08:21.521000000, Loss: 0.2847, Nodes_count: 440, Cost Time: 2.08s\n",
      "Time: 2018-04-07 07:08:21.521000000~2018-04-07 07:30:13.991000000, Loss: 0.5971, Nodes_count: 468, Cost Time: 2.17s\n",
      "Time: 2018-04-07 07:30:13.991000000~2018-04-07 07:54:56.858000000, Loss: 0.4826, Nodes_count: 492, Cost Time: 2.26s\n",
      "Time: 2018-04-07 07:54:56.858000000~2018-04-07 08:17:34.639000000, Loss: 0.4817, Nodes_count: 514, Cost Time: 2.35s\n",
      "Time: 2018-04-07 08:17:34.639000000~2018-04-07 08:34:49.687000000, Loss: 0.8152, Nodes_count: 537, Cost Time: 2.54s\n",
      "Time: 2018-04-07 08:34:49.687000000~2018-04-07 08:58:17.722000000, Loss: 0.3259, Nodes_count: 561, Cost Time: 2.64s\n",
      "Time: 2018-04-07 08:58:17.722000000~2018-04-07 09:13:29.714000000, Loss: 0.7053, Nodes_count: 700, Cost Time: 7.82s\n",
      "Time: 2018-04-07 09:13:29.714000000~2018-04-07 09:28:31.696000000, Loss: 0.6939, Nodes_count: 772, Cost Time: 15.14s\n",
      "Time: 2018-04-07 09:28:31.696000000~2018-04-07 09:45:27.476000000, Loss: 0.7264, Nodes_count: 816, Cost Time: 23.20s\n",
      "Time: 2018-04-07 09:45:27.476000000~2018-04-07 10:00:33.713000000, Loss: 1.0386, Nodes_count: 835, Cost Time: 24.10s\n",
      "Time: 2018-04-07 10:00:33.713000000~2018-04-07 10:22:14.290000000, Loss: 0.8396, Nodes_count: 1542, Cost Time: 27.79s\n",
      "Time: 2018-04-07 10:22:14.290000000~2018-04-07 10:39:58.094000000, Loss: 0.9716, Nodes_count: 2469, Cost Time: 31.49s\n",
      "Time: 2018-04-07 10:39:58.094000000~2018-04-07 11:00:08.471000000, Loss: 0.7851, Nodes_count: 3246, Cost Time: 38.90s\n",
      "Time: 2018-04-07 11:00:08.471000000~2018-04-07 11:15:32.309000000, Loss: 0.7768, Nodes_count: 3821, Cost Time: 43.60s\n",
      "Time: 2018-04-07 11:15:32.309000000~2018-04-07 11:35:46.576000000, Loss: 0.7932, Nodes_count: 4297, Cost Time: 48.52s\n",
      "Time: 2018-04-07 11:35:46.576000000~2018-04-07 11:51:38.305000000, Loss: 0.5588, Nodes_count: 4381, Cost Time: 50.08s\n",
      "Time: 2018-04-07 11:51:38.305000000~2018-04-07 12:06:57.429000000, Loss: 0.6236, Nodes_count: 4472, Cost Time: 57.30s\n",
      "Time: 2018-04-07 12:06:57.429000000~2018-04-07 12:21:58.368000000, Loss: 0.7976, Nodes_count: 4623, Cost Time: 59.12s\n",
      "Time: 2018-04-07 12:21:58.368000000~2018-04-07 12:37:22.947000000, Loss: 1.0276, Nodes_count: 5239, Cost Time: 61.59s\n",
      "Time: 2018-04-07 12:37:22.947000000~2018-04-07 12:52:26.920000000, Loss: 0.8125, Nodes_count: 6067, Cost Time: 69.44s\n",
      "Time: 2018-04-07 12:52:26.920000000~2018-04-07 13:14:42.953000000, Loss: 0.9990, Nodes_count: 6329, Cost Time: 71.87s\n",
      "Time: 2018-04-07 13:14:42.953000000~2018-04-07 13:41:49.780000000, Loss: 0.4192, Nodes_count: 6352, Cost Time: 72.22s\n",
      "Time: 2018-04-07 13:41:49.780000000~2018-04-07 14:00:00.583000000, Loss: 0.7536, Nodes_count: 6413, Cost Time: 72.66s\n",
      "Time: 2018-04-07 14:00:00.583000000~2018-04-07 14:15:33.098000000, Loss: 0.7697, Nodes_count: 7312, Cost Time: 79.06s\n",
      "Time: 2018-04-07 14:15:33.098000000~2018-04-07 14:37:39.333000000, Loss: 0.8282, Nodes_count: 7844, Cost Time: 84.09s\n",
      "Time: 2018-04-07 14:37:39.333000000~2018-04-07 14:53:20.642000000, Loss: 0.8805, Nodes_count: 7871, Cost Time: 84.76s\n",
      "Time: 2018-04-07 14:53:20.642000000~2018-04-07 15:08:21.119000000, Loss: 0.6780, Nodes_count: 7938, Cost Time: 86.60s\n",
      "Time: 2018-04-07 15:08:21.119000000~2018-04-07 15:24:08.091000000, Loss: 0.7169, Nodes_count: 7950, Cost Time: 91.19s\n",
      "Time: 2018-04-07 15:24:08.091000000~2018-04-07 15:39:13.623000000, Loss: 0.6933, Nodes_count: 8026, Cost Time: 94.70s\n",
      "Time: 2018-04-07 15:39:13.623000000~2018-04-07 16:00:04.364000000, Loss: 0.7201, Nodes_count: 8070, Cost Time: 96.19s\n",
      "Time: 2018-04-07 16:00:04.364000000~2018-04-07 16:25:54.546000000, Loss: 1.7360, Nodes_count: 8424, Cost Time: 97.71s\n",
      "Time: 2018-04-07 16:25:54.546000000~2018-04-07 16:42:27.814000000, Loss: 0.9334, Nodes_count: 8432, Cost Time: 97.91s\n",
      "Time: 2018-04-07 16:42:27.814000000~2018-04-07 16:57:38.063000000, Loss: 0.6657, Nodes_count: 8527, Cost Time: 101.30s\n",
      "Time: 2018-04-07 16:57:38.063000000~2018-04-07 17:12:59.921000000, Loss: 1.1256, Nodes_count: 9175, Cost Time: 105.43s\n",
      "Time: 2018-04-07 17:12:59.921000000~2018-04-07 17:30:04.373000000, Loss: 0.8632, Nodes_count: 9731, Cost Time: 109.56s\n",
      "Time: 2018-04-07 17:30:04.373000000~2018-04-07 17:45:12.921000000, Loss: 0.5756, Nodes_count: 9826, Cost Time: 111.95s\n",
      "Time: 2018-04-07 17:45:12.921000000~2018-04-07 18:00:33.420000000, Loss: 1.0462, Nodes_count: 10730, Cost Time: 117.81s\n",
      "Time: 2018-04-07 18:00:33.420000000~2018-04-07 18:24:44.896000000, Loss: 0.3147, Nodes_count: 10749, Cost Time: 118.89s\n",
      "Time: 2018-04-07 18:24:44.896000000~2018-04-07 18:39:52.248000000, Loss: 1.1285, Nodes_count: 11190, Cost Time: 121.08s\n",
      "Time: 2018-04-07 18:39:52.248000000~2018-04-07 18:56:09.290000000, Loss: 0.7195, Nodes_count: 11300, Cost Time: 125.22s\n",
      "Time: 2018-04-07 18:56:09.290000000~2018-04-07 19:21:04.391000000, Loss: 0.6506, Nodes_count: 11330, Cost Time: 125.77s\n",
      "Time: 2018-04-07 19:21:04.391000000~2018-04-07 19:45:02.042000000, Loss: 0.7872, Nodes_count: 11352, Cost Time: 125.88s\n",
      "Time: 2018-04-07 19:45:02.042000000~2018-04-07 20:00:59.751000000, Loss: 0.7222, Nodes_count: 11389, Cost Time: 126.09s\n",
      "Time: 2018-04-07 20:00:59.751000000~2018-04-07 20:21:50.560000000, Loss: 1.2186, Nodes_count: 11503, Cost Time: 126.63s\n",
      "Time: 2018-04-07 20:21:50.560000000~2018-04-07 20:37:54.893000000, Loss: 1.5084, Nodes_count: 12079, Cost Time: 128.69s\n",
      "Time: 2018-04-07 20:37:54.893000000~2018-04-07 20:52:58.485000000, Loss: 0.8552, Nodes_count: 12858, Cost Time: 137.07s\n",
      "Time: 2018-04-07 20:52:58.485000000~2018-04-07 21:08:28.188000000, Loss: 0.7222, Nodes_count: 12892, Cost Time: 146.61s\n",
      "Time: 2018-04-07 21:08:28.188000000~2018-04-07 21:24:36.756000000, Loss: 0.8160, Nodes_count: 13376, Cost Time: 152.73s\n",
      "Time: 2018-04-07 21:24:36.756000000~2018-04-07 21:42:26.573000000, Loss: 0.2434, Nodes_count: 13379, Cost Time: 153.21s\n",
      "Time: 2018-04-07 21:42:26.573000000~2018-04-07 22:05:45.793000000, Loss: 0.6358, Nodes_count: 13426, Cost Time: 153.55s\n",
      "Time: 2018-04-07 22:05:45.793000000~2018-04-07 22:30:10.178000000, Loss: 0.7088, Nodes_count: 13468, Cost Time: 153.79s\n",
      "Time: 2018-04-07 22:30:10.178000000~2018-04-07 22:47:23.900000000, Loss: 1.0812, Nodes_count: 13481, Cost Time: 153.99s\n",
      "Time: 2018-04-07 22:47:23.900000000~2018-04-07 23:22:01.561000000, Loss: 0.6294, Nodes_count: 13519, Cost Time: 154.32s\n",
      "Time: 2018-04-07 23:22:01.561000000~2018-04-07 23:44:24.554000000, Loss: 0.6840, Nodes_count: 13543, Cost Time: 154.43s\n",
      "Time: 2018-04-07 23:44:24.554000000~2018-04-07 23:59:58.135000000, Loss: 0.2472, Nodes_count: 13557, Cost Time: 154.50s\n"
     ]
    }
   ],
   "source": [
    "ans_4_7=test_day_new(graph_4_7,\"graph_4_7\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after merge: TemporalData(dst=[2554245], msg=[2554245, 40], src=[2554245], t=[2554245])\n",
      "Time: 2018-04-10 00:00:00.041000000~2018-04-10 01:14:00.502000000, Loss: 2.2879, Nodes_count: 81, Cost Time: 0.07s\n",
      "Time: 2018-04-10 01:14:00.502000000~2018-04-10 02:24:00.583000000, Loss: 0.8633, Nodes_count: 156, Cost Time: 0.16s\n",
      "Time: 2018-04-10 02:24:00.583000000~2018-04-10 03:30:37.185000000, Loss: 1.4760, Nodes_count: 226, Cost Time: 0.25s\n",
      "Time: 2018-04-10 03:30:37.185000000~2018-04-10 04:51:00.454000000, Loss: 1.1623, Nodes_count: 300, Cost Time: 0.34s\n",
      "Time: 2018-04-10 04:51:00.454000000~2018-04-10 06:00:07.009000000, Loss: 0.8640, Nodes_count: 367, Cost Time: 0.43s\n",
      "Time: 2018-04-10 06:00:07.009000000~2018-04-10 07:04:52.138000000, Loss: 0.9286, Nodes_count: 442, Cost Time: 0.53s\n",
      "Time: 2018-04-10 07:04:52.138000000~2018-04-10 07:19:59.157000000, Loss: 0.7921, Nodes_count: 1100, Cost Time: 3.18s\n",
      "Time: 2018-04-10 07:19:59.157000000~2018-04-10 07:52:43.143000000, Loss: 0.6916, Nodes_count: 1151, Cost Time: 4.33s\n",
      "Time: 2018-04-10 07:52:43.143000000~2018-04-10 08:11:35.864000000, Loss: 0.9320, Nodes_count: 2173, Cost Time: 8.42s\n",
      "Time: 2018-04-10 08:11:35.864000000~2018-04-10 08:33:00.367000000, Loss: 0.7261, Nodes_count: 2462, Cost Time: 16.84s\n",
      "Time: 2018-04-10 08:33:00.367000000~2018-04-10 08:48:00.599000000, Loss: 0.6990, Nodes_count: 2505, Cost Time: 25.88s\n",
      "Time: 2018-04-10 08:48:00.599000000~2018-04-10 09:03:00.861000000, Loss: 0.5770, Nodes_count: 2560, Cost Time: 27.98s\n",
      "Time: 2018-04-10 09:03:00.861000000~2018-04-10 09:19:34.766000000, Loss: 0.8222, Nodes_count: 2925, Cost Time: 30.41s\n",
      "Time: 2018-04-10 09:19:34.766000000~2018-04-10 09:45:04.552000000, Loss: 0.8275, Nodes_count: 3671, Cost Time: 36.97s\n",
      "Time: 2018-04-10 09:45:04.552000000~2018-04-10 10:06:29.544000000, Loss: 1.0441, Nodes_count: 4097, Cost Time: 40.19s\n",
      "Time: 2018-04-10 10:06:29.544000000~2018-04-10 10:21:32.315000000, Loss: 0.4354, Nodes_count: 4210, Cost Time: 43.96s\n",
      "Time: 2018-04-10 10:21:32.315000000~2018-04-10 10:36:35.907000000, Loss: 0.6237, Nodes_count: 4385, Cost Time: 50.43s\n",
      "Time: 2018-04-10 10:36:35.907000000~2018-04-10 10:53:14.920000000, Loss: 0.6906, Nodes_count: 4417, Cost Time: 51.75s\n",
      "Time: 2018-04-10 10:53:14.920000000~2018-04-10 11:12:35.572000000, Loss: 0.5764, Nodes_count: 4512, Cost Time: 55.47s\n",
      "Time: 2018-04-10 11:12:35.572000000~2018-04-10 11:28:38.819000000, Loss: 0.4850, Nodes_count: 4737, Cost Time: 56.67s\n",
      "Time: 2018-04-10 11:28:38.819000000~2018-04-10 11:43:48.286000000, Loss: 0.6621, Nodes_count: 4848, Cost Time: 59.68s\n",
      "Time: 2018-04-10 11:43:48.286000000~2018-04-10 12:05:28.835000000, Loss: 0.8021, Nodes_count: 4939, Cost Time: 61.16s\n",
      "Time: 2018-04-10 12:05:28.835000000~2018-04-10 12:21:06.782000000, Loss: 0.7489, Nodes_count: 5614, Cost Time: 65.22s\n",
      "Time: 2018-04-10 12:21:06.782000000~2018-04-10 12:45:20.100000000, Loss: 0.9358, Nodes_count: 5769, Cost Time: 66.74s\n",
      "Time: 2018-04-10 12:45:20.100000000~2018-04-10 13:02:56.955000000, Loss: 0.9761, Nodes_count: 6670, Cost Time: 72.61s\n",
      "Time: 2018-04-10 13:02:56.955000000~2018-04-10 13:18:18.179000000, Loss: 0.0475, Nodes_count: 6749, Cost Time: 106.57s\n",
      "Time: 2018-04-10 13:18:18.179000000~2018-04-10 13:34:14.316000000, Loss: 0.5411, Nodes_count: 6832, Cost Time: 115.03s\n",
      "Time: 2018-04-10 13:34:14.316000000~2018-04-10 13:49:28.078000000, Loss: 0.7808, Nodes_count: 7389, Cost Time: 124.05s\n",
      "Time: 2018-04-10 13:49:28.078000000~2018-04-10 14:05:08.084000000, Loss: 1.0579, Nodes_count: 8060, Cost Time: 129.20s\n",
      "Time: 2018-04-10 14:05:08.084000000~2018-04-10 14:21:43.187000000, Loss: 0.6188, Nodes_count: 8142, Cost Time: 131.42s\n",
      "Time: 2018-04-10 14:21:43.187000000~2018-04-10 14:36:57.738000000, Loss: 0.6299, Nodes_count: 8256, Cost Time: 138.54s\n",
      "Time: 2018-04-10 14:36:57.738000000~2018-04-10 14:54:56.906000000, Loss: 0.9062, Nodes_count: 8459, Cost Time: 140.88s\n",
      "Time: 2018-04-10 14:54:56.906000000~2018-04-10 15:28:27.687000000, Loss: 0.7073, Nodes_count: 8595, Cost Time: 148.71s\n",
      "Time: 2018-04-10 15:28:27.687000000~2018-04-10 15:51:57.466000000, Loss: 1.1503, Nodes_count: 9434, Cost Time: 152.56s\n",
      "Time: 2018-04-10 15:51:57.466000000~2018-04-10 16:10:37.208000000, Loss: 0.9214, Nodes_count: 9567, Cost Time: 153.62s\n",
      "Time: 2018-04-10 16:10:37.208000000~2018-04-10 16:25:54.701000000, Loss: 0.7731, Nodes_count: 9787, Cost Time: 156.43s\n",
      "Time: 2018-04-10 16:25:54.701000000~2018-04-10 16:41:04.776000000, Loss: 0.7350, Nodes_count: 10062, Cost Time: 160.28s\n",
      "Time: 2018-04-10 16:41:04.776000000~2018-04-10 16:56:25.853000000, Loss: 0.8742, Nodes_count: 10664, Cost Time: 167.02s\n",
      "Time: 2018-04-10 16:56:25.853000000~2018-04-10 17:11:31.301000000, Loss: 0.5936, Nodes_count: 10929, Cost Time: 172.05s\n",
      "Time: 2018-04-10 17:11:31.301000000~2018-04-10 17:27:31.099000000, Loss: 0.7660, Nodes_count: 11314, Cost Time: 177.89s\n",
      "Time: 2018-04-10 17:27:31.099000000~2018-04-10 17:47:55.016000000, Loss: 0.5212, Nodes_count: 11373, Cost Time: 179.08s\n",
      "Time: 2018-04-10 17:47:55.016000000~2018-04-10 18:02:59.013000000, Loss: 0.9398, Nodes_count: 11877, Cost Time: 182.67s\n",
      "Time: 2018-04-10 18:02:59.013000000~2018-04-10 18:17:59.904000000, Loss: 0.5896, Nodes_count: 12019, Cost Time: 191.02s\n",
      "Time: 2018-04-10 18:17:59.904000000~2018-04-10 18:38:58.139000000, Loss: 0.7718, Nodes_count: 12080, Cost Time: 194.47s\n",
      "Time: 2018-04-10 18:38:58.139000000~2018-04-10 19:00:34.975000000, Loss: 1.0822, Nodes_count: 12515, Cost Time: 196.83s\n",
      "Time: 2018-04-10 19:00:34.975000000~2018-04-10 19:20:44.840000000, Loss: 1.1259, Nodes_count: 12685, Cost Time: 197.71s\n",
      "Time: 2018-04-10 19:20:44.840000000~2018-04-10 19:40:54.882000000, Loss: 1.4362, Nodes_count: 12812, Cost Time: 198.25s\n",
      "Time: 2018-04-10 19:40:54.882000000~2018-04-10 20:01:07.769000000, Loss: 1.3905, Nodes_count: 12983, Cost Time: 198.88s\n",
      "Time: 2018-04-10 20:01:07.769000000~2018-04-10 20:26:09.203000000, Loss: 0.9781, Nodes_count: 13943, Cost Time: 206.29s\n",
      "Time: 2018-04-10 20:26:09.203000000~2018-04-10 20:52:04.007000000, Loss: 1.0045, Nodes_count: 13975, Cost Time: 206.94s\n",
      "Time: 2018-04-10 20:52:04.007000000~2018-04-10 21:12:55.647000000, Loss: 0.9374, Nodes_count: 14407, Cost Time: 209.37s\n",
      "Time: 2018-04-10 21:12:55.647000000~2018-04-10 21:38:13.511000000, Loss: 0.9310, Nodes_count: 14450, Cost Time: 209.98s\n",
      "Time: 2018-04-10 21:38:13.511000000~2018-04-10 21:57:28.644000000, Loss: 0.4284, Nodes_count: 14478, Cost Time: 210.18s\n",
      "Time: 2018-04-10 21:57:28.644000000~2018-04-10 22:17:10.628000000, Loss: 0.2255, Nodes_count: 14503, Cost Time: 211.06s\n",
      "Time: 2018-04-10 22:17:10.628000000~2018-04-10 22:44:13.982000000, Loss: 0.4642, Nodes_count: 14520, Cost Time: 211.24s\n",
      "Time: 2018-04-10 22:44:13.982000000~2018-04-10 23:09:32.277000000, Loss: 0.3280, Nodes_count: 14550, Cost Time: 211.51s\n",
      "Time: 2018-04-10 23:09:32.277000000~2018-04-10 23:59:32.865000000, Loss: 1.3241, Nodes_count: 14606, Cost Time: 211.58s\n"
     ]
    }
   ],
   "source": [
    "ans_4_10=test_day_new(graph_4_10,\"graph_4_10\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after merge: TemporalData(dst=[1976440], msg=[1976440, 40], src=[1976440], t=[1976440])\n",
      "Time: 2018-04-11 00:00:00.063000000~2018-04-11 02:00:00.161000000, Loss: 2.3246, Nodes_count: 127, Cost Time: 0.07s\n",
      "Time: 2018-04-11 02:00:00.161000000~2018-04-11 03:45:00.251000000, Loss: 1.1766, Nodes_count: 230, Cost Time: 0.16s\n",
      "Time: 2018-04-11 03:45:00.251000000~2018-04-11 05:30:00.544000000, Loss: 1.2557, Nodes_count: 333, Cost Time: 0.26s\n",
      "Time: 2018-04-11 05:30:00.544000000~2018-04-11 07:00:52.974000000, Loss: 1.2609, Nodes_count: 439, Cost Time: 0.35s\n",
      "Time: 2018-04-11 07:00:52.974000000~2018-04-11 07:25:16.408000000, Loss: 0.7562, Nodes_count: 482, Cost Time: 0.65s\n",
      "Time: 2018-04-11 07:25:16.408000000~2018-04-11 07:46:44.962000000, Loss: 0.8973, Nodes_count: 514, Cost Time: 0.85s\n",
      "Time: 2018-04-11 07:46:44.962000000~2018-04-11 08:05:53.555000000, Loss: 0.7641, Nodes_count: 548, Cost Time: 0.96s\n",
      "Time: 2018-04-11 08:05:53.555000000~2018-04-11 08:21:10.246000000, Loss: 0.8983, Nodes_count: 744, Cost Time: 2.77s\n",
      "Time: 2018-04-11 08:21:10.246000000~2018-04-11 08:43:29.852000000, Loss: 0.8373, Nodes_count: 786, Cost Time: 3.08s\n",
      "Time: 2018-04-11 08:43:29.852000000~2018-04-11 08:58:36.759000000, Loss: 0.5990, Nodes_count: 891, Cost Time: 3.87s\n",
      "Time: 2018-04-11 08:58:36.759000000~2018-04-11 09:15:23.460000000, Loss: 0.8615, Nodes_count: 2594, Cost Time: 10.58s\n",
      "Time: 2018-04-11 09:15:23.460000000~2018-04-11 09:30:40.459000000, Loss: 0.5498, Nodes_count: 2661, Cost Time: 11.61s\n",
      "Time: 2018-04-11 09:30:40.459000000~2018-04-11 09:45:41.202000000, Loss: 0.7354, Nodes_count: 3334, Cost Time: 15.85s\n",
      "Time: 2018-04-11 09:45:41.202000000~2018-04-11 10:02:14.946000000, Loss: 0.5304, Nodes_count: 3515, Cost Time: 18.66s\n",
      "Time: 2018-04-11 10:02:14.946000000~2018-04-11 10:21:29.924000000, Loss: 0.6472, Nodes_count: 3752, Cost Time: 22.38s\n",
      "Time: 2018-04-11 10:21:29.924000000~2018-04-11 10:36:41.077000000, Loss: 0.5038, Nodes_count: 4436, Cost Time: 35.54s\n",
      "Time: 2018-04-11 10:36:41.077000000~2018-04-11 10:53:51.436000000, Loss: 0.3343, Nodes_count: 4597, Cost Time: 41.02s\n",
      "Time: 2018-04-11 10:53:51.436000000~2018-04-11 11:11:03.735000000, Loss: 0.8736, Nodes_count: 4831, Cost Time: 44.29s\n",
      "Time: 2018-04-11 11:11:03.735000000~2018-04-11 11:27:06.742000000, Loss: 1.0515, Nodes_count: 5587, Cost Time: 49.09s\n",
      "Time: 2018-04-11 11:27:06.742000000~2018-04-11 11:42:13.848000000, Loss: 0.4572, Nodes_count: 5777, Cost Time: 51.86s\n",
      "Time: 2018-04-11 11:42:13.848000000~2018-04-11 11:59:50.272000000, Loss: 0.7308, Nodes_count: 6163, Cost Time: 57.10s\n",
      "Time: 2018-04-11 11:59:50.272000000~2018-04-11 12:15:10.640000000, Loss: 1.1011, Nodes_count: 6619, Cost Time: 59.93s\n",
      "Time: 2018-04-11 12:15:10.640000000~2018-04-11 12:30:15.769000000, Loss: 1.2114, Nodes_count: 6680, Cost Time: 60.32s\n",
      "Time: 2018-04-11 12:30:15.769000000~2018-04-11 12:45:30.888000000, Loss: 1.7398, Nodes_count: 6832, Cost Time: 64.96s\n",
      "Time: 2018-04-11 12:45:30.888000000~2018-04-11 13:01:09.797000000, Loss: 1.3769, Nodes_count: 7500, Cost Time: 69.20s\n",
      "Time: 2018-04-11 13:01:09.797000000~2018-04-11 13:16:29.838000000, Loss: 1.5668, Nodes_count: 8579, Cost Time: 72.99s\n",
      "Time: 2018-04-11 13:16:29.838000000~2018-04-11 13:31:30.828000000, Loss: 1.7550, Nodes_count: 9592, Cost Time: 78.13s\n",
      "Time: 2018-04-11 13:31:30.828000000~2018-04-11 13:46:38.658000000, Loss: 1.7512, Nodes_count: 10337, Cost Time: 83.39s\n",
      "Time: 2018-04-11 13:46:38.658000000~2018-04-11 14:02:21.103000000, Loss: 1.6285, Nodes_count: 11153, Cost Time: 89.73s\n",
      "Time: 2018-04-11 14:02:21.103000000~2018-04-11 14:18:19.001000000, Loss: 1.6953, Nodes_count: 12116, Cost Time: 95.22s\n",
      "Time: 2018-04-11 14:18:19.001000000~2018-04-11 14:33:38.600000000, Loss: 1.6350, Nodes_count: 13192, Cost Time: 100.85s\n",
      "Time: 2018-04-11 14:33:38.600000000~2018-04-11 14:49:05.326000000, Loss: 0.9075, Nodes_count: 13440, Cost Time: 104.45s\n",
      "Time: 2018-04-11 14:49:05.326000000~2018-04-11 15:04:48.749000000, Loss: 1.3407, Nodes_count: 13870, Cost Time: 108.65s\n",
      "Time: 2018-04-11 15:04:48.749000000~2018-04-11 15:23:04.703000000, Loss: 1.4841, Nodes_count: 14026, Cost Time: 110.53s\n",
      "Time: 2018-04-11 15:23:04.703000000~2018-04-11 15:39:01.167000000, Loss: 2.0491, Nodes_count: 14174, Cost Time: 112.87s\n",
      "Time: 2018-04-11 15:39:01.167000000~2018-04-11 15:54:01.828000000, Loss: 1.5326, Nodes_count: 14762, Cost Time: 117.46s\n",
      "Time: 2018-04-11 15:54:01.828000000~2018-04-11 16:09:02.909000000, Loss: 1.6620, Nodes_count: 15025, Cost Time: 120.34s\n",
      "Time: 2018-04-11 16:09:02.909000000~2018-04-11 16:24:25.756000000, Loss: 1.6067, Nodes_count: 15746, Cost Time: 124.29s\n",
      "Time: 2018-04-11 16:24:25.756000000~2018-04-11 16:44:13.639000000, Loss: 1.4602, Nodes_count: 16251, Cost Time: 128.96s\n",
      "Time: 2018-04-11 16:44:13.639000000~2018-04-11 16:59:14.685000000, Loss: 1.2729, Nodes_count: 16933, Cost Time: 132.75s\n",
      "Time: 2018-04-11 16:59:14.685000000~2018-04-11 17:14:26.172000000, Loss: 1.2842, Nodes_count: 17438, Cost Time: 137.00s\n",
      "Time: 2018-04-11 17:14:26.172000000~2018-04-11 17:29:29.020000000, Loss: 1.5520, Nodes_count: 17966, Cost Time: 141.98s\n",
      "Time: 2018-04-11 17:29:29.020000000~2018-04-11 17:44:48.533000000, Loss: 1.5638, Nodes_count: 18466, Cost Time: 147.23s\n",
      "Time: 2018-04-11 17:44:48.533000000~2018-04-11 17:59:53.338000000, Loss: 1.5892, Nodes_count: 19017, Cost Time: 152.55s\n",
      "Time: 2018-04-11 17:59:53.338000000~2018-04-11 18:16:39.616000000, Loss: 0.3697, Nodes_count: 19268, Cost Time: 159.81s\n",
      "Time: 2018-04-11 18:16:39.616000000~2018-04-11 18:32:06.970000000, Loss: 1.5863, Nodes_count: 19822, Cost Time: 165.53s\n",
      "Time: 2018-04-11 18:32:06.970000000~2018-04-11 18:47:24.055000000, Loss: 1.5061, Nodes_count: 20424, Cost Time: 169.64s\n",
      "Time: 2018-04-11 18:47:24.055000000~2018-04-11 19:02:37.294000000, Loss: 1.4319, Nodes_count: 20887, Cost Time: 175.80s\n",
      "Time: 2018-04-11 19:02:37.294000000~2018-04-11 19:18:03.851000000, Loss: 0.8661, Nodes_count: 21288, Cost Time: 181.31s\n",
      "Time: 2018-04-11 19:18:03.851000000~2018-04-11 19:33:17.804000000, Loss: 1.5229, Nodes_count: 21834, Cost Time: 186.64s\n",
      "Time: 2018-04-11 19:33:17.804000000~2018-04-11 19:48:43.675000000, Loss: 1.4288, Nodes_count: 22117, Cost Time: 191.76s\n",
      "Time: 2018-04-11 19:48:43.675000000~2018-04-11 20:04:35.343000000, Loss: 0.3387, Nodes_count: 22349, Cost Time: 198.90s\n",
      "Time: 2018-04-11 20:04:35.343000000~2018-04-11 20:27:04.183000000, Loss: 1.5131, Nodes_count: 22587, Cost Time: 201.49s\n",
      "Time: 2018-04-11 20:27:04.183000000~2018-04-11 20:46:59.749000000, Loss: 1.4586, Nodes_count: 23551, Cost Time: 205.91s\n",
      "Time: 2018-04-11 20:46:59.749000000~2018-04-11 21:04:21.264000000, Loss: 1.5987, Nodes_count: 23854, Cost Time: 208.30s\n",
      "Time: 2018-04-11 21:04:21.264000000~2018-04-11 21:29:14.176000000, Loss: 1.5152, Nodes_count: 24180, Cost Time: 210.60s\n",
      "Time: 2018-04-11 21:29:14.176000000~2018-04-11 21:44:15.839000000, Loss: 0.7105, Nodes_count: 24644, Cost Time: 215.19s\n",
      "Time: 2018-04-11 21:44:15.839000000~2018-04-11 21:59:40.869000000, Loss: 1.4229, Nodes_count: 25119, Cost Time: 219.21s\n",
      "Time: 2018-04-11 21:59:40.869000000~2018-04-11 22:18:09.134000000, Loss: 0.4378, Nodes_count: 25398, Cost Time: 224.37s\n",
      "Time: 2018-04-11 22:18:09.134000000~2018-04-11 22:33:22.263000000, Loss: 1.5473, Nodes_count: 26030, Cost Time: 230.20s\n",
      "Time: 2018-04-11 22:33:22.263000000~2018-04-11 22:49:00.957000000, Loss: 1.3433, Nodes_count: 26346, Cost Time: 233.65s\n",
      "Time: 2018-04-11 22:49:00.957000000~2018-04-11 23:46:00.517000000, Loss: 1.6301, Nodes_count: 26396, Cost Time: 233.90s\n"
     ]
    }
   ],
   "source": [
    "ans_4_11=test_day_new(graph_4_11,\"graph_4_11\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize the node IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 293/293 [03:00<00:00,  1.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IDF weight calculate complete!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "node_set=set()\n",
    "\n",
    "file_list=[]\n",
    "\n",
    "file_path=\"graph_4_4/\"\n",
    "file_l=os.listdir(\"graph_4_4/\")\n",
    "for i in file_l:\n",
    "    file_list.append(file_path+i)\n",
    "\n",
    "file_path=\"graph_4_5/\"\n",
    "file_l=os.listdir(\"graph_4_5/\")\n",
    "for i in file_l:\n",
    "    file_list.append(file_path+i)\n",
    "\n",
    "file_path=\"graph_4_6/\"\n",
    "file_l=os.listdir(\"graph_4_6/\")\n",
    "for i in file_l:\n",
    "    file_list.append(file_path+i)\n",
    "\n",
    "\n",
    "file_path=\"graph_4_7/\"\n",
    "file_l=os.listdir(\"graph_4_7/\")\n",
    "for i in file_l:\n",
    "    file_list.append(file_path+i)\n",
    "\n",
    "node_IDF={}\n",
    "node_set = {}\n",
    "for f_path in tqdm(file_list):\n",
    "    f=open(f_path)\n",
    "    for line in f:\n",
    "        l=line.strip()\n",
    "        jdata=eval(l)\n",
    "        jdata=eval(l)\n",
    "        if jdata['loss']>0:\n",
    "            if 'netflow' not in str(jdata['srcmsg']):\n",
    "                if str(jdata['srcmsg']) not in node_set.keys():\n",
    "                    node_set[str(jdata['srcmsg'])] = set([f_path])\n",
    "                else:\n",
    "                    node_set[str(jdata['srcmsg'])].add(f_path)\n",
    "            if 'netflow' not in str(jdata['dstmsg']):\n",
    "                if str(jdata['dstmsg']) not in node_set.keys():\n",
    "                    node_set[str(jdata['dstmsg'])] = set([f_path])\n",
    "                else:\n",
    "                    node_set[str(jdata['dstmsg'])].add(f_path)\n",
    "for n in node_set:\n",
    "    include_count = len(node_set[n])   \n",
    "    IDF=math.log(len(file_list)/(include_count+1))\n",
    "    node_IDF[n] = IDF    \n",
    "\n",
    "\n",
    "torch.save(node_IDF,\"node_IDF\")\n",
    "print(\"IDF weight calculate complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_train_IDF(find_str,file_list):\n",
    "    include_count=0\n",
    "    for f_path in (file_list):\n",
    "        f=open(f_path)\n",
    "        if find_str in f.read():\n",
    "            include_count+=1             \n",
    "    IDF=math.log(len(file_list)/(include_count+1))\n",
    "    return IDF\n",
    "\n",
    "\n",
    "def cal_IDF(find_str,file_path,file_list):\n",
    "    file_list=os.listdir(file_path)\n",
    "    include_count=0\n",
    "    different_neighbor=set()\n",
    "    for f_path in (file_list):\n",
    "        f=open(file_path+f_path)\n",
    "        if find_str in f.read():\n",
    "            include_count+=1                \n",
    "                \n",
    "    IDF=math.log(len(file_list)/(include_count+1))\n",
    "    \n",
    "    return IDF,1\n",
    "\n",
    "def cal_redundant(find_str,edge_list):\n",
    "    \n",
    "    different_neighbor=set()\n",
    "    for e in edge_list:\n",
    "        if find_str in str(e):\n",
    "            different_neighbor.add(e[0])\n",
    "            different_neighbor.add(e[1])\n",
    "    return len(different_neighbor)-2\n",
    "\n",
    "def cal_anomaly_loss(loss_list,edge_list,file_path):\n",
    "    \n",
    "    if len(loss_list)!=len(edge_list):\n",
    "        print(\"error!\")\n",
    "        return 0\n",
    "    count=0\n",
    "    loss_sum=0\n",
    "    loss_std=std(loss_list)\n",
    "    loss_mean=mean(loss_list)\n",
    "    edge_set=set()\n",
    "    node_set=set()\n",
    "    node2redundant={}\n",
    "    \n",
    "    thr=loss_mean+1.5*loss_std\n",
    "\n",
    "    print(\"thr:\",thr)\n",
    "\n",
    "    for i in range(len(loss_list)):\n",
    "        if loss_list[i]>thr:\n",
    "            count+=1\n",
    "            src_node=edge_list[i][0]\n",
    "            dst_node=edge_list[i][1]\n",
    "            \n",
    "            loss_sum+=loss_list[i]\n",
    "    \n",
    "            node_set.add(src_node)\n",
    "            node_set.add(dst_node)\n",
    "            edge_set.add(edge_list[i][0]+edge_list[i][1])\n",
    "    return count, loss_sum/(count + 0.00001) ,node_set,edge_set\n",
    "#     return count, count/len(loss_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construct the relations between time windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def is_include_key_word(s):\n",
    "    keywords=[\n",
    "         'netflow',\n",
    "\n",
    "        'glx_alsa_675',\n",
    "        '/data/system/',\n",
    "         '/storage/emulated/',\n",
    "        '/data/data/com.android',\n",
    "        '/proc/',\n",
    "        'nz9885vc.default',\n",
    "      \n",
    "      ]\n",
    "    flag=False\n",
    "    for i in keywords:\n",
    "        if i in s:\n",
    "            flag=True\n",
    "    return flag\n",
    "\n",
    "\n",
    "\n",
    "def cal_set_rel(s1,s2,node_IDF, file_list):\n",
    "    new_s=s1 & s2\n",
    "    count=0\n",
    "    for i in new_s:\n",
    "#     jdata=json.loads(i)\n",
    "        if is_include_key_word(i) is False :\n",
    "            if i in node_IDF.keys():\n",
    "                IDF=node_IDF[i]\n",
    "            else:\n",
    "                IDF=math.log(len(file_list)/(1))\n",
    "            if IDF>6:\n",
    "                print(\"node:\",i,\" IDF:\",IDF)\n",
    "                count+=1\n",
    "    return count\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# def cal_set_rel(s1,s2,node_IDF, file_list, node_IDF_4_4_7, file_list_4_4_7):\n",
    "#     new_s=s1 & s2\n",
    "#     count=0\n",
    "#     for i in new_s:\n",
    "# #     jdata=json.loads(i)\n",
    "#         if 'netflow' not in i and 'glx_alsa_675' not in i and '/data/system/' not in i and '/storage/emulated/' not in i and  '/data/data/com.android' not in i and  '/proc/' not in i and 'nz9885vc.default' not in i :\n",
    "\n",
    "# #         'netflow' not in i\n",
    "# #         and 'usr' not in i and 'var' not in i\n",
    "#             if i in node_IDF.keys():\n",
    "#                 IDF=node_IDF[i]\n",
    "#             else:\n",
    "#                 IDF=math.log(len(file_list)/(1))\n",
    "                \n",
    "#             if i in node_IDF_4_4_7.keys():\n",
    "#                 IDF4=node_IDF_4_4_7[i]\n",
    "#             else:\n",
    "#                 IDF4=math.log(len(file_list_4_4_7)/(1))    \n",
    "            \n",
    "# #             print(IDF)\n",
    "#             if (IDF+IDF4)>9:\n",
    "#                 print(\"node:\",i,\" IDF:\",IDF+IDF4)\n",
    "#                 count+=1\n",
    "#     return count\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# label generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels={}\n",
    "    \n",
    "    \n",
    "filelist = os.listdir(\"graph_4_10\")\n",
    "for f in filelist:\n",
    "    labels[\"graph_4_10/\"+f]=0\n",
    "\n",
    "filelist = os.listdir(\"graph_4_11\")\n",
    "for f in filelist:\n",
    "    labels[\"graph_4_11/\"+f]=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "attack_list=[\n",
    "    'graph_4_11/2018-04-11 13:46:38.658000000~2018-04-11 14:02:21.103000000.txt',\n",
    "    'graph_4_11/2018-04-11 14:02:21.103000000~2018-04-11 14:18:19.001000000.txt',\n",
    "    'graph_4_11/2018-04-11 14:18:19.001000000~2018-04-11 14:33:38.600000000.txt',\n",
    "    'graph_4_11/2018-04-11 14:33:38.600000000~2018-04-11 14:49:05.326000000.txt',\n",
    "    'graph_4_11/2018-04-11 14:49:05.326000000~2018-04-11 15:04:48.749000000.txt',\n",
    "]\n",
    "for i in attack_list:\n",
    "    labels[i]=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benign count: 114\n",
      "Attack count: 5\n"
     ]
    }
   ],
   "source": [
    "print(f\"Benign count: {len(labels.values()) - sum(labels.values())}\")\n",
    "print(f\"Attack count: {sum(labels.values())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_label={}\n",
    "\n",
    "filelist = os.listdir(\"graph_4_10/\")\n",
    "for f in filelist:\n",
    "    pred_label[\"graph_4_10/\"+f]=0\n",
    "    \n",
    "filelist = os.listdir(\"graph_4_11/\")\n",
    "for f in filelist:\n",
    "    pred_label[\"graph_4_11/\"+f]=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_list=[]\n",
    "\n",
    "file_path=\"graph_4_4/\"\n",
    "file_l=os.listdir(\"graph_4_4/\")\n",
    "for i in file_l:\n",
    "    file_list.append(file_path+i)\n",
    "\n",
    "file_path=\"graph_4_5/\"\n",
    "file_l=os.listdir(\"graph_4_5/\")\n",
    "for i in file_l:\n",
    "    file_list.append(file_path+i)\n",
    "\n",
    "file_path=\"graph_4_6/\"\n",
    "file_l=os.listdir(\"graph_4_6/\")\n",
    "for i in file_l:\n",
    "    file_list.append(file_path+i)\n",
    "\n",
    "\n",
    "file_path=\"graph_4_7/\"\n",
    "file_l=os.listdir(\"graph_4_7/\")\n",
    "for i in file_l:\n",
    "    file_list.append(file_path+i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index_count: 0\n",
      "thr: 2.8531981129075934\n",
      "2018-04-10 00:00:00.041000000~2018-04-10 01:14:00.502000000.txt    0.0  count: 0  percentage: 0.0  node count: 0  edge count: 0\n",
      "index_count: 1\n",
      "thr: 2.62544484760982\n",
      "2018-04-10 01:14:00.502000000~2018-04-10 02:24:00.583000000.txt    3.256865427079852  count: 111  percentage: 0.1083984375  node count: 71  edge count: 69\n",
      "index_count: 2\n",
      "thr: 4.441793639232343\n",
      "2018-04-10 02:24:00.583000000~2018-04-10 03:30:37.185000000.txt    6.022709648928218  count: 119  percentage: 0.1162109375  node count: 5  edge count: 3\n",
      "index_count: 3\n",
      "thr: 3.5811508321365713\n",
      "2018-04-10 03:30:37.185000000~2018-04-10 04:51:00.454000000.txt    4.999476101846638  count: 110  percentage: 0.107421875  node count: 6  edge count: 5\n",
      "index_count: 4\n",
      "thr: 2.608054670654323\n",
      "2018-04-10 04:51:00.454000000~2018-04-10 06:00:07.009000000.txt    3.9850593269069865  count: 54  percentage: 0.052734375  node count: 11  edge count: 9\n",
      "index_count: 5\n",
      "thr: 2.6798890573177845\n",
      "2018-04-10 06:00:07.009000000~2018-04-10 07:04:52.138000000.txt    3.3933401013357307  count: 84  percentage: 0.08203125  node count: 20  edge count: 15\n",
      "index_count: 6\n",
      "thr: 2.210513429367689\n",
      "2018-04-10 07:04:52.138000000~2018-04-10 07:19:59.157000000.txt    3.1436579273522285  count: 3578  percentage: 0.1027688419117647  node count: 418  edge count: 418\n",
      "index_count: 7\n",
      "thr: 1.5078098090070209\n",
      "2018-04-10 07:19:59.157000000~2018-04-10 07:52:43.143000000.txt    2.2739083224426255  count: 540  percentage: 0.04056490384615385  node count: 79  edge count: 74\n",
      "index_count: 8\n",
      "thr: 2.272825108853063\n",
      "2018-04-10 07:52:43.143000000~2018-04-10 08:11:35.864000000.txt    3.357772526561615  count: 4751  percentage: 0.08922400841346154  node count: 694  edge count: 693\n",
      "index_count: 9\n",
      "thr: 1.4240812689941835\n",
      "2018-04-10 08:11:35.864000000~2018-04-10 08:33:00.367000000.txt    2.917444670145515  count: 2898  percentage: 0.023983712923728813  node count: 348  edge count: 355\n",
      "index_count: 10\n",
      "thr: 1.1649350111139931\n",
      "2018-04-10 08:33:00.367000000~2018-04-10 08:48:00.599000000.txt    2.5917220254220634  count: 788  percentage: 0.006011962890625  node count: 89  edge count: 93\n",
      "index_count: 11\n",
      "thr: 1.6459653430025416\n",
      "2018-04-10 08:48:00.599000000~2018-04-10 09:03:00.861000000.txt    3.543401342241999  count: 504  percentage: 0.02734375  node count: 101  edge count: 96\n",
      "index_count: 12\n",
      "thr: 2.4559386401820382\n",
      "2018-04-10 09:03:00.861000000~2018-04-10 09:19:34.766000000.txt    3.4086982314773775  count: 3081  percentage: 0.11143663194444445  node count: 258  edge count: 262\n",
      "index_count: 13\n",
      "thr: 2.2008520792260686\n",
      "2018-04-10 09:19:34.766000000~2018-04-10 09:45:04.552000000.txt    3.3297131556266066  count: 6768  percentage: 0.08696546052631579  node count: 421  edge count: 427\n",
      "index_count: 14\n",
      "thr: 2.731117982757869\n",
      "2018-04-10 09:45:04.552000000~2018-04-10 10:06:29.544000000.txt    3.5339115333201905  count: 3376  percentage: 0.11774553571428571  node count: 330  edge count: 329\n",
      "index_count: 15\n",
      "thr: 1.682197202270462\n",
      "2018-04-10 10:06:29.544000000~2018-04-10 10:21:32.315000000.txt    3.647081600662617  count: 1786  percentage: 0.037916100543478264  node count: 176  edge count: 185\n",
      "index_count: 16\n",
      "thr: 1.516505386201049\n",
      "2018-04-10 10:21:32.315000000~2018-04-10 10:36:35.907000000.txt    3.1523271935688557  count: 1951  percentage: 0.02268182663690476  node count: 249  edge count: 288\n",
      "index_count: 17\n",
      "thr: 1.600909173264323\n",
      "2018-04-10 10:36:35.907000000~2018-04-10 10:53:14.920000000.txt    3.1765882618657018  count: 376  percentage: 0.033380681818181816  node count: 85  edge count: 88\n",
      "index_count: 18\n",
      "thr: 1.6016568601582781\n",
      "2018-04-10 10:53:14.920000000~2018-04-10 11:12:35.572000000.txt    3.554776054597685  count: 1273  percentage: 0.02486328125  node count: 190  edge count: 198\n",
      "index_count: 19\n",
      "thr: 2.1999616586836717\n",
      "2018-04-10 11:12:35.572000000~2018-04-10 11:28:38.819000000.txt    3.5950309409013386  count: 940  percentage: 0.091796875  node count: 297  edge count: 308\n",
      "index_count: 20\n",
      "thr: 1.5406384396754462\n",
      "2018-04-10 11:28:38.819000000~2018-04-10 11:43:48.286000000.txt    2.774362476249044  count: 1388  percentage: 0.03388671875  node count: 176  edge count: 186\n",
      "index_count: 21\n",
      "thr: 2.1520109064720017\n",
      "2018-04-10 11:43:48.286000000~2018-04-10 12:05:28.835000000.txt    3.449425827736447  count: 986  percentage: 0.06877790178571429  node count: 236  edge count: 277\n",
      "index_count: 22\n",
      "thr: 2.4000386955571527\n",
      "2018-04-10 12:05:28.835000000~2018-04-10 12:21:06.782000000.txt    3.8248491616366582  count: 3981  percentage: 0.08451511548913043  node count: 653  edge count: 716\n",
      "index_count: 23\n",
      "thr: 2.504587304428769\n",
      "2018-04-10 12:21:06.782000000~2018-04-10 12:45:20.100000000.txt    3.2550391710908273  count: 1531  percentage: 0.10679408482142858  node count: 136  edge count: 135\n",
      "index_count: 24\n",
      "thr: 2.470030252399823\n",
      "2018-04-10 12:45:20.100000000~2018-04-10 13:02:56.955000000.txt    3.6684756674578445  count: 6528  percentage: 0.09659090909090909  node count: 585  edge count: 583\n",
      "index_count: 25\n",
      "thr: 0.3345450437564341\n",
      "2018-04-10 13:02:56.955000000~2018-04-10 13:18:18.179000000.txt    0.9009461842287649  count: 14035  percentage: 0.02757757482394366  node count: 167  edge count: 187\n",
      "index_count: 26\n",
      "thr: 1.2061416757887105\n",
      "2018-04-10 13:18:18.179000000~2018-04-10 13:34:14.316000000.txt    2.181960512649792  count: 1303  percentage: 0.017673068576388888  node count: 159  edge count: 186\n",
      "index_count: 27\n",
      "thr: 1.7871336340939084\n",
      "2018-04-10 13:34:14.316000000~2018-04-10 13:49:28.078000000.txt    3.22854370406642  count: 5356  percentage: 0.047985951834862386  node count: 574  edge count: 601\n",
      "index_count: 28\n",
      "thr: 2.8170953688874922\n",
      "2018-04-10 13:49:28.078000000~2018-04-10 14:05:08.084000000.txt    3.8911794080749686  count: 4960  percentage: 0.11264534883720931  node count: 317  edge count: 349\n",
      "index_count: 29\n",
      "thr: 2.298489000123968\n",
      "2018-04-10 14:05:08.084000000~2018-04-10 14:21:43.187000000.txt    3.3178634968258676  count: 2510  percentage: 0.11141690340909091  node count: 136  edge count: 140\n",
      "index_count: 30\n",
      "thr: 1.4362312625201743\n",
      "2018-04-10 14:21:43.187000000~2018-04-10 14:36:57.738000000.txt    3.27761400522857  count: 1704  percentage: 0.01715528350515464  node count: 222  edge count: 260\n",
      "index_count: 31\n",
      "thr: 2.5414466655625407\n",
      "2018-04-10 14:36:57.738000000~2018-04-10 14:54:56.906000000.txt    3.586202842811138  count: 1738  percentage: 0.1060791015625  node count: 290  edge count: 299\n",
      "index_count: 32\n",
      "thr: 1.3747478569041975\n",
      "2018-04-10 14:54:56.906000000~2018-04-10 15:28:27.687000000.txt    2.527756402794053  count: 2853  percentage: 0.02603862441588785  node count: 381  edge count: 419\n",
      "index_count: 33\n",
      "thr: 2.936293303730661\n",
      "2018-04-10 15:28:27.687000000~2018-04-10 15:51:57.466000000.txt    3.6422830208754373  count: 4332  percentage: 0.141015625  node count: 476  edge count: 473\n",
      "index_count: 34\n",
      "thr: 2.5835614343961084\n",
      "2018-04-10 15:51:57.466000000~2018-04-10 16:10:37.208000000.txt    3.615604835616067  count: 898  percentage: 0.109619140625  node count: 115  edge count: 117\n",
      "index_count: 35\n",
      "thr: 2.496631161961684\n",
      "2018-04-10 16:10:37.208000000~2018-04-10 16:25:54.701000000.txt    3.583011395217153  count: 2989  percentage: 0.10065328663793104  node count: 223  edge count: 244\n",
      "index_count: 36\n",
      "thr: 2.080277505724041\n",
      "2018-04-10 16:25:54.701000000~2018-04-10 16:41:04.776000000.txt    2.955064106921198  count: 4100  percentage: 0.09311409883720931  node count: 315  edge count: 324\n",
      "index_count: 37\n",
      "thr: 2.288456267454205\n",
      "2018-04-10 16:41:04.776000000~2018-04-10 16:56:25.853000000.txt    3.5540838202412983  count: 6031  percentage: 0.08535722373188406  node count: 513  edge count: 529\n",
      "index_count: 38\n",
      "thr: 2.01804123461486\n",
      "2018-04-10 16:56:25.853000000~2018-04-10 17:11:31.301000000.txt    3.8469382640201286  count: 2898  percentage: 0.051455965909090906  node count: 380  edge count: 412\n",
      "index_count: 39\n",
      "thr: 2.0993517117965568\n",
      "2018-04-10 17:11:31.301000000~2018-04-10 17:27:31.099000000.txt    3.5527139614512375  count: 4373  percentage: 0.06887915826612903  node count: 617  edge count: 633\n",
      "index_count: 40\n",
      "thr: 1.9849093770466182\n",
      "2018-04-10 17:27:31.099000000~2018-04-10 17:47:55.016000000.txt    3.1762024348909375  count: 720  percentage: 0.087890625  node count: 147  edge count: 179\n",
      "index_count: 41\n",
      "thr: 2.555545218342427\n",
      "2018-04-10 17:47:55.016000000~2018-04-10 18:02:59.013000000.txt    3.6384057203740063  count: 3445  percentage: 0.10194720643939394  node count: 432  edge count: 437\n",
      "index_count: 42\n",
      "thr: 1.3053159384519573\n",
      "2018-04-10 18:02:59.013000000~2018-04-10 18:17:59.904000000.txt    2.6789951326450483  count: 1961  percentage: 0.016947248340707963  node count: 240  edge count: 285\n",
      "index_count: 43\n",
      "thr: 1.5663191726377508\n",
      "2018-04-10 18:17:59.904000000~2018-04-10 18:38:58.139000000.txt    2.8666549407922735  count: 1330  percentage: 0.036078559027777776  node count: 292  edge count: 325\n",
      "index_count: 44\n",
      "thr: 2.693497274327946\n",
      "2018-04-10 18:38:58.139000000~2018-04-10 19:00:34.975000000.txt    3.661834249612933  count: 2480  percentage: 0.11532738095238096  node count: 367  edge count: 367\n",
      "index_count: 45\n",
      "thr: 2.8335269910765772\n",
      "2018-04-10 19:00:34.975000000~2018-04-10 19:20:44.840000000.txt    3.5612359203336017  count: 970  percentage: 0.13532366071428573  node count: 150  edge count: 149\n",
      "index_count: 46\n",
      "thr: 3.305415306470232\n",
      "2018-04-10 19:20:44.840000000~2018-04-10 19:40:54.882000000.txt    3.9031264167932056  count: 492  percentage: 0.1201171875  node count: 95  edge count: 91\n",
      "index_count: 47\n",
      "thr: 3.1673762941594568\n",
      "2018-04-10 19:40:54.882000000~2018-04-10 20:01:07.769000000.txt    3.6233359534140797  count: 810  percentage: 0.158203125  node count: 118  edge count: 116\n",
      "index_count: 48\n",
      "thr: 2.4474867587494864\n",
      "2018-04-10 20:01:07.769000000~2018-04-10 20:26:09.203000000.txt    3.554540969219827  count: 8375  percentage: 0.10352798655063292  node count: 894  edge count: 894\n",
      "index_count: 49\n",
      "thr: 2.866268761537629\n",
      "2018-04-10 20:26:09.203000000~2018-04-10 20:52:04.007000000.txt    3.8357432601939414  count: 104  percentage: 0.1015625  node count: 26  edge count: 22\n",
      "index_count: 50\n",
      "thr: 2.4936756374461124\n",
      "2018-04-10 20:52:04.007000000~2018-04-10 21:12:55.647000000.txt    3.6101647842751703  count: 2754  percentage: 0.1034405048076923  node count: 343  edge count: 343\n",
      "index_count: 51\n",
      "thr: 2.6717847778740316\n",
      "2018-04-10 21:12:55.647000000~2018-04-10 21:38:13.511000000.txt    3.6591201836429446  count: 375  percentage: 0.091552734375  node count: 65  edge count: 82\n",
      "index_count: 52\n",
      "thr: 1.6763289890876623\n",
      "2018-04-10 21:38:13.511000000~2018-04-10 21:57:28.644000000.txt    2.9313446663081146  count: 151  percentage: 0.07373046875  node count: 71  edge count: 69\n",
      "index_count: 53\n",
      "thr: 1.713458777340625\n",
      "2018-04-10 21:57:28.644000000~2018-04-10 22:17:10.628000000.txt    4.4398124613402095  count: 472  percentage: 0.04190340909090909  node count: 67  edge count: 64\n",
      "index_count: 54\n",
      "thr: 1.6971343454611316\n",
      "2018-04-10 22:17:10.628000000~2018-04-10 22:44:13.982000000.txt    2.34164694107558  count: 139  percentage: 0.1357421875  node count: 44  edge count: 40\n",
      "index_count: 55\n",
      "thr: 1.7306274941923208\n",
      "2018-04-10 22:44:13.982000000~2018-04-10 23:09:32.277000000.txt    3.112306998994098  count: 233  percentage: 0.07584635416666667  node count: 65  edge count: 61\n",
      "index_count: 56\n",
      "thr: 2.9626588578970585\n",
      "2018-04-10 23:09:32.277000000~2018-04-10 23:59:32.865000000.txt    4.191190405860209  count: 12  percentage: 0.030848329048843187  node count: 7  edge count: 5\n"
     ]
    }
   ],
   "source": [
    "# node_IDF=torch.load(\"node_IDF_4_10\")\n",
    "# node_IDF_4_7=torch.load(\"node_IDF_4_4-7\")\n",
    "node_IDF_4_4_7=torch.load(\"node_IDF\")\n",
    "y_data_4_10=[]\n",
    "df_list_4_10=[]\n",
    "# node_set_list=[]\n",
    "history_list=[]\n",
    "tw_que=[]\n",
    "his_tw={}\n",
    "current_tw={}\n",
    "loss_list_4_10=[]\n",
    "\n",
    "\n",
    "file_l=os.listdir(\"graph_4_10\")\n",
    "index_count=0\n",
    "for f_path in sorted(file_l):\n",
    "    f=open(\"graph_4_10/\"+f_path)\n",
    "    edge_loss_list=[]\n",
    "    edge_list=[]\n",
    "    print('index_count:',index_count)\n",
    "#     print(f_path)\n",
    "    for line in f:\n",
    "        l=line.strip()\n",
    "        jdata=eval(l)\n",
    "        edge_loss_list.append(jdata['loss'])\n",
    "        edge_list.append([str(jdata['srcmsg']),str(jdata['dstmsg'])])\n",
    "    df_list_4_10.append(pd.DataFrame(edge_loss_list))\n",
    "    count,loss_avg,node_set,edge_set=cal_anomaly_loss(edge_loss_list,edge_list,\"graph_4_10/\")\n",
    "    current_tw['name']=f_path\n",
    "    current_tw['loss']=loss_avg\n",
    "    current_tw['index']=index_count\n",
    "    current_tw['nodeset']=node_set\n",
    "\n",
    "    added_que_flag=False\n",
    "    for hq in history_list:\n",
    "        for his_tw in hq:\n",
    "            if cal_set_rel(current_tw['nodeset'],his_tw['nodeset'],node_IDF_4_4_7, file_list)!=0 and current_tw['name']!=his_tw['name']:\n",
    "#                 print(\"history queue:\",his_tw['name'])\n",
    "                hq.append(copy.deepcopy(current_tw))\n",
    "                added_que_flag=True\n",
    "                break\n",
    "            if added_que_flag:\n",
    "                break\n",
    "    if added_que_flag is False:\n",
    "        temp_hq=[copy.deepcopy(current_tw)]\n",
    "        history_list.append(temp_hq)\n",
    "    index_count+=1\n",
    "    loss_list_4_10.append(loss_avg)\n",
    "    print( f_path,\"  \",loss_avg,\" count:\",count,\" percentage:\",count/len(edge_list),\" node count:\",len(node_set),\" edge count:\",len(edge_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_list=[]\n",
    "for hl in history_list:\n",
    "    loss_count=0\n",
    "    for hq in hl:\n",
    "        if loss_count==0:\n",
    "            loss_count=(loss_count+1)*(hq['loss']+1)\n",
    "        else:\n",
    "            loss_count=(loss_count)*(hq['loss']+1)\n",
    "#     name_list=[]\n",
    "    if loss_count>9:\n",
    "        name_list=[]\n",
    "        for i in hl:\n",
    "            name_list.append(i['name']) \n",
    "        print(name_list)\n",
    "        for i in name_list:\n",
    "            pred_label[i]=1\n",
    "        print(loss_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4-11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index_count: 0\n",
      "thr: 2.9659018460488697\n",
      "graph_4_11/2018-04-11 00:00:00.063000000~2018-04-11 02:00:00.161000000.txt    0.0  count: 0  percentage: 0.0  node count: 0  edge count: 0\n",
      "index_count: 1\n",
      "thr: 2.7465355863712353\n",
      "graph_4_11/2018-04-11 02:00:00.161000000~2018-04-11 03:45:00.251000000.txt    3.791358893114378  count: 35  percentage: 0.0341796875  node count: 7  edge count: 5\n",
      "index_count: 2\n",
      "thr: 2.908069439074869\n",
      "graph_4_11/2018-04-11 03:45:00.251000000~2018-04-11 05:30:00.544000000.txt    4.347204530669758  count: 36  percentage: 0.03515625  node count: 6  edge count: 4\n",
      "index_count: 3\n",
      "thr: 2.874940965561514\n",
      "graph_4_11/2018-04-11 05:30:00.544000000~2018-04-11 07:00:52.974000000.txt    4.243466263908712  count: 21  percentage: 0.0205078125  node count: 4  edge count: 2\n",
      "index_count: 4\n",
      "thr: 1.7660760315509239\n",
      "graph_4_11/2018-04-11 07:00:52.974000000~2018-04-11 07:25:16.408000000.txt    2.49229529783925  count: 354  percentage: 0.08642578125  node count: 59  edge count: 56\n",
      "index_count: 5\n",
      "thr: 2.3171783572744253\n",
      "graph_4_11/2018-04-11 07:25:16.408000000~2018-04-11 07:46:44.962000000.txt    3.0135318900240606  count: 139  percentage: 0.06787109375  node count: 44  edge count: 41\n",
      "index_count: 6\n",
      "thr: 2.262420493245486\n",
      "graph_4_11/2018-04-11 07:46:44.962000000~2018-04-11 08:05:53.555000000.txt    2.9315359420208598  count: 124  percentage: 0.12109375  node count: 44  edge count: 41\n",
      "index_count: 7\n",
      "thr: 2.409059356095989\n",
      "graph_4_11/2018-04-11 08:05:53.555000000~2018-04-11 08:21:10.246000000.txt    3.0548700192277582  count: 2404  percentage: 0.11179315476190477  node count: 131  edge count: 129\n",
      "index_count: 8\n",
      "thr: 2.3698886365267984\n",
      "graph_4_11/2018-04-11 08:21:10.246000000~2018-04-11 08:43:29.852000000.txt    2.958717158995642  count: 209  percentage: 0.10205078125  node count: 62  edge count: 60\n",
      "index_count: 9\n",
      "thr: 2.325188163947045\n",
      "graph_4_11/2018-04-11 08:43:29.852000000~2018-04-11 08:58:36.759000000.txt    3.8894392478556514  count: 793  percentage: 0.07744140625  node count: 123  edge count: 120\n",
      "index_count: 10\n",
      "thr: 2.4563153702649663\n",
      "graph_4_11/2018-04-11 08:58:36.759000000~2018-04-11 09:15:23.460000000.txt    3.4759261458975157  count: 8325  percentage: 0.10162353515625  node count: 887  edge count: 921\n",
      "index_count: 11\n",
      "thr: 2.054900983452307\n",
      "graph_4_11/2018-04-11 09:15:23.460000000~2018-04-11 09:30:40.459000000.txt    3.0886469128105247  count: 559  percentage: 0.09098307291666667  node count: 123  edge count: 134\n",
      "index_count: 12\n",
      "thr: 2.2258127899565525\n",
      "graph_4_11/2018-04-11 09:30:40.459000000~2018-04-11 09:45:41.202000000.txt    3.5249333245803363  count: 3911  percentage: 0.07794563137755102  node count: 673  edge count: 736\n",
      "index_count: 13\n",
      "thr: 1.9574062975344892\n",
      "graph_4_11/2018-04-11 09:45:41.202000000~2018-04-11 10:02:14.946000000.txt    3.66312135303265  count: 1663  percentage: 0.05413411458333333  node count: 241  edge count: 291\n",
      "index_count: 14\n",
      "thr: 2.5095097915382283\n",
      "graph_4_11/2018-04-11 10:02:14.946000000~2018-04-11 10:21:29.924000000.txt    4.137276823748895  count: 3113  percentage: 0.07238188244047619  node count: 231  edge count: 274\n",
      "index_count: 15\n",
      "thr: 1.6135581469174933\n",
      "graph_4_11/2018-04-11 10:21:29.924000000~2018-04-11 10:36:41.077000000.txt    2.9193670238138494  count: 9007  percentage: 0.052987339984939756  node count: 805  edge count: 905\n",
      "index_count: 16\n",
      "thr: 1.5842621645690997\n",
      "graph_4_11/2018-04-11 10:36:41.077000000~2018-04-11 10:53:51.436000000.txt    2.9321443511176355  count: 3865  percentage: 0.07121535966981132  node count: 431  edge count: 510\n",
      "index_count: 17\n",
      "thr: 2.4114460038875007\n",
      "graph_4_11/2018-04-11 10:53:51.436000000~2018-04-11 11:11:03.735000000.txt    3.2143688215119712  count: 3821  percentage: 0.116607666015625  node count: 294  edge count: 345\n",
      "index_count: 18\n",
      "thr: 2.709887323793813\n",
      "graph_4_11/2018-04-11 11:11:03.735000000~2018-04-11 11:27:06.742000000.txt    3.6904366385119283  count: 4909  percentage: 0.10895330255681818  node count: 465  edge count: 499\n",
      "index_count: 19\n",
      "thr: 1.9019997482543558\n",
      "graph_4_11/2018-04-11 11:27:06.742000000~2018-04-11 11:42:13.848000000.txt    3.162182495776053  count: 2277  percentage: 0.08235677083333333  node count: 409  edge count: 446\n",
      "index_count: 20\n",
      "thr: 2.2994461582392267\n",
      "graph_4_11/2018-04-11 11:42:13.848000000~2018-04-11 11:59:50.272000000.txt    3.4007393647752844  count: 4985  percentage: 0.09545419730392157  node count: 500  edge count: 616\n",
      "index_count: 21\n",
      "thr: 2.7548178126619236\n",
      "graph_4_11/2018-04-11 11:59:50.272000000~2018-04-11 12:15:10.640000000.txt    3.99556624129703  count: 2639  percentage: 0.09912109375  node count: 298  edge count: 311\n",
      "index_count: 22\n",
      "thr: 3.0772109475712925\n",
      "graph_4_11/2018-04-11 12:15:10.640000000~2018-04-11 12:30:15.769000000.txt    3.8044311358890717  count: 214  percentage: 0.1044921875  node count: 43  edge count: 44\n",
      "index_count: 23\n",
      "thr: 4.953419232637098\n",
      "graph_4_11/2018-04-11 12:30:15.769000000~2018-04-11 12:45:30.888000000.txt    6.3560723837427515  count: 7694  percentage: 0.14449368990384615  node count: 49  edge count: 48\n",
      "index_count: 24\n",
      "thr: 2.970062598715308\n",
      "graph_4_11/2018-04-11 12:45:30.888000000~2018-04-11 13:01:09.797000000.txt    4.191067133384475  count: 2487  percentage: 0.057826450892857144  node count: 266  edge count: 305\n",
      "index_count: 25\n",
      "thr: 3.826441820770276\n",
      "graph_4_11/2018-04-11 13:01:09.797000000~2018-04-11 13:16:29.838000000.txt    4.715615451808681  count: 3007  percentage: 0.10876012731481481  node count: 503  edge count: 530\n",
      "index_count: 26\n",
      "thr: 4.206400510046031\n",
      "graph_4_11/2018-04-11 13:16:29.838000000~2018-04-11 13:31:30.828000000.txt    5.194131418778668  count: 3554  percentage: 0.10207950367647059  node count: 623  edge count: 639\n",
      "index_count: 27\n",
      "thr: 4.14052908212059\n",
      "graph_4_11/2018-04-11 13:31:30.828000000~2018-04-11 13:46:38.658000000.txt    5.025429070073623  count: 3736  percentage: 0.10424107142857143  node count: 444  edge count: 462\n",
      "index_count: 28\n",
      "thr: 4.074353110595942\n",
      "graph_4_11/2018-04-11 13:46:38.658000000~2018-04-11 14:02:21.103000000.txt    5.694624050850828  count: 4950  percentage: 0.08951822916666667  node count: 526  edge count: 545\n",
      "index_count: 29\n",
      "thr: 3.8935982863576193\n",
      "graph_4_11/2018-04-11 14:02:21.103000000~2018-04-11 14:18:19.001000000.txt    4.879340719595452  count: 3755  percentage: 0.0916748046875  node count: 498  edge count: 512\n",
      "index_count: 30\n",
      "thr: 3.815626323447663\n",
      "graph_4_11/2018-04-11 14:18:19.001000000~2018-04-11 14:33:38.600000000.txt    4.863578454298355  count: 4181  percentage: 0.09958555640243902  node count: 620  edge count: 638\n",
      "index_count: 31\n",
      "thr: 2.325076241689729\n",
      "graph_4_11/2018-04-11 14:33:38.600000000~2018-04-11 14:49:05.326000000.txt    3.5563680678919205  count: 3027  percentage: 0.08211263020833333  node count: 453  edge count: 496\n",
      "index_count: 32\n",
      "thr: 3.479550799271503\n",
      "graph_4_11/2018-04-11 14:49:05.326000000~2018-04-11 15:04:48.749000000.txt    5.02531223251333  count: 2904  percentage: 0.08102678571428572  node count: 415  edge count: 432\n",
      "index_count: 33\n",
      "thr: 3.9256480399675358\n",
      "graph_4_11/2018-04-11 15:04:48.749000000~2018-04-11 15:23:04.703000000.txt    5.010343615237716  count: 1184  percentage: 0.10511363636363637  node count: 353  edge count: 358\n",
      "index_count: 34\n",
      "thr: 4.484382082445231\n",
      "graph_4_11/2018-04-11 15:23:04.703000000~2018-04-11 15:39:01.167000000.txt    5.681077513418122  count: 1208  percentage: 0.05617559523809524  node count: 268  edge count: 267\n",
      "index_count: 35\n",
      "thr: 3.9525994070702035\n",
      "graph_4_11/2018-04-11 15:39:01.167000000~2018-04-11 15:54:01.828000000.txt    4.94436420538096  count: 2951  percentage: 0.10292271205357142  node count: 424  edge count: 439\n",
      "index_count: 36\n",
      "thr: 4.162175825225571\n",
      "graph_4_11/2018-04-11 15:54:01.828000000~2018-04-11 16:09:02.909000000.txt    5.122246182277585  count: 1683  percentage: 0.09130859375  node count: 411  edge count: 416\n",
      "index_count: 37\n",
      "thr: 3.9966585607554457\n",
      "graph_4_11/2018-04-11 16:09:02.909000000~2018-04-11 16:24:25.756000000.txt    5.013812961622587  count: 2941  percentage: 0.10257393973214286  node count: 389  edge count: 401\n",
      "index_count: 38\n",
      "thr: 3.807758004967754\n",
      "graph_4_11/2018-04-11 16:24:25.756000000~2018-04-11 16:44:13.639000000.txt    4.802000092129571  count: 3084  percentage: 0.10756138392857142  node count: 400  edge count: 435\n",
      "index_count: 39\n",
      "thr: 3.2889430257401653\n",
      "graph_4_11/2018-04-11 16:44:13.639000000~2018-04-11 16:59:14.685000000.txt    4.253143913744236  count: 2871  percentage: 0.10013253348214286  node count: 470  edge count: 500\n",
      "index_count: 40\n",
      "thr: 3.3077155922245067\n",
      "graph_4_11/2018-04-11 16:59:14.685000000~2018-04-11 17:14:26.172000000.txt    4.305853628341629  count: 3125  percentage: 0.09844380040322581  node count: 522  edge count: 546\n",
      "index_count: 41\n",
      "thr: 3.9678594504392724\n",
      "graph_4_11/2018-04-11 17:14:26.172000000~2018-04-11 17:29:29.020000000.txt    4.934922241547309  count: 3316  percentage: 0.10794270833333333  node count: 457  edge count: 471\n",
      "index_count: 42\n",
      "thr: 3.9923601694157758\n",
      "graph_4_11/2018-04-11 17:29:29.020000000~2018-04-11 17:44:48.533000000.txt    5.0208027277204055  count: 3163  percentage: 0.10296223958333334  node count: 454  edge count: 470\n",
      "index_count: 43\n",
      "thr: 4.05990247452711\n",
      "graph_4_11/2018-04-11 17:44:48.533000000~2018-04-11 17:59:53.338000000.txt    5.087689842548907  count: 3417  percentage: 0.10764238911290322  node count: 426  edge count: 442\n",
      "index_count: 44\n",
      "thr: 1.725525142854083\n",
      "graph_4_11/2018-04-11 17:59:53.338000000~2018-04-11 18:16:39.616000000.txt    2.992010746274205  count: 5948  percentage: 0.07956977739726027  node count: 651  edge count: 861\n",
      "index_count: 45\n",
      "thr: 4.097658958744777\n",
      "graph_4_11/2018-04-11 18:16:39.616000000~2018-04-11 18:32:06.970000000.txt    5.1391919761953515  count: 3497  percentage: 0.106719970703125  node count: 395  edge count: 408\n",
      "index_count: 46\n",
      "thr: 3.9866294064957732\n",
      "graph_4_11/2018-04-11 18:32:06.970000000~2018-04-11 18:47:24.055000000.txt    5.054695944369544  count: 2936  percentage: 0.10619212962962964  node count: 403  edge count: 409\n",
      "index_count: 47\n",
      "thr: 3.6620916138150053\n",
      "graph_4_11/2018-04-11 18:47:24.055000000~2018-04-11 19:02:37.294000000.txt    4.525230966807852  count: 3815  percentage: 0.1064453125  node count: 357  edge count: 387\n",
      "index_count: 48\n",
      "thr: 2.906480393042164\n",
      "graph_4_11/2018-04-11 19:02:37.294000000~2018-04-11 19:18:03.851000000.txt    4.077974063073392  count: 4486  percentage: 0.1043061755952381  node count: 477  edge count: 528\n",
      "index_count: 49\n",
      "thr: 3.885912334090122\n",
      "graph_4_11/2018-04-11 19:18:03.851000000~2018-04-11 19:33:17.804000000.txt    4.886190000009451  count: 3522  percentage: 0.10748291015625  node count: 440  edge count: 458\n",
      "index_count: 50\n",
      "thr: 3.8024798027100077\n",
      "graph_4_11/2018-04-11 19:33:17.804000000~2018-04-11 19:48:43.675000000.txt    4.912452172983923  count: 3104  percentage: 0.10104166666666667  node count: 438  edge count: 459\n",
      "index_count: 51\n",
      "thr: 1.611357834089824\n",
      "graph_4_11/2018-04-11 19:48:43.675000000~2018-04-11 20:04:35.343000000.txt    2.862694763150998  count: 5867  percentage: 0.07742557010135136  node count: 638  edge count: 915\n",
      "index_count: 52\n",
      "thr: 4.074342090087187\n",
      "graph_4_11/2018-04-11 20:04:35.343000000~2018-04-11 20:27:04.183000000.txt    5.19974099413912  count: 1624  percentage: 0.11328125  node count: 435  edge count: 445\n",
      "index_count: 53\n",
      "thr: 3.619089572702234\n",
      "graph_4_11/2018-04-11 20:27:04.183000000~2018-04-11 20:46:59.749000000.txt    4.490586591432226  count: 3351  percentage: 0.1055632560483871  node count: 609  edge count: 636\n",
      "index_count: 54\n",
      "thr: 3.9004251294561696\n",
      "graph_4_11/2018-04-11 20:46:59.749000000~2018-04-11 21:04:21.264000000.txt    4.691817548627459  count: 1482  percentage: 0.08040364583333333  node count: 317  edge count: 322\n",
      "index_count: 55\n",
      "thr: 3.7628631667042116\n",
      "graph_4_11/2018-04-11 21:04:21.264000000~2018-04-11 21:29:14.176000000.txt    4.632479660196893  count: 1670  percentage: 0.1019287109375  node count: 381  edge count: 391\n",
      "index_count: 56\n",
      "thr: 2.3993640267288816\n",
      "graph_4_11/2018-04-11 21:29:14.176000000~2018-04-11 21:44:15.839000000.txt    3.548167277919841  count: 4370  percentage: 0.10160900297619048  node count: 584  edge count: 704\n",
      "index_count: 57\n",
      "thr: 3.741908899005891\n",
      "graph_4_11/2018-04-11 21:44:15.839000000~2018-04-11 21:59:40.869000000.txt    4.741057378631182  count: 2919  percentage: 0.10963792067307693  node count: 548  edge count: 567\n",
      "index_count: 58\n",
      "thr: 1.9905665330513236\n",
      "graph_4_11/2018-04-11 21:59:40.869000000~2018-04-11 22:18:09.134000000.txt    3.428100561022535  count: 4218  percentage: 0.0823828125  node count: 527  edge count: 604\n",
      "index_count: 59\n",
      "thr: 3.985408066512577\n",
      "graph_4_11/2018-04-11 22:18:09.134000000~2018-04-11 22:33:22.263000000.txt    5.075934544219871  count: 3774  percentage: 0.10530133928571428  node count: 380  edge count: 395\n",
      "index_count: 60\n",
      "thr: 3.3667287330710587\n",
      "graph_4_11/2018-04-11 22:33:22.263000000~2018-04-11 22:49:00.957000000.txt    4.214001438255488  count: 2105  percentage: 0.102783203125  node count: 348  edge count: 394\n",
      "index_count: 61\n",
      "thr: 4.0547624868140435\n",
      "graph_4_11/2018-04-11 22:49:00.957000000~2018-04-11 23:46:00.517000000.txt    5.287220734181544  count: 84  percentage: 0.08203125  node count: 19  edge count: 16\n"
     ]
    }
   ],
   "source": [
    "# node_IDF=torch.load(\"node_IDF_4_11\")\n",
    "# node_IDF_4_4_7=torch.load(\"node_IDF_4_4-7\")\n",
    "node_IDF_4_4_7=torch.load(\"node_IDF\")\n",
    "y_data_4_11=[]\n",
    "df_list_4_11=[]\n",
    "# node_set_list=[]\n",
    "history_list=[]\n",
    "tw_que=[]\n",
    "his_tw={}\n",
    "current_tw={}\n",
    "\n",
    "loss_list_4_11=[]\n",
    "\n",
    "file_path_list=[]\n",
    "\n",
    "\n",
    "file_path=\"graph_4_11/\"\n",
    "file_l=os.listdir(\"graph_4_11/\")\n",
    "for i in file_l:\n",
    "    file_path_list.append(file_path+i)\n",
    "\n",
    "index_count=0\n",
    "for f_path in sorted(file_path_list):\n",
    "    f=open(f_path)\n",
    "    edge_loss_list=[]\n",
    "    edge_list=[]\n",
    "    print('index_count:',index_count)\n",
    "    \n",
    "    for line in f:\n",
    "        l=line.strip()\n",
    "        jdata=eval(l)\n",
    "        edge_loss_list.append(jdata['loss'])\n",
    "        edge_list.append([str(jdata['srcmsg']),str(jdata['dstmsg'])])\n",
    "    df_list_4_11.append(pd.DataFrame(edge_loss_list))\n",
    "    count,loss_avg,node_set,edge_set=cal_anomaly_loss(edge_loss_list,edge_list,\"graph_4_11/\")\n",
    "\n",
    "    current_tw['name']=f_path\n",
    "    current_tw['loss']=loss_avg\n",
    "    current_tw['index']=index_count\n",
    "    current_tw['nodeset']=node_set\n",
    "\n",
    "    added_que_flag=False\n",
    "    for hq in history_list:\n",
    "        for his_tw in hq:\n",
    "            if cal_set_rel(current_tw['nodeset'],his_tw['nodeset'],node_IDF_4_4_7, file_list)!=0 and current_tw['name']!=his_tw['name']:\n",
    "#                 print(\"history queue:\",his_tw['name'])\n",
    "                hq.append(copy.deepcopy(current_tw))\n",
    "                added_que_flag=True\n",
    "                break\n",
    "            if added_que_flag:\n",
    "                break\n",
    "    if added_que_flag is False:\n",
    "        temp_hq=[copy.deepcopy(current_tw)]\n",
    "        history_list.append(temp_hq)\n",
    "    index_count+=1\n",
    "    loss_list_4_11.append(loss_avg)\n",
    "    print( f_path,\"  \",loss_avg,\" count:\",count,\" percentage:\",count/len(edge_list),\" node count:\",len(node_set),\" edge count:\",len(edge_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_list=[]\n",
    "for hl in history_list:\n",
    "    loss_count=0\n",
    "    for hq in hl:\n",
    "        if loss_count==0:\n",
    "            loss_count=(loss_count+1)*(hq['loss']+1)\n",
    "        else:\n",
    "            loss_count=(loss_count)*(hq['loss']+1)\n",
    "#     name_list=[]\n",
    "    if loss_count>9:\n",
    "        name_list=[]\n",
    "        for i in hl:\n",
    "            name_list.append(i['name']) \n",
    "        print(name_list)\n",
    "        for i in name_list:\n",
    "            pred_label[i]=1\n",
    "        print(loss_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import average_precision_score, roc_auc_score\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import torch\n",
    "from sklearn import preprocessing\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def plot_thr():\n",
    "    np.seterr(invalid='ignore')\n",
    "    step=0.01\n",
    "    thr_list=torch.arange(-5,5,step)\n",
    "    \n",
    "    \n",
    "\n",
    "    precision_list=[]\n",
    "    recall_list=[]\n",
    "    fscore_list=[]\n",
    "    accuracy_list=[]\n",
    "    auc_val_list=[]\n",
    "    for thr in thr_list:\n",
    "        threshold=thr\n",
    "        y_prediction=[]\n",
    "        for i in y_test_scores:\n",
    "            if i >threshold:\n",
    "                y_prediction.append(1)\n",
    "            else:\n",
    "                y_prediction.append(0)\n",
    "        precision,recall,fscore,accuracy,auc_val=classifier_evaluation(y_test, y_prediction)   \n",
    "        precision_list.append(float(precision))\n",
    "        recall_list.append(float(recall))\n",
    "        fscore_list.append(float(fscore))\n",
    "        accuracy_list.append(float(accuracy))\n",
    "        auc_val_list.append(float(auc_val))\n",
    "\n",
    "    max_fscore=max(fscore_list)\n",
    "    max_fscore_index=fscore_list.index(max_fscore)\n",
    "    print(max_fscore_index)\n",
    "    print(\"max threshold:\",thr_list[max_fscore_index])\n",
    "    print('precision:',precision_list[max_fscore_index])\n",
    "    print('recall:',recall_list[max_fscore_index])\n",
    "    print('fscore:',fscore_list[max_fscore_index])\n",
    "    print('accuracy:',accuracy_list[max_fscore_index])    \n",
    "    print('auc:',auc_val_list[max_fscore_index])\n",
    "    \n",
    "        \n",
    "     # list tensor\n",
    "#     precision_list=torch.tensor(precision_list)   \n",
    "#     recall_list=torch.tensor(recall_list)   \n",
    "#     fscore_list=torch.tensor(fscore_list)   \n",
    "#     accuracy_list=torch.tensor(accuracy_list)   \n",
    "#     auc_val_list=torch.tensor(auc_val_list)   \n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "    # plt.scatter(attack_x, attack_y, s=20, c='r', label='Attack graph',marker='*')\n",
    "    # plt.scatter(bengin_x, bengin_y, s=20, c='g', label='Bengin graph',marker='1')\n",
    "    # plt.scatter(bengin_x, bengin_y, s=20, c='g', label='Bengin graph',marker='1')\n",
    "\n",
    "    plt.plot(thr_list,precision_list,color='red',label='precision',linewidth=2.0,linestyle='-')\n",
    "    plt.plot(thr_list,recall_list,color='orange',label='recall',linewidth=2.0,linestyle='solid')\n",
    "    plt.plot(thr_list,fscore_list,color='y',label='F-score',linewidth=2.0,linestyle='dashed')\n",
    "    plt.plot(thr_list,accuracy_list,color='g',label='accuracy',linewidth=2.0,linestyle='dashdot')\n",
    "    plt.plot(thr_list,auc_val_list,color='b',label='auc_val',linewidth=2.0,linestyle='dotted')\n",
    "    # '-', '--', '-.', ':', 'None', ' ', '', 'solid', 'dashed', 'dashdot', 'dotted'\n",
    "\n",
    "\n",
    "    # plt.scatter(turnovers, graph_loss, c=color)\n",
    "    plt.xlabel(\"Threshold\", fontdict={'size': 16})\n",
    "    plt.ylabel(\"Rate\", fontdict={'size': 16})\n",
    "    plt.title(\"Different evaluation Indicators by varying threshold value\", fontdict={'size': 12})\n",
    "    plt.legend(loc='best', fontsize=12, markerscale=0.5)\n",
    "    plt.show()\n",
    "\n",
    "def classifier_evaluation(y_test, y_test_pred):\n",
    "    # groundtruth, pred_value\n",
    "    tn, fp, fn, tp =confusion_matrix(y_test, y_test_pred).ravel()\n",
    "#     tn+=100\n",
    "#     print(clf_name,\" : \")\n",
    "    print('tn:',tn)\n",
    "    print('fp:',fp)\n",
    "    print('fn:',fn)\n",
    "    print('tp:',tp)\n",
    "    precision=tp/(tp+fp)\n",
    "    recall=tp/(tp+fn)\n",
    "    accuracy=(tp+tn)/(tp+tn+fp+fn)\n",
    "    fscore=2*(precision*recall)/(precision+recall)    \n",
    "    auc_val=roc_auc_score(y_test, y_test_pred)\n",
    "    print(\"precision:\",precision)\n",
    "    print(\"recall:\",recall)\n",
    "    print(\"fscore:\",fscore)\n",
    "    print(\"accuracy:\",accuracy)\n",
    "    print(\"auc_val:\",auc_val)\n",
    "    return precision,recall,fscore,accuracy,auc_val\n",
    "\n",
    "def minmax(data):\n",
    "    min_val=min(data)\n",
    "    max_val=max(data)\n",
    "    ans=[]\n",
    "    for i in data:\n",
    "        ans.append((i-min_val)/(max_val-min_val))\n",
    "    return ans\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=[]\n",
    "y_pred=[]\n",
    "for i in labels:\n",
    "    y.append(labels[i])\n",
    "    y_pred.append(pred_label[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tn: 114\n",
      "fp: 0\n",
      "fn: 5\n",
      "tp: 0\n",
      "precision: nan\n",
      "recall: 0.0\n",
      "fscore: nan\n",
      "accuracy: 0.957983193277311\n",
      "auc_val: 0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_761422/3255821976.py:88: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  precision=tp/(tp+fp)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(nan, 0.0, nan, 0.957983193277311, 0.5)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier_evaluation(y,y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Count attack edge numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keyword_hit(line):\n",
    "    attack_nodes=[\n",
    "            'shared_files',\n",
    "        'csb.tracee.27331.27355',\n",
    "        'netrecon',\n",
    "#         '/data/data/org.mozilla.fennec_firefox_dev/',\n",
    "     \n",
    "#             'firefox',\n",
    "        '153.178.46.202',\n",
    "       '111.82.111.27',\n",
    "        '166.199.230.185',\n",
    "        '140.57.183.17',\n",
    "      \n",
    "        \n",
    "        ]\n",
    "    flag=False\n",
    "    for i in attack_nodes:\n",
    "        if i in line:\n",
    "            flag=True\n",
    "            break\n",
    "    return flag\n",
    "\n",
    "\n",
    "\n",
    "files=[\n",
    "    \n",
    "        'graph_4_11/2018-04-11 13:46:38.658000000~2018-04-11 14:02:21.103000000.txt',\n",
    "    'graph_4_11/2018-04-11 14:02:21.103000000~2018-04-11 14:18:19.001000000.txt',\n",
    "    'graph_4_11/2018-04-11 14:18:19.001000000~2018-04-11 14:33:38.600000000.txt',\n",
    "    'graph_4_11/2018-04-11 14:33:38.600000000~2018-04-11 14:49:05.326000000.txt',\n",
    "    'graph_4_11/2018-04-11 14:49:05.326000000~2018-04-11 15:04:48.749000000.txt',\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:00<00:00, 26.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "647\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "attack_edge_count=0\n",
    "for fpath in tqdm(files):\n",
    "    f=open(fpath)\n",
    "    for line in f:\n",
    "        if keyword_hit(line):\n",
    "            attack_edge_count+=1\n",
    "print(attack_edge_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 1/5 [00:01<00:04,  1.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.6285169453354447\n",
      "1.6305574435069978\n",
      "thr: 4.074353110595942\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 2/5 [00:02<00:02,  1.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.6952151654125829\n",
      "1.4655887472966909\n",
      "thr: 3.8935982863576193\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 3/5 [00:02<00:01,  1.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.6349986380277541\n",
      "1.4537517902799393\n",
      "thr: 3.815626323447663\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 4/5 [00:03<00:00,  1.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9074441219848823\n",
      "0.9450880798032311\n",
      "thr: 2.325076241689729\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:04<00:00,  1.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.340633728827119\n",
      "1.425944713629589\n",
      "thr: 3.479550799271503\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from graphviz import Digraph\n",
    "import networkx as nx\n",
    "import datetime\n",
    "import community.community_louvain as community_louvain\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "\n",
    "# Some common path abstraction for visualization\n",
    "replace_dic={\n",
    " '/data/data/org.mozilla.fennec_firefox_dev/cache/':'/data/data/org.mozilla.fennec_firefox_dev/cache/*',\n",
    "     '/data/data/org.mozilla.fennec_firefox_dev/files/':'/data/data/org.mozilla.fennec_firefox_dev/files/*',\n",
    "    '/system/fonts/':'/system/fonts/*',\n",
    "    '/data/data/com.android.email/cache/':'/data/data/com.android.email/cache/*',\n",
    "    '/data/data/com.android.email/files/':'/data/data/com.android.email/files/*',\n",
    "    'UNNAMED':'UNNAMED:*',\n",
    "    \n",
    "}\n",
    "\n",
    "\n",
    "def replace_path_name(path_name):\n",
    "    for i in replace_dic:\n",
    "        if i in path_name:\n",
    "            return replace_dic[i]\n",
    "    return path_name\n",
    "\n",
    "\n",
    "# Users should manually put the detected anomalous time windows here\n",
    "attack_list = [\n",
    "        'graph_4_11/2018-04-11 13:46:38.658000000~2018-04-11 14:02:21.103000000.txt',\n",
    "    'graph_4_11/2018-04-11 14:02:21.103000000~2018-04-11 14:18:19.001000000.txt',\n",
    "    'graph_4_11/2018-04-11 14:18:19.001000000~2018-04-11 14:33:38.600000000.txt',\n",
    "    'graph_4_11/2018-04-11 14:33:38.600000000~2018-04-11 14:49:05.326000000.txt',\n",
    "    'graph_4_11/2018-04-11 14:49:05.326000000~2018-04-11 15:04:48.749000000.txt',\n",
    "]\n",
    "\n",
    "original_edges_count = 0\n",
    "graphs = []\n",
    "gg = nx.DiGraph()\n",
    "count = 0\n",
    "for path in tqdm(attack_list):\n",
    "    if \".txt\" in path:\n",
    "        line_count = 0\n",
    "        node_set = set()\n",
    "        tempg = nx.DiGraph()\n",
    "        f = open(path, \"r\")\n",
    "        edge_list = []\n",
    "        for line in f:\n",
    "            count += 1\n",
    "            l = line.strip()\n",
    "            jdata = eval(l)\n",
    "            edge_list.append(jdata)\n",
    "\n",
    "        edge_list = sorted(edge_list, key=lambda x: x['loss'], reverse=True)\n",
    "        original_edges_count += len(edge_list)\n",
    "\n",
    "        loss_list = []\n",
    "        for i in edge_list:\n",
    "            loss_list.append(i['loss'])\n",
    "        loss_mean = mean(loss_list)\n",
    "        loss_std = std(loss_list)\n",
    "        print(loss_mean)\n",
    "        print(loss_std)\n",
    "        thr = loss_mean + 1.5 * loss_std\n",
    "        print(\"thr:\", thr)\n",
    "        for e in edge_list:\n",
    "            if e['loss'] > thr:\n",
    "                tempg.add_edge(str(hashgen(replace_path_name(e['srcmsg']))),\n",
    "                               str(hashgen(replace_path_name(e['dstmsg']))))\n",
    "                gg.add_edge(str(hashgen(replace_path_name(e['srcmsg']))), str(hashgen(replace_path_name(e['dstmsg']))),\n",
    "                            loss=e['loss'], srcmsg=e['srcmsg'], dstmsg=e['dstmsg'], edge_type=e['edge_type'],\n",
    "                            time=e['time'])\n",
    "\n",
    "\n",
    "partition = community_louvain.best_partition(gg.to_undirected())\n",
    "\n",
    "# Generate the candidate subgraphs based on community discovery results\n",
    "communities = {}\n",
    "max_partition = 0\n",
    "for i in partition:\n",
    "    if partition[i] > max_partition:\n",
    "        max_partition = partition[i]\n",
    "for i in range(max_partition + 1):\n",
    "    communities[i] = nx.DiGraph()\n",
    "for e in gg.edges:\n",
    "    communities[partition[e[0]]].add_edge(e[0], e[1])\n",
    "    communities[partition[e[1]]].add_edge(e[0], e[1])\n",
    "\n",
    "\n",
    "# Define the attack nodes. They are **only be used to plot the colors of attack nodes and edges**.\n",
    "# They won't change the detection results.\n",
    "# Didn't add too much nodes for coloring. Most of the results are compared with the ground truth documentations manually\n",
    "def attack_edge_flag(msg):\n",
    "    attack_nodes = [\n",
    "        '/data/data/org.mozilla.fennec_firefox_dev/',\n",
    "        '/data/data/org.mozilla.fennec_firefox_dev/shared_files',\n",
    "        '/data/local/tmp',\n",
    "        'csb.tracee.27331.27355',\n",
    "        '/data/data/org.mozilla.fennec_firefox_dev/csb.tracee.27331.27355',\n",
    "        '111.82.111.27',\n",
    "        '166.199.230.185',\n",
    "        'glx_alsa_675',\n",
    "    ]\n",
    "    flag = False\n",
    "    for i in attack_nodes:\n",
    "        if i in str(msg):\n",
    "            flag = True\n",
    "    return flag\n",
    "\n",
    "\n",
    "# Plot and render candidate subgraph\n",
    "os.system(f\"mkdir -p ./graph_visual/\")\n",
    "graph_index = 0\n",
    "for c in communities:\n",
    "    dot = Digraph(name=\"MyPicture\", comment=\"the test\", format=\"pdf\")\n",
    "    dot.graph_attr['rankdir'] = 'LR'\n",
    "\n",
    "    for e in communities[c].edges:\n",
    "        try:\n",
    "            temp_edge = gg.edges[e]\n",
    "            srcnode = e['srcnode']\n",
    "            dstnode = e['dstnode']\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        if True:\n",
    "            # source node\n",
    "            if \"'subject': '\" in temp_edge['srcmsg']:\n",
    "                src_shape = 'box'\n",
    "            elif \"'file': '\" in temp_edge['srcmsg']:\n",
    "                src_shape = 'oval'\n",
    "            elif \"'netflow': '\" in temp_edge['srcmsg']:\n",
    "                src_shape = 'diamond'\n",
    "            if attack_edge_flag(temp_edge['srcmsg']):\n",
    "                src_node_color = 'red'\n",
    "            else:\n",
    "                src_node_color = 'blue'\n",
    "            dot.node(name=str(hashgen(replace_path_name(temp_edge['srcmsg']))), label=str(\n",
    "                replace_path_name(temp_edge['srcmsg']) + str(\n",
    "                    partition[str(hashgen(replace_path_name(temp_edge['srcmsg'])))])), color=src_node_color,\n",
    "                     shape=src_shape)\n",
    "\n",
    "            # destination node\n",
    "            if \"'subject': '\" in temp_edge['dstmsg']:\n",
    "                dst_shape = 'box'\n",
    "            elif \"'file': '\" in temp_edge['dstmsg']:\n",
    "                dst_shape = 'oval'\n",
    "            elif \"'netflow': '\" in temp_edge['dstmsg']:\n",
    "                dst_shape = 'diamond'\n",
    "            if attack_edge_flag(temp_edge['dstmsg']):\n",
    "                dst_node_color = 'red'\n",
    "            else:\n",
    "                dst_node_color = 'blue'\n",
    "            dot.node(name=str(hashgen(replace_path_name(temp_edge['dstmsg']))), label=str(\n",
    "                replace_path_name(temp_edge['dstmsg']) + str(\n",
    "                    partition[str(hashgen(replace_path_name(temp_edge['dstmsg'])))])), color=dst_node_color,\n",
    "                     shape=dst_shape)\n",
    "\n",
    "            if attack_edge_flag(temp_edge['srcmsg']) and attack_edge_flag(temp_edge['dstmsg']):\n",
    "                edge_color = 'red'\n",
    "            else:\n",
    "                edge_color = 'blue'\n",
    "            dot.edge(str(hashgen(replace_path_name(temp_edge['srcmsg']))),\n",
    "                     str(hashgen(replace_path_name(temp_edge['dstmsg']))), label=temp_edge['edge_type'],\n",
    "                     color=edge_color)\n",
    "\n",
    "    dot.render(f'./graph_visual/subgraph_' + str(graph_index), view=False)\n",
    "    graph_index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#END"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

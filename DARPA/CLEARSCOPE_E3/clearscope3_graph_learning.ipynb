{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoding=utf-8\n",
    "import os.path as osp\n",
    "import os\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch.nn import Linear\n",
    "from sklearn.metrics import average_precision_score, roc_auc_score\n",
    "from torch_geometric.data import TemporalData\n",
    "from torch_geometric.datasets import JODIEDataset\n",
    "from torch_geometric.datasets import ICEWS18\n",
    "from torch_geometric.nn import TGNMemory, TransformerConv\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn.models.tgn import (LastNeighborLoader, IdentityMessage, MeanAggregator,\n",
    "                                           LastAggregator)\n",
    "from torch_geometric import *\n",
    "from torch_geometric.utils import negative_sampling\n",
    "from tqdm import tqdm\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import math\n",
    "import copy\n",
    "import re\n",
    "import time\n",
    "import json\n",
    "import pandas as pd\n",
    "from random import choice\n",
    "import gc\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# device = 'cpu'\n",
    "# msg structure:      [src_node_feature,edge_attr,dst_node_feature]\n",
    "\n",
    "# compute the best partition\n",
    "import datetime\n",
    "# import community as community_louvain\n",
    "\n",
    "import xxhash\n",
    "# Find the edge index which the edge vector is corresponding to\n",
    "def tensor_find(t,x):\n",
    "    t_np=t.cpu().numpy()\n",
    "    idx=np.argwhere(t_np==x)\n",
    "    return idx[0][0]+1\n",
    "\n",
    "\n",
    "def std(t):\n",
    "    t = np.array(t)\n",
    "    return np.std(t)\n",
    "\n",
    "\n",
    "def var(t):\n",
    "    t = np.array(t)\n",
    "    return np.var(t)\n",
    "\n",
    "\n",
    "def mean(t):\n",
    "    t = np.array(t)\n",
    "    return np.mean(t)\n",
    "\n",
    "def hashgen(l):\n",
    "    \"\"\"Generate a single hash value from a list. @l is a list of\n",
    "    string values, which can be properties of a node/edge. This\n",
    "    function returns a single hashed integer value.\"\"\"\n",
    "    hasher = xxhash.xxh64()\n",
    "    for e in l:\n",
    "        hasher.update(e)\n",
    "    return hasher.intdigest()\n",
    "\n",
    "\n",
    "def cal_pos_edges_loss(link_pred_ratio):\n",
    "    loss=[]\n",
    "    for i in link_pred_ratio:\n",
    "        loss.append(criterion(i,torch.ones(1)))\n",
    "    return torch.tensor(loss)\n",
    "\n",
    "def cal_pos_edges_loss_multiclass(link_pred_ratio,labels):\n",
    "    loss=[] \n",
    "    for i in range(len(link_pred_ratio)):\n",
    "        loss.append(criterion(link_pred_ratio[i].reshape(1,-1),labels[i].reshape(-1)))\n",
    "    return torch.tensor(loss)\n",
    "\n",
    "def cal_pos_edges_loss_autoencoder(decoded,msg):\n",
    "    loss=[] \n",
    "    for i in range(len(decoded)):\n",
    "        loss.append(criterion(decoded[i].reshape(1,-1),msg[i].reshape(-1)))\n",
    "    return torch.tensor(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": "IPython.notebook.set_autosave_interval(120000)"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autosaving every 120 seconds\n"
     ]
    }
   ],
   "source": [
    "%autosave 120  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timezone\n",
    "import time\n",
    "import pytz\n",
    "from time import mktime\n",
    "from datetime import datetime\n",
    "import time\n",
    "def ns_time_to_datetime(ns):\n",
    "    \"\"\"\n",
    "    :param ns: int nano timestamp\n",
    "    :return: datetime   format: 2013-10-10 23:40:00.000000000\n",
    "    \"\"\"\n",
    "    dt = datetime.fromtimestamp(int(ns) // 1000000000)\n",
    "    s = dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "    s += '.' + str(int(int(ns) % 1000000000)).zfill(9)\n",
    "    return s\n",
    "\n",
    "def ns_time_to_datetime_US(ns):\n",
    "    \"\"\"\n",
    "    :param ns: int nano timestamp\n",
    "    :return: datetime   format: 2013-10-10 23:40:00.000000000\n",
    "    \"\"\"\n",
    "    tz = pytz.timezone('US/Eastern')\n",
    "    dt = pytz.datetime.datetime.fromtimestamp(int(ns) // 1000000000, tz)\n",
    "    s = dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "    s += '.' + str(int(int(ns) % 1000000000)).zfill(9)\n",
    "    return s\n",
    "\n",
    "def time_to_datetime_US(s):\n",
    "    \"\"\"\n",
    "    :param ns: int nano timestamp\n",
    "    :return: datetime   format: 2013-10-10 23:40:00\n",
    "    \"\"\"\n",
    "    tz = pytz.timezone('US/Eastern')\n",
    "    dt = pytz.datetime.datetime.fromtimestamp(int(s), tz)\n",
    "    s = dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "    return s\n",
    "\n",
    "def datetime_to_ns_time(date):\n",
    "    \"\"\"\n",
    "    :param date: str   format: %Y-%m-%d %H:%M:%S   e.g. 2013-10-10 23:40:00\n",
    "    :return: nano timestamp\n",
    "    \"\"\"\n",
    "    timeArray = time.strptime(date, \"%Y-%m-%d %H:%M:%S\")\n",
    "    timeStamp = int(time.mktime(timeArray))\n",
    "    timeStamp = timeStamp * 1000000000\n",
    "    return timeStamp\n",
    "\n",
    "def datetime_to_ns_time_US(date):\n",
    "    \"\"\"\n",
    "    :param date: str   format: %Y-%m-%d %H:%M:%S   e.g. 2013-10-10 23:40:00\n",
    "    :return: nano timestamp\n",
    "    \"\"\"\n",
    "    tz = pytz.timezone('US/Eastern')\n",
    "    timeArray = time.strptime(date, \"%Y-%m-%d %H:%M:%S\")\n",
    "    dt = datetime.fromtimestamp(mktime(timeArray))\n",
    "    timestamp = tz.localize(dt)\n",
    "    timestamp = timestamp.timestamp()\n",
    "    timeStamp = timestamp * 1000000000\n",
    "    return int(timeStamp)\n",
    "\n",
    "def datetime_to_timestamp_US(date):\n",
    "    \"\"\"\n",
    "    :param date: str   format: %Y-%m-%d %H:%M:%S   e.g. 2013-10-10 23:40:00\n",
    "    :return: nano timestamp\n",
    "    \"\"\"\n",
    "    tz = pytz.timezone('US/Eastern')\n",
    "    timeArray = time.strptime(date, \"%Y-%m-%d %H:%M:%S\")\n",
    "    dt = datetime.fromtimestamp(mktime(timeArray))\n",
    "    timestamp = tz.localize(dt)\n",
    "    timestamp = timestamp.timestamp()\n",
    "    timeStamp = timestamp\n",
    "    return int(timeStamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "OperationalError",
     "evalue": "could not connect to server: Connection refused\n\tIs the server running on host \"localhost\" (127.0.0.1) and accepting\n\tTCP/IP connections on port 5432?\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOperationalError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpsycopg2\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpsycopg2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m extras \u001b[38;5;28;01mas\u001b[39;00m ex\n\u001b[0;32m----> 4\u001b[0m connect \u001b[38;5;241m=\u001b[39m \u001b[43mpsycopg2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdatabase\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtc_clearscope3_dataset_db\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mhost\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlocalhost\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m                           \u001b[49m\u001b[43muser\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpostgres\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mpassword\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m123456\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mport\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m5432\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\n\u001b[1;32m      9\u001b[0m \u001b[43m                          \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m cur \u001b[38;5;241m=\u001b[39m connect\u001b[38;5;241m.\u001b[39mcursor()\n",
      "File \u001b[0;32m~/Fall2024/COMP7860_Project/.conda/lib/python3.9/site-packages/psycopg2/__init__.py:122\u001b[0m, in \u001b[0;36mconnect\u001b[0;34m(dsn, connection_factory, cursor_factory, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m     kwasync[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124masync_\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124masync_\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    121\u001b[0m dsn \u001b[38;5;241m=\u001b[39m _ext\u001b[38;5;241m.\u001b[39mmake_dsn(dsn, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 122\u001b[0m conn \u001b[38;5;241m=\u001b[39m \u001b[43m_connect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdsn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconnection_factory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconnection_factory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwasync\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cursor_factory \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m     conn\u001b[38;5;241m.\u001b[39mcursor_factory \u001b[38;5;241m=\u001b[39m cursor_factory\n",
      "\u001b[0;31mOperationalError\u001b[0m: could not connect to server: Connection refused\n\tIs the server running on host \"localhost\" (127.0.0.1) and accepting\n\tTCP/IP connections on port 5432?\n"
     ]
    }
   ],
   "source": [
    "import psycopg2\n",
    "\n",
    "from psycopg2 import extras as ex\n",
    "connect = psycopg2.connect(database = 'tc_clearscope3_dataset_db',\n",
    "                           host = 'localhost',\n",
    "                           user = 'postgres',\n",
    "                           password = '123456',\n",
    "                           port = '5432'\n",
    "                          )\n",
    "\n",
    "cur = connect.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_4_4=torch.load(\"./train_graphs/graph_4_4.TemporalData.simple\").to(device=device)\n",
    "graph_4_5=torch.load(\"./train_graphs/graph_4_5.TemporalData.simple\").to(device=device)\n",
    "graph_4_6=torch.load(\"./train_graphs/graph_4_6.TemporalData.simple\").to(device=device)\n",
    "\n",
    "\n",
    "train_data=graph_4_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cur' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Constructing the map for nodeid to msg\u001b[39;00m\n\u001b[1;32m      2\u001b[0m sql\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mselect * from node2id ORDER BY index_id;\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 3\u001b[0m \u001b[43mcur\u001b[49m\u001b[38;5;241m.\u001b[39mexecute(sql)\n\u001b[1;32m      4\u001b[0m rows \u001b[38;5;241m=\u001b[39m cur\u001b[38;5;241m.\u001b[39mfetchall()\n\u001b[1;32m      6\u001b[0m nodeid2msg\u001b[38;5;241m=\u001b[39m{}  \u001b[38;5;66;03m# nodeid => msg and node hash => nodeid\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'cur' is not defined"
     ]
    }
   ],
   "source": [
    "# Constructing the map for nodeid to msg\n",
    "sql=\"select * from node2id ORDER BY index_id;\"\n",
    "cur.execute(sql)\n",
    "rows = cur.fetchall()\n",
    "\n",
    "nodeid2msg={}  # nodeid => msg and node hash => nodeid\n",
    "for i in rows:\n",
    "    nodeid2msg[i[0]]=i[-1]\n",
    "    nodeid2msg[i[-1]]={i[1]:i[2]}  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "rel2id={1: 'EVENT_CLOSE',\n",
    " 'EVENT_CLOSE': 1,\n",
    " 2: 'EVENT_OPEN',\n",
    " 'EVENT_OPEN': 2,\n",
    " 3: 'EVENT_READ',\n",
    " 'EVENT_READ': 3,\n",
    " 4: 'EVENT_WRITE',\n",
    " 'EVENT_WRITE': 4,\n",
    " 5: 'EVENT_RECVFROM',\n",
    " 'EVENT_RECVFROM': 5,\n",
    " 6: 'EVENT_RECVMSG',\n",
    " 'EVENT_RECVMSG': 6,\n",
    " 7: 'EVENT_SENDMSG',\n",
    " 'EVENT_SENDMSG': 7,\n",
    " 8: 'EVENT_SENDTO',\n",
    " 'EVENT_SENDTO': 8}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data, val_data, test_data = data.train_val_test_split(val_ratio=0.15, test_ratio=0.15)\n",
    "# max_node_num = max(torch.cat([data.dst,data.src]))+1\n",
    "# max_node_num = data.num_nodes+1\n",
    "max_node_num = 172724  # +1\n",
    "# min_dst_idx, max_dst_idx = int(data.dst.min()), int(data.dst.max())\n",
    "min_dst_idx, max_dst_idx = 0, max_node_num\n",
    "neighbor_loader = LastNeighborLoader(max_node_num, size=20, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 57\u001b[0m\n\u001b[1;32m     52\u001b[0m time_dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m\n\u001b[1;32m     53\u001b[0m embedding_dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m      \u001b[38;5;66;03m# edge embedding\u001b[39;00m\n\u001b[1;32m     55\u001b[0m memory \u001b[38;5;241m=\u001b[39m TGNMemory(\n\u001b[1;32m     56\u001b[0m     max_node_num,\n\u001b[0;32m---> 57\u001b[0m     \u001b[43mtrain_data\u001b[49m\u001b[38;5;241m.\u001b[39mmsg\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m),\n\u001b[1;32m     58\u001b[0m     memory_dim,\n\u001b[1;32m     59\u001b[0m     time_dim,\n\u001b[1;32m     60\u001b[0m     message_module\u001b[38;5;241m=\u001b[39mIdentityMessage(train_data\u001b[38;5;241m.\u001b[39mmsg\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m), memory_dim, time_dim),\n\u001b[1;32m     61\u001b[0m     aggregator_module\u001b[38;5;241m=\u001b[39mLastAggregator(),\n\u001b[1;32m     62\u001b[0m )\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     64\u001b[0m gnn \u001b[38;5;241m=\u001b[39m GraphAttentionEmbedding(\n\u001b[1;32m     65\u001b[0m     in_channels\u001b[38;5;241m=\u001b[39mmemory_dim,\n\u001b[1;32m     66\u001b[0m     out_channels\u001b[38;5;241m=\u001b[39membedding_dim,\n\u001b[1;32m     67\u001b[0m     msg_dim\u001b[38;5;241m=\u001b[39mtrain_data\u001b[38;5;241m.\u001b[39mmsg\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m),\n\u001b[1;32m     68\u001b[0m     time_enc\u001b[38;5;241m=\u001b[39mmemory\u001b[38;5;241m.\u001b[39mtime_enc,\n\u001b[1;32m     69\u001b[0m )\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     71\u001b[0m link_pred \u001b[38;5;241m=\u001b[39m LinkPredictor(in_channels\u001b[38;5;241m=\u001b[39membedding_dim)\u001b[38;5;241m.\u001b[39mto(device)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_data' is not defined"
     ]
    }
   ],
   "source": [
    "class GraphAttentionEmbedding(torch.nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, msg_dim, time_enc):\n",
    "        super(GraphAttentionEmbedding, self).__init__()\n",
    "        self.time_enc = time_enc\n",
    "        edge_dim = msg_dim + time_enc.out_channels\n",
    "        self.conv = TransformerConv(in_channels, out_channels, heads=8,\n",
    "                                    dropout=0.0, edge_dim=edge_dim)\n",
    "        self.conv2 = TransformerConv(out_channels*8, out_channels,heads=1, concat=False,\n",
    "                             dropout=0.0, edge_dim=edge_dim)\n",
    "\n",
    "    def forward(self, x, last_update, edge_index, t, msg):\n",
    "        last_update.to(device)\n",
    "        x = x.to(device)\n",
    "        t = t.to(device)\n",
    "        rel_t = last_update[edge_index[0]] - t\n",
    "        rel_t_enc = self.time_enc(rel_t.to(x.dtype))\n",
    "        edge_attr = torch.cat([rel_t_enc, msg], dim=-1)\n",
    "        x = F.relu(self.conv(x, edge_index, edge_attr))\n",
    "        x = F.relu(self.conv2(x, edge_index, edge_attr))\n",
    "        return x\n",
    "\n",
    "\n",
    "class LinkPredictor(torch.nn.Module):\n",
    "    def __init__(self, in_channels):\n",
    "        super(LinkPredictor, self).__init__()\n",
    "        self.lin_src = Linear(in_channels, in_channels*2)\n",
    "        self.lin_dst = Linear(in_channels, in_channels*2)\n",
    "        \n",
    "        self.lin_seq = nn.Sequential(\n",
    "            \n",
    "            Linear(in_channels*4, in_channels*8),\n",
    "            torch.nn.Dropout(0.5),\n",
    "            nn.Tanh(),\n",
    "            Linear(in_channels*8, in_channels*2),\n",
    "            torch.nn.Dropout(0.5),\n",
    "            nn.Tanh(),\n",
    "            Linear(in_channels*2, int(in_channels//2)),\n",
    "            torch.nn.Dropout(0.5),\n",
    "            nn.Tanh(),\n",
    "            Linear(int(in_channels//2), train_data.msg.shape[1]-32)                   \n",
    "        )\n",
    "        \n",
    "\n",
    "    def forward(self, z_src, z_dst):\n",
    "        h = torch.cat([self.lin_src(z_src) , self.lin_dst(z_dst)],dim=-1)      \n",
    "         \n",
    "        h = self.lin_seq (h)\n",
    "        \n",
    "        return h\n",
    "\n",
    "memory_dim = 100         # node state\n",
    "time_dim = 100\n",
    "embedding_dim = 100      # edge embedding\n",
    "\n",
    "memory = TGNMemory(\n",
    "    max_node_num,\n",
    "    train_data.msg.size(-1),\n",
    "    memory_dim,\n",
    "    time_dim,\n",
    "    message_module=IdentityMessage(train_data.msg.size(-1), memory_dim, time_dim),\n",
    "    aggregator_module=LastAggregator(),\n",
    ").to(device)\n",
    "\n",
    "gnn = GraphAttentionEmbedding(\n",
    "    in_channels=memory_dim,\n",
    "    out_channels=embedding_dim,\n",
    "    msg_dim=train_data.msg.size(-1),\n",
    "    time_enc=memory.time_enc,\n",
    ").to(device)\n",
    "\n",
    "link_pred = LinkPredictor(in_channels=embedding_dim).to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(\n",
    "    set(memory.parameters()) | set(gnn.parameters())\n",
    "    | set(link_pred.parameters()), lr=0.00005, eps=1e-08,weight_decay=0.01)\n",
    "\n",
    "\n",
    "# scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Helper vector to map global node indices to local ones.\n",
    "assoc = torch.empty(max_node_num, dtype=torch.long, device=device)\n",
    "\n",
    "saved_nodes=set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gnn/gnn_model.png'"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchviz import make_dot\n",
    "\n",
    "memory.reset_state()  # Start with a fresh memory.\n",
    "neighbor_loader.reset_state()  # Start with an empty graph.\n",
    "\n",
    "for batch in train_data.seq_batches(batch_size=BATCH):\n",
    "    src, pos_dst, t, msg = batch.src, batch.dst, batch.t, batch.msg   \n",
    "    optimizer.zero_grad()\n",
    "    src, pos_dst, t, msg = batch.src, batch.dst, batch.t, batch.msg        \n",
    "\n",
    "    n_id = torch.cat([src, pos_dst]).unique()\n",
    "    n_id, edge_index, e_id = neighbor_loader(n_id)\n",
    "    assoc[n_id] = torch.arange(n_id.size(0), device=device)\n",
    "    z, last_update = memory(n_id)\n",
    "    output = gnn(z, last_update, edge_index, train_data.t[e_id], train_data.msg[e_id])\n",
    "    dot = make_dot(output, params=dict(gnn.named_parameters()))\n",
    "         \n",
    "    break\n",
    "dot.graph_attr.update(dpi=\"300\")\n",
    "dot.render('./gnn/gnn_model', format=\"svg\", cleanup=True)\n",
    "dot.render('./gnn/gnn_model', format=\"png\", cleanup=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH=1024\n",
    "def train(train_data):\n",
    "\n",
    "    \n",
    "    memory.train()\n",
    "    gnn.train()\n",
    "    link_pred.train()\n",
    "\n",
    "    memory.reset_state()  # Start with a fresh memory.\n",
    "    neighbor_loader.reset_state()  # Start with an empty graph.\n",
    "    saved_nodes=set()\n",
    "\n",
    "    total_loss = 0\n",
    "    \n",
    "#     print(\"train_before_stage_data:\",train_data)\n",
    "    for batch in train_data.seq_batches(batch_size=BATCH):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        src, pos_dst, t, msg = batch.src, batch.dst, batch.t, batch.msg        \n",
    "        \n",
    "        n_id = torch.cat([src, pos_dst]).unique()\n",
    "#         n_id = torch.cat([src, pos_dst, neg_src, neg_dst]).unique()\n",
    "        n_id, edge_index, e_id = neighbor_loader(n_id)\n",
    "        assoc[n_id] = torch.arange(n_id.size(0), device=device)\n",
    "\n",
    "        # Get updated memory of all nodes involved in the computation.\n",
    "        z, last_update = memory(n_id)\n",
    "      \n",
    "        z = gnn(z, last_update, edge_index, train_data.t[e_id], train_data.msg[e_id])\n",
    "        \n",
    "        pos_out = link_pred(z[assoc[src]], z[assoc[pos_dst]])       \n",
    "\n",
    "        y_pred = torch.cat([pos_out], dim=0)\n",
    "        \n",
    "#         y_true = torch.cat([torch.zeros(pos_out.size(0),1),torch.ones(neg_out.size(0),1)], dim=0)#\n",
    "        y_true=[]\n",
    "        for m in msg:\n",
    "            l=tensor_find(m[16:-16],1)-1\n",
    "            y_true.append(l)           \n",
    "          \n",
    "        y_true = torch.tensor(y_true).to(device=device)\n",
    "        y_true=y_true.reshape(-1).to(torch.long).to(device=device)\n",
    "        \n",
    "        loss = criterion(y_pred, y_true)\n",
    "        \n",
    "#         loss = criterion(pos_out, torch.ones_like(pos_out))\n",
    "#         loss += criterion(neg_out, torch.zeros_like(neg_out))\n",
    "\n",
    "        # Update memory and neighbor loader with ground-truth state.\n",
    "        memory.update_state(src, pos_dst, t, msg)\n",
    "        neighbor_loader.insert(src, pos_dst)\n",
    "        \n",
    "#         for i in range(len(src)):\n",
    "#             saved_nodes.add(int(src[i]))\n",
    "#             saved_nodes.add(int(pos_dst[i]))\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        memory.detach()\n",
    "#         print(z.shape)\n",
    "        total_loss += float(loss) * batch.num_events\n",
    "#     print(\"trained_stage_data:\",train_data)\n",
    "    return total_loss / train_data.num_events\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/30 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch: 01, Loss: 1.3259\n",
      "  Epoch: 01, Loss: 1.0878\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 1/30 [03:16<1:35:08, 196.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch: 01, Loss: 1.0974\n",
      "  Epoch: 02, Loss: 0.8865\n",
      "  Epoch: 02, Loss: 0.8823\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 2/30 [06:35<1:32:28, 198.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch: 02, Loss: 0.9999\n",
      "  Epoch: 03, Loss: 0.8215\n",
      "  Epoch: 03, Loss: 0.8063\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 3/30 [10:34<1:37:33, 216.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch: 03, Loss: 0.9485\n",
      "  Epoch: 04, Loss: 0.7844\n",
      "  Epoch: 04, Loss: 0.7546\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 4/30 [14:49<1:40:24, 231.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch: 04, Loss: 0.9176\n",
      "  Epoch: 05, Loss: 0.7632\n",
      "  Epoch: 05, Loss: 0.7246\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 5/30 [19:11<1:41:07, 242.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch: 05, Loss: 0.9017\n",
      "  Epoch: 06, Loss: 0.7502\n",
      "  Epoch: 06, Loss: 0.7079\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 6/30 [23:33<1:39:41, 249.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch: 06, Loss: 0.8913\n",
      "  Epoch: 07, Loss: 0.7419\n",
      "  Epoch: 07, Loss: 0.6953\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 7/30 [28:26<1:40:59, 263.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch: 07, Loss: 0.8818\n",
      "  Epoch: 08, Loss: 0.7350\n",
      "  Epoch: 08, Loss: 0.6867\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 8/30 [33:37<1:42:10, 278.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch: 08, Loss: 0.8727\n",
      "  Epoch: 09, Loss: 0.7287\n",
      "  Epoch: 09, Loss: 0.6793\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 9/30 [38:20<1:37:57, 279.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch: 09, Loss: 0.8622\n",
      "  Epoch: 10, Loss: 0.7216\n",
      "  Epoch: 10, Loss: 0.6669\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 10/30 [42:49<1:32:12, 276.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch: 10, Loss: 0.8520\n",
      "  Epoch: 11, Loss: 0.7168\n",
      "  Epoch: 11, Loss: 0.6619\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|███▋      | 11/30 [47:25<1:27:29, 276.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch: 11, Loss: 0.8447\n",
      "  Epoch: 12, Loss: 0.7126\n",
      "  Epoch: 12, Loss: 0.6579\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 12/30 [52:19<1:24:30, 281.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch: 12, Loss: 0.8395\n",
      "  Epoch: 13, Loss: 0.7096\n",
      "  Epoch: 13, Loss: 0.6533\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 13/30 [57:15<1:21:04, 286.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch: 13, Loss: 0.8337\n",
      "  Epoch: 14, Loss: 0.7073\n",
      "  Epoch: 14, Loss: 0.6501\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████▋     | 14/30 [1:02:17<1:17:35, 291.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch: 14, Loss: 0.8266\n",
      "  Epoch: 15, Loss: 0.7043\n",
      "  Epoch: 15, Loss: 0.6479\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 15/30 [1:07:32<1:14:32, 298.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch: 15, Loss: 0.8197\n",
      "  Epoch: 16, Loss: 0.7022\n",
      "  Epoch: 16, Loss: 0.6448\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████▎    | 16/30 [1:13:15<1:12:42, 311.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch: 16, Loss: 0.8188\n",
      "  Epoch: 17, Loss: 0.7005\n",
      "  Epoch: 17, Loss: 0.6434\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████▋    | 17/30 [1:19:11<1:10:24, 324.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch: 17, Loss: 0.8147\n",
      "  Epoch: 18, Loss: 0.6992\n",
      "  Epoch: 18, Loss: 0.6409\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 18/30 [1:25:11<1:07:08, 335.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch: 18, Loss: 0.8124\n",
      "  Epoch: 19, Loss: 0.6982\n",
      "  Epoch: 19, Loss: 0.6388\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|██████▎   | 19/30 [1:31:12<1:02:55, 343.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch: 19, Loss: 0.8086\n",
      "  Epoch: 20, Loss: 0.6973\n",
      "  Epoch: 20, Loss: 0.6374\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 20/30 [1:37:13<58:04, 348.49s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch: 20, Loss: 0.8060\n",
      "  Epoch: 21, Loss: 0.6959\n",
      "  Epoch: 21, Loss: 0.6357\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 21/30 [1:43:05<52:26, 349.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch: 21, Loss: 0.8038\n",
      "  Epoch: 22, Loss: 0.6951\n",
      "  Epoch: 22, Loss: 0.6343\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|███████▎  | 22/30 [1:48:43<46:08, 346.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch: 22, Loss: 0.8021\n",
      "  Epoch: 23, Loss: 0.6942\n",
      "  Epoch: 23, Loss: 0.6339\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|███████▋  | 23/30 [1:54:23<40:09, 344.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch: 23, Loss: 0.8007\n",
      "  Epoch: 24, Loss: 0.6933\n",
      "  Epoch: 24, Loss: 0.6331\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 24/30 [2:00:09<34:28, 344.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch: 24, Loss: 0.7997\n",
      "  Epoch: 25, Loss: 0.6925\n",
      "  Epoch: 25, Loss: 0.6319\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 25/30 [2:06:01<28:55, 347.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch: 25, Loss: 0.7985\n",
      "  Epoch: 26, Loss: 0.6919\n",
      "  Epoch: 26, Loss: 0.6308\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|████████▋ | 26/30 [2:11:51<23:11, 347.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch: 26, Loss: 0.7965\n",
      "  Epoch: 27, Loss: 0.6915\n",
      "  Epoch: 27, Loss: 0.6299\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 27/30 [2:17:51<17:34, 351.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch: 27, Loss: 0.7948\n",
      "  Epoch: 28, Loss: 0.6912\n",
      "  Epoch: 28, Loss: 0.6292\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|█████████▎| 28/30 [2:24:07<11:57, 358.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch: 28, Loss: 0.7941\n",
      "  Epoch: 29, Loss: 0.6901\n",
      "  Epoch: 29, Loss: 0.6282\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|█████████▋| 29/30 [2:31:05<06:16, 376.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch: 29, Loss: 0.7935\n",
      "  Epoch: 30, Loss: 0.6902\n",
      "  Epoch: 30, Loss: 0.6274\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [2:41:38<00:00, 323.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch: 30, Loss: 0.7928\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_graphs=[graph_4_4, graph_4_5, graph_4_6]\n",
    "\n",
    "for epoch in tqdm(range(1, 31)):\n",
    "    for g in train_graphs:\n",
    "        loss = train(g)\n",
    "        print(f'  Epoch: {epoch:02d}, Loss: {loss:.4f}')\n",
    "#     scheduler.step()\n",
    "model=[memory,gnn, link_pred,neighbor_loader]\n",
    "os.system(\"mkdir -p ./models/\")\n",
    "torch.save(model,\"./models/model_saved_share.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time \n",
    "\n",
    "@torch.no_grad()\n",
    "def test_day_new(inference_data,path):\n",
    "    if os.path.exists(path):\n",
    "        pass\n",
    "    else:\n",
    "        os.mkdir(path)\n",
    "    \n",
    "    memory.eval()\n",
    "    gnn.eval()\n",
    "    link_pred.eval()\n",
    "    \n",
    "    memory.reset_state()  # Start with a fresh memory.  \n",
    "    neighbor_loader.reset_state()  # Start with an empty graph.\n",
    "    \n",
    "    time_with_loss={}\n",
    "    total_loss = 0    \n",
    "    edge_list=[]\n",
    "    \n",
    "    unique_nodes=torch.tensor([]).to(device=device)\n",
    "    total_edges=0\n",
    "\n",
    "\n",
    "    start_time=inference_data.t[0]\n",
    "    event_count=0\n",
    "    \n",
    "    pos_o=[]\n",
    "    \n",
    "    loss_list=[]\n",
    "    \n",
    "\n",
    "    print(\"after merge:\",inference_data)\n",
    "    \n",
    "    # Record the running time to evaluate the performance\n",
    "    start = time.perf_counter()\n",
    "\n",
    "    for batch in inference_data.seq_batches(batch_size=BATCH):\n",
    "        \n",
    "        src, pos_dst, t, msg = batch.src, batch.dst, batch.t, batch.msg\n",
    "        unique_nodes=torch.cat([unique_nodes,src,pos_dst]).unique()\n",
    "        total_edges+=BATCH\n",
    "        \n",
    "       \n",
    "        n_id = torch.cat([src, pos_dst]).unique()       \n",
    "        n_id, edge_index, e_id = neighbor_loader(n_id)\n",
    "        assoc[n_id] = torch.arange(n_id.size(0), device=device)\n",
    "\n",
    "        z, last_update = memory(n_id)\n",
    "        z = gnn(z, last_update, edge_index, inference_data.t[e_id], inference_data.msg[e_id])\n",
    "\n",
    "        pos_out = link_pred(z[assoc[src]], z[assoc[pos_dst]])\n",
    "        \n",
    "        pos_o.append(pos_out)\n",
    "        y_pred = torch.cat([pos_out], dim=0)\n",
    "#         y_true = torch.cat(\n",
    "#             [torch.ones(pos_out.size(0))], dim=0).to(torch.long)     \n",
    "#         y_true=y_true.reshape(-1).to(torch.long)\n",
    "\n",
    "        y_true=[]\n",
    "        for m in msg:\n",
    "            l=tensor_find(m[16:-16],1)-1\n",
    "            y_true.append(l) \n",
    "        y_true = torch.tensor(y_true).to(device=device)\n",
    "        y_true=y_true.reshape(-1).to(torch.long).to(device=device)\n",
    "\n",
    "        # Only consider which edge hasn't been correctly predicted.\n",
    "        # For benign graphs, the behaviors patterns are similar and therefore their losses are small\n",
    "        # For anoamlous behaviors, some behaviors might not be seen before, so the probability of predicting those edges are low. Thus their losses are high.\n",
    "        loss = criterion(y_pred, y_true)\n",
    "\n",
    "        total_loss += float(loss) * batch.num_events\n",
    "     \n",
    "        \n",
    "        # update the edges in the batch to the memory and neighbor_loader\n",
    "        memory.update_state(src, pos_dst, t, msg)\n",
    "        neighbor_loader.insert(src, pos_dst)\n",
    "        \n",
    "        # compute the loss for each edge\n",
    "        each_edge_loss= cal_pos_edges_loss_multiclass(pos_out,y_true)\n",
    "        \n",
    "        for i in range(len(pos_out)):\n",
    "            srcnode=int(src[i])\n",
    "            dstnode=int(pos_dst[i])  \n",
    "            \n",
    "            srcmsg=str(nodeid2msg[srcnode]) \n",
    "            dstmsg=str(nodeid2msg[dstnode])\n",
    "            t_var=int(t[i])\n",
    "            edgeindex=tensor_find(msg[i][16:-16],1)   \n",
    "            edge_type=rel2id[edgeindex]\n",
    "            loss=each_edge_loss[i]    \n",
    "\n",
    "            temp_dic={}\n",
    "            temp_dic['loss']=float(loss)\n",
    "            temp_dic['srcnode']=srcnode\n",
    "            temp_dic['dstnode']=dstnode\n",
    "            temp_dic['srcmsg']=srcmsg\n",
    "            temp_dic['dstmsg']=dstmsg\n",
    "            temp_dic['edge_type']=edge_type\n",
    "            temp_dic['time']=t_var\n",
    "            \n",
    "#             if \"netflow\" in srcmsg or \"netflow\" in dstmsg:\n",
    "#                 temp_dic['loss']=0\n",
    "            edge_list.append(temp_dic)\n",
    "        \n",
    "        event_count+=len(batch.src)\n",
    "        if t[-1]>start_time+60000000000*15:\n",
    "            # Here is a checkpoint, which records all edge losses in the current time window\n",
    "#             loss=total_loss/event_count\n",
    "            time_interval=ns_time_to_datetime_US(start_time)+\"~\"+ns_time_to_datetime_US(t[-1])\n",
    "\n",
    "            end = time.perf_counter()\n",
    "            time_with_loss[time_interval]={'loss':loss,\n",
    "                                \n",
    "                                          'nodes_count':len(unique_nodes),\n",
    "                                          'total_edges':total_edges,\n",
    "                                          'costed_time':(end-start)}\n",
    "            \n",
    "            \n",
    "            log=open(path+\"/\"+time_interval+\".txt\",'w')\n",
    "            \n",
    "            for e in edge_list: \n",
    "#                 temp_key=e['srcmsg']+e['dstmsg']+e['edge_type']\n",
    "#                 if temp_key in train_edge_set:      \n",
    "# #                     e['loss']=(e['loss']-train_edge_set[temp_key]) if e['loss']>=train_edge_set[temp_key] else 0  \n",
    "# #                     e['loss']=abs(e['loss']-train_edge_set[temp_key])\n",
    "                    \n",
    "#                     e['modified']=True\n",
    "#                 else:\n",
    "#                     e['modified']=False\n",
    "                loss+=e['loss']\n",
    "\n",
    "            loss=loss/event_count   \n",
    "            print(f'Time: {time_interval}, Loss: {loss:.4f}, Nodes_count: {len(unique_nodes)}, Cost Time: {(end-start):.2f}s')\n",
    "            edge_list = sorted(edge_list, key=lambda x:x['loss'],reverse=True)  # Rank the results based on edge losses\n",
    "            for e in edge_list: \n",
    "                log.write(str(e))\n",
    "                log.write(\"\\n\") \n",
    "            event_count=0\n",
    "            total_loss=0\n",
    "            loss=0\n",
    "            start_time=t[-1]\n",
    "            log.close()\n",
    "            edge_list.clear()\n",
    "            \n",
    " \n",
    "    return time_with_loss\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_4_7=torch.load(\"./train_graphs/graph_4_7.TemporalData.simple\").to(device=device)\n",
    "graph_4_10=torch.load(\"./train_graphs/graph_4_10.TemporalData.simple\").to(device=device)\n",
    "graph_4_11=torch.load(\"./train_graphs/graph_4_11.TemporalData.simple\").to(device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "Can't get attribute 'GraphAttentionEmbedding' on <module '__main__'>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model\u001b[38;5;241m=\u001b[39m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./models/model_saved_share.pt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m memory,gnn, link_pred,neighbor_loader\u001b[38;5;241m=\u001b[39mmodel\n",
      "File \u001b[0;32m~/Fall2024/COMP7860_Project/.conda/lib/python3.9/site-packages/torch/serialization.py:789\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, **pickle_load_args)\u001b[0m\n\u001b[1;32m    787\u001b[0m             \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    788\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mUnpicklingError(UNSAFE_MESSAGE \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(e)) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 789\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43mopened_zipfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpickle_module\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpickle_load_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    790\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m weights_only:\n\u001b[1;32m    791\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/Fall2024/COMP7860_Project/.conda/lib/python3.9/site-packages/torch/serialization.py:1131\u001b[0m, in \u001b[0;36m_load\u001b[0;34m(zip_file, map_location, pickle_module, pickle_file, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1129\u001b[0m unpickler \u001b[38;5;241m=\u001b[39m UnpicklerWrapper(data_file, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpickle_load_args)\n\u001b[1;32m   1130\u001b[0m unpickler\u001b[38;5;241m.\u001b[39mpersistent_load \u001b[38;5;241m=\u001b[39m persistent_load\n\u001b[0;32m-> 1131\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43munpickler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1133\u001b[0m torch\u001b[38;5;241m.\u001b[39m_utils\u001b[38;5;241m.\u001b[39m_validate_loaded_sparse_tensors()\n\u001b[1;32m   1135\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/Fall2024/COMP7860_Project/.conda/lib/python3.9/site-packages/torch/serialization.py:1124\u001b[0m, in \u001b[0;36m_load.<locals>.UnpicklerWrapper.find_class\u001b[0;34m(self, mod_name, name)\u001b[0m\n\u001b[1;32m   1122\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m   1123\u001b[0m mod_name \u001b[38;5;241m=\u001b[39m load_module_mapping\u001b[38;5;241m.\u001b[39mget(mod_name, mod_name)\n\u001b[0;32m-> 1124\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind_class\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmod_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mAttributeError\u001b[0m: Can't get attribute 'GraphAttentionEmbedding' on <module '__main__'>"
     ]
    }
   ],
   "source": [
    "model=torch.load(\"./models/model_saved_share.pt\", map_location=device)\n",
    "memory,gnn, link_pred,neighbor_loader=model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after merge: TemporalData(dst=[1357851], msg=[1357851, 40], src=[1357851], t=[1357851])\n",
      "Time: 2018-04-04 00:00:00.030000000~2018-04-04 00:18:00.409000000, Loss: 2.3579, Nodes_count: 45, Cost Time: 0.23s\n",
      "Time: 2018-04-04 00:18:00.409000000~2018-04-04 00:44:00.541000000, Loss: 0.2353, Nodes_count: 81, Cost Time: 0.45s\n",
      "Time: 2018-04-04 00:44:00.541000000~2018-04-04 01:01:40.901000000, Loss: 0.4007, Nodes_count: 103, Cost Time: 0.69s\n",
      "Time: 2018-04-04 01:01:40.901000000~2018-04-04 01:28:00.498000000, Loss: 0.2123, Nodes_count: 129, Cost Time: 0.84s\n",
      "Time: 2018-04-04 01:28:00.498000000~2018-04-04 01:46:29.679000000, Loss: 0.3696, Nodes_count: 145, Cost Time: 1.02s\n",
      "Time: 2018-04-04 01:46:29.679000000~2018-04-04 02:13:00.519000000, Loss: 0.2157, Nodes_count: 160, Cost Time: 1.19s\n",
      "Time: 2018-04-04 02:13:00.519000000~2018-04-04 02:30:02.355000000, Loss: 0.4941, Nodes_count: 195, Cost Time: 1.34s\n",
      "Time: 2018-04-04 02:30:02.355000000~2018-04-04 02:57:00.339000000, Loss: 0.1719, Nodes_count: 215, Cost Time: 1.47s\n",
      "Time: 2018-04-04 02:57:00.339000000~2018-04-04 03:19:23.600000000, Loss: 0.2562, Nodes_count: 233, Cost Time: 1.66s\n",
      "Time: 2018-04-04 03:19:23.600000000~2018-04-04 03:41:40.895000000, Loss: 0.2862, Nodes_count: 250, Cost Time: 1.87s\n",
      "Time: 2018-04-04 03:41:40.895000000~2018-04-04 04:02:12.886000000, Loss: 0.3251, Nodes_count: 281, Cost Time: 2.13s\n",
      "Time: 2018-04-04 04:02:12.886000000~2018-04-04 04:28:26.253000000, Loss: 0.1977, Nodes_count: 304, Cost Time: 2.31s\n",
      "Time: 2018-04-04 04:28:26.253000000~2018-04-04 04:47:03.432000000, Loss: 0.3455, Nodes_count: 325, Cost Time: 2.48s\n",
      "Time: 2018-04-04 04:47:03.432000000~2018-04-04 05:13:47.869000000, Loss: 0.2008, Nodes_count: 346, Cost Time: 2.62s\n",
      "Time: 2018-04-04 05:13:47.869000000~2018-04-04 05:35:28.922000000, Loss: 0.2516, Nodes_count: 365, Cost Time: 2.77s\n",
      "Time: 2018-04-04 05:35:28.922000000~2018-04-04 05:57:55.470000000, Loss: 0.3079, Nodes_count: 390, Cost Time: 2.92s\n",
      "Time: 2018-04-04 05:57:55.470000000~2018-04-04 06:20:00.400000000, Loss: 0.2654, Nodes_count: 408, Cost Time: 3.18s\n",
      "Time: 2018-04-04 06:20:00.400000000~2018-04-04 06:42:05.998000000, Loss: 0.2638, Nodes_count: 425, Cost Time: 3.36s\n",
      "Time: 2018-04-04 06:42:05.998000000~2018-04-04 07:00:01.112000000, Loss: 0.4084, Nodes_count: 456, Cost Time: 3.79s\n",
      "Time: 2018-04-04 07:00:01.112000000~2018-04-04 07:25:31.328000000, Loss: 0.2085, Nodes_count: 476, Cost Time: 4.02s\n",
      "Time: 2018-04-04 07:25:31.328000000~2018-04-04 07:45:00.920000000, Loss: 0.3114, Nodes_count: 499, Cost Time: 4.46s\n",
      "Time: 2018-04-04 07:45:00.920000000~2018-04-04 08:05:00.337000000, Loss: 0.3139, Nodes_count: 521, Cost Time: 4.93s\n",
      "Time: 2018-04-04 08:05:00.337000000~2018-04-04 08:30:00.340000000, Loss: 0.2435, Nodes_count: 549, Cost Time: 5.05s\n",
      "Time: 2018-04-04 08:30:00.340000000~2018-04-04 08:53:44.098000000, Loss: 0.2232, Nodes_count: 566, Cost Time: 5.18s\n",
      "Time: 2018-04-04 08:53:44.098000000~2018-04-04 09:08:54.146000000, Loss: 0.6944, Nodes_count: 682, Cost Time: 10.23s\n",
      "Time: 2018-04-04 09:08:54.146000000~2018-04-04 09:24:14.060000000, Loss: 0.6938, Nodes_count: 742, Cost Time: 11.13s\n",
      "Time: 2018-04-04 09:24:14.060000000~2018-04-04 09:46:59.997000000, Loss: 0.5086, Nodes_count: 784, Cost Time: 14.37s\n",
      "Time: 2018-04-04 09:46:59.997000000~2018-04-04 10:07:04.753000000, Loss: 0.7343, Nodes_count: 1076, Cost Time: 16.50s\n",
      "Time: 2018-04-04 10:07:04.753000000~2018-04-04 10:24:56.390000000, Loss: 0.5947, Nodes_count: 1202, Cost Time: 21.30s\n",
      "Time: 2018-04-04 10:24:56.390000000~2018-04-04 10:43:20.118000000, Loss: 0.5882, Nodes_count: 1272, Cost Time: 28.15s\n",
      "Time: 2018-04-04 10:43:20.118000000~2018-04-04 11:00:02.549000000, Loss: 0.6524, Nodes_count: 1476, Cost Time: 40.62s\n",
      "Time: 2018-04-04 11:00:02.549000000~2018-04-04 11:15:17.515000000, Loss: 0.3252, Nodes_count: 1551, Cost Time: 44.56s\n",
      "Time: 2018-04-04 11:15:17.515000000~2018-04-04 11:31:03.856000000, Loss: 0.6524, Nodes_count: 1667, Cost Time: 46.45s\n",
      "Time: 2018-04-04 11:31:03.856000000~2018-04-04 11:46:32.145000000, Loss: 0.8690, Nodes_count: 2097, Cost Time: 50.14s\n",
      "Time: 2018-04-04 11:46:32.145000000~2018-04-04 12:01:38.539000000, Loss: 0.6297, Nodes_count: 2208, Cost Time: 59.15s\n",
      "Time: 2018-04-04 12:01:38.539000000~2018-04-04 12:16:41.081000000, Loss: 0.9764, Nodes_count: 3487, Cost Time: 66.35s\n",
      "Time: 2018-04-04 12:16:41.081000000~2018-04-04 12:34:19.710000000, Loss: 0.8044, Nodes_count: 4046, Cost Time: 70.41s\n",
      "Time: 2018-04-04 12:34:19.710000000~2018-04-04 12:49:22.540000000, Loss: 1.1030, Nodes_count: 4993, Cost Time: 75.50s\n",
      "Time: 2018-04-04 12:49:22.540000000~2018-04-04 13:08:07.223000000, Loss: 0.8763, Nodes_count: 5326, Cost Time: 78.98s\n",
      "Time: 2018-04-04 13:08:07.223000000~2018-04-04 13:30:01.377000000, Loss: 0.5480, Nodes_count: 5386, Cost Time: 79.94s\n",
      "Time: 2018-04-04 13:30:01.377000000~2018-04-04 13:45:50.641000000, Loss: 0.7847, Nodes_count: 5543, Cost Time: 82.96s\n",
      "Time: 2018-04-04 13:45:50.641000000~2018-04-04 14:01:08.905000000, Loss: 0.8523, Nodes_count: 5895, Cost Time: 86.91s\n",
      "Time: 2018-04-04 14:01:08.905000000~2018-04-04 14:17:40.088000000, Loss: 0.5673, Nodes_count: 5944, Cost Time: 93.82s\n",
      "Time: 2018-04-04 14:17:40.088000000~2018-04-04 14:37:36.937000000, Loss: 0.3391, Nodes_count: 5980, Cost Time: 96.16s\n",
      "Time: 2018-04-04 14:37:36.937000000~2018-04-04 14:56:53.830000000, Loss: 0.7261, Nodes_count: 6235, Cost Time: 102.23s\n",
      "Time: 2018-04-04 14:56:53.830000000~2018-04-04 15:11:59.827000000, Loss: 0.5860, Nodes_count: 6276, Cost Time: 105.62s\n",
      "Time: 2018-04-04 15:11:59.827000000~2018-04-04 15:30:59.833000000, Loss: 0.6052, Nodes_count: 6503, Cost Time: 111.09s\n",
      "Time: 2018-04-04 15:30:59.833000000~2018-04-04 15:46:27.669000000, Loss: 0.6188, Nodes_count: 6619, Cost Time: 113.53s\n",
      "Time: 2018-04-04 15:46:27.669000000~2018-04-04 16:01:28.278000000, Loss: 0.6550, Nodes_count: 6824, Cost Time: 139.70s\n",
      "Time: 2018-04-04 16:01:28.278000000~2018-04-04 16:19:10.228000000, Loss: 0.7131, Nodes_count: 6953, Cost Time: 150.59s\n",
      "Time: 2018-04-04 16:19:10.228000000~2018-04-04 16:34:10.523000000, Loss: 0.4320, Nodes_count: 6992, Cost Time: 151.90s\n",
      "Time: 2018-04-04 16:34:10.523000000~2018-04-04 16:49:13.914000000, Loss: 0.7452, Nodes_count: 7254, Cost Time: 158.49s\n",
      "Time: 2018-04-04 16:49:13.914000000~2018-04-04 17:04:39.724000000, Loss: 0.5372, Nodes_count: 7346, Cost Time: 160.32s\n",
      "Time: 2018-04-04 17:04:39.724000000~2018-04-04 17:28:27.768000000, Loss: 0.4381, Nodes_count: 7375, Cost Time: 160.61s\n",
      "Time: 2018-04-04 17:28:27.768000000~2018-04-04 17:50:34.864000000, Loss: 0.2683, Nodes_count: 7388, Cost Time: 160.72s\n",
      "Time: 2018-04-04 17:50:34.864000000~2018-04-04 18:12:26.456000000, Loss: 0.2679, Nodes_count: 7404, Cost Time: 160.82s\n",
      "Time: 2018-04-04 18:12:26.456000000~2018-04-04 18:30:37.518000000, Loss: 0.4140, Nodes_count: 7430, Cost Time: 160.93s\n",
      "Time: 2018-04-04 18:30:37.518000000~2018-04-04 18:55:41.922000000, Loss: 0.1777, Nodes_count: 7446, Cost Time: 161.03s\n",
      "Time: 2018-04-04 18:55:41.922000000~2018-04-04 19:15:01.002000000, Loss: 0.3092, Nodes_count: 7468, Cost Time: 161.14s\n",
      "Time: 2018-04-04 19:15:01.002000000~2018-04-04 19:34:59.220000000, Loss: 0.2786, Nodes_count: 7484, Cost Time: 161.25s\n",
      "Time: 2018-04-04 19:34:59.220000000~2018-04-04 20:00:00.180000000, Loss: 0.2327, Nodes_count: 7510, Cost Time: 161.36s\n",
      "Time: 2018-04-04 20:00:00.180000000~2018-04-04 20:21:35.285000000, Loss: 0.2265, Nodes_count: 7521, Cost Time: 161.46s\n",
      "Time: 2018-04-04 20:21:35.285000000~2018-04-04 20:43:53.871000000, Loss: 0.2777, Nodes_count: 7545, Cost Time: 161.60s\n",
      "Time: 2018-04-04 20:43:53.871000000~2018-04-04 21:05:30.588000000, Loss: 0.2872, Nodes_count: 7568, Cost Time: 161.70s\n",
      "Time: 2018-04-04 21:05:30.588000000~2018-04-04 21:27:32.722000000, Loss: 0.2659, Nodes_count: 7594, Cost Time: 161.80s\n",
      "Time: 2018-04-04 21:27:32.722000000~2018-04-04 21:48:59.197000000, Loss: 0.2604, Nodes_count: 7610, Cost Time: 161.91s\n",
      "Time: 2018-04-04 21:48:59.197000000~2018-04-04 22:09:45.403000000, Loss: 0.3347, Nodes_count: 7628, Cost Time: 162.03s\n",
      "Time: 2018-04-04 22:09:45.403000000~2018-04-04 22:30:16.698000000, Loss: 0.3401, Nodes_count: 7655, Cost Time: 162.13s\n",
      "Time: 2018-04-04 22:30:16.698000000~2018-04-04 22:52:49.267000000, Loss: 0.2002, Nodes_count: 7669, Cost Time: 162.24s\n",
      "Time: 2018-04-04 22:52:49.267000000~2018-04-04 23:13:54.971000000, Loss: 0.3176, Nodes_count: 7695, Cost Time: 162.35s\n",
      "Time: 2018-04-04 23:13:54.971000000~2018-04-04 23:35:49.374000000, Loss: 0.2865, Nodes_count: 7712, Cost Time: 162.46s\n",
      "Time: 2018-04-04 23:35:49.374000000~2018-04-04 23:59:02.265000000, Loss: 0.2325, Nodes_count: 7740, Cost Time: 162.57s\n"
     ]
    }
   ],
   "source": [
    "ans_4_4=test_day_new(graph_4_4,\"graph_4_4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after merge: TemporalData(dst=[840914], msg=[840914, 40], src=[840914], t=[840914])\n",
      "Time: 2018-04-05 00:00:00.041000000~2018-04-05 00:19:19.331000000, Loss: 2.2361, Nodes_count: 42, Cost Time: 0.07s\n",
      "Time: 2018-04-05 00:19:19.331000000~2018-04-05 00:44:22.644000000, Loss: 0.2355, Nodes_count: 75, Cost Time: 0.16s\n",
      "Time: 2018-04-05 00:44:22.644000000~2018-04-05 01:05:24.697000000, Loss: 0.2756, Nodes_count: 93, Cost Time: 0.25s\n",
      "Time: 2018-04-05 01:05:24.697000000~2018-04-05 01:27:27.947000000, Loss: 0.2711, Nodes_count: 118, Cost Time: 0.36s\n",
      "Time: 2018-04-05 01:27:27.947000000~2018-04-05 01:50:05.181000000, Loss: 0.2506, Nodes_count: 138, Cost Time: 0.45s\n",
      "Time: 2018-04-05 01:50:05.181000000~2018-04-05 02:15:00.642000000, Loss: 0.2614, Nodes_count: 168, Cost Time: 0.56s\n",
      "Time: 2018-04-05 02:15:00.642000000~2018-04-05 02:37:19.553000000, Loss: 0.3002, Nodes_count: 186, Cost Time: 0.66s\n",
      "Time: 2018-04-05 02:37:19.553000000~2018-04-05 02:58:26.087000000, Loss: 0.3119, Nodes_count: 216, Cost Time: 0.77s\n",
      "Time: 2018-04-05 02:58:26.087000000~2018-04-05 03:22:10.102000000, Loss: 0.2382, Nodes_count: 231, Cost Time: 0.87s\n",
      "Time: 2018-04-05 03:22:10.102000000~2018-04-05 03:45:00.640000000, Loss: 0.2645, Nodes_count: 259, Cost Time: 0.98s\n",
      "Time: 2018-04-05 03:45:00.640000000~2018-04-05 04:05:13.730000000, Loss: 0.3543, Nodes_count: 283, Cost Time: 1.09s\n",
      "Time: 2018-04-05 04:05:13.730000000~2018-04-05 04:30:00.519000000, Loss: 0.2290, Nodes_count: 309, Cost Time: 1.20s\n",
      "Time: 2018-04-05 04:30:00.519000000~2018-04-05 04:50:19.505000000, Loss: 0.3276, Nodes_count: 323, Cost Time: 1.30s\n",
      "Time: 2018-04-05 04:50:19.505000000~2018-04-05 05:12:47.557000000, Loss: 0.2648, Nodes_count: 341, Cost Time: 1.41s\n",
      "Time: 2018-04-05 05:12:47.557000000~2018-04-05 05:34:28.245000000, Loss: 0.2870, Nodes_count: 366, Cost Time: 1.52s\n",
      "Time: 2018-04-05 05:34:28.245000000~2018-04-05 05:56:18.371000000, Loss: 0.2678, Nodes_count: 390, Cost Time: 1.63s\n",
      "Time: 2018-04-05 05:56:18.371000000~2018-04-05 06:18:38.888000000, Loss: 0.2429, Nodes_count: 409, Cost Time: 1.73s\n",
      "Time: 2018-04-05 06:18:38.888000000~2018-04-05 06:39:58.601000000, Loss: 0.2718, Nodes_count: 428, Cost Time: 1.84s\n",
      "Time: 2018-04-05 06:39:58.601000000~2018-04-05 07:01:00.582000000, Loss: 0.3080, Nodes_count: 453, Cost Time: 1.95s\n",
      "Time: 2018-04-05 07:01:00.582000000~2018-04-05 07:23:23.952000000, Loss: 0.2174, Nodes_count: 466, Cost Time: 2.06s\n",
      "Time: 2018-04-05 07:23:23.952000000~2018-04-05 07:44:31.912000000, Loss: 0.2761, Nodes_count: 491, Cost Time: 2.17s\n",
      "Time: 2018-04-05 07:44:31.912000000~2018-04-05 08:01:00.827000000, Loss: 0.4073, Nodes_count: 516, Cost Time: 2.28s\n",
      "Time: 2018-04-05 08:01:00.827000000~2018-04-05 08:27:13.911000000, Loss: 0.1925, Nodes_count: 540, Cost Time: 2.38s\n",
      "Time: 2018-04-05 08:27:13.911000000~2018-04-05 08:49:43.480000000, Loss: 0.2290, Nodes_count: 558, Cost Time: 2.49s\n",
      "Time: 2018-04-05 08:49:43.480000000~2018-04-05 09:07:37.850000000, Loss: 0.7582, Nodes_count: 640, Cost Time: 3.24s\n",
      "Time: 2018-04-05 09:07:37.850000000~2018-04-05 09:23:08.586000000, Loss: 0.6155, Nodes_count: 725, Cost Time: 3.90s\n",
      "Time: 2018-04-05 09:23:08.586000000~2018-04-05 09:38:53.652000000, Loss: 0.8567, Nodes_count: 1444, Cost Time: 7.96s\n",
      "Time: 2018-04-05 09:38:53.652000000~2018-04-05 09:53:59.568000000, Loss: 0.6810, Nodes_count: 1604, Cost Time: 11.44s\n",
      "Time: 2018-04-05 09:53:59.568000000~2018-04-05 10:09:49.956000000, Loss: 0.6546, Nodes_count: 1723, Cost Time: 13.02s\n",
      "Time: 2018-04-05 10:09:49.956000000~2018-04-05 10:25:38.986000000, Loss: 0.3717, Nodes_count: 1756, Cost Time: 13.52s\n",
      "Time: 2018-04-05 10:25:38.986000000~2018-04-05 10:41:04.361000000, Loss: 0.7102, Nodes_count: 1917, Cost Time: 16.02s\n",
      "Time: 2018-04-05 10:41:04.361000000~2018-04-05 10:57:00.432000000, Loss: 0.6277, Nodes_count: 2079, Cost Time: 17.81s\n",
      "Time: 2018-04-05 10:57:00.432000000~2018-04-05 11:13:38.196000000, Loss: 0.2800, Nodes_count: 2111, Cost Time: 18.87s\n",
      "Time: 2018-04-05 11:13:38.196000000~2018-04-05 11:28:40.147000000, Loss: 0.7226, Nodes_count: 2313, Cost Time: 20.90s\n",
      "Time: 2018-04-05 11:28:40.147000000~2018-04-05 11:43:41.792000000, Loss: 0.6392, Nodes_count: 2392, Cost Time: 24.73s\n",
      "Time: 2018-04-05 11:43:41.792000000~2018-04-05 11:59:09.930000000, Loss: 0.2661, Nodes_count: 2449, Cost Time: 26.19s\n",
      "Time: 2018-04-05 11:59:09.930000000~2018-04-05 12:14:12.827000000, Loss: 0.9368, Nodes_count: 3626, Cost Time: 32.86s\n",
      "Time: 2018-04-05 12:14:12.827000000~2018-04-05 12:32:50.607000000, Loss: 0.7206, Nodes_count: 3659, Cost Time: 36.64s\n",
      "Time: 2018-04-05 12:32:50.607000000~2018-04-05 12:56:48.191000000, Loss: 0.4216, Nodes_count: 3769, Cost Time: 38.21s\n",
      "Time: 2018-04-05 12:56:48.191000000~2018-04-05 13:14:01.066000000, Loss: 0.2993, Nodes_count: 3793, Cost Time: 38.63s\n",
      "Time: 2018-04-05 13:14:01.066000000~2018-04-05 13:29:05.129000000, Loss: 0.8245, Nodes_count: 3935, Cost Time: 41.50s\n",
      "Time: 2018-04-05 13:29:05.129000000~2018-04-05 13:45:20.418000000, Loss: 0.5182, Nodes_count: 3998, Cost Time: 42.21s\n",
      "Time: 2018-04-05 13:45:20.418000000~2018-04-05 14:00:32.997000000, Loss: 0.5961, Nodes_count: 4137, Cost Time: 44.16s\n",
      "Time: 2018-04-05 14:00:32.997000000~2018-04-05 14:20:15.390000000, Loss: 0.3756, Nodes_count: 4208, Cost Time: 45.26s\n",
      "Time: 2018-04-05 14:20:15.390000000~2018-04-05 14:36:58.088000000, Loss: 0.6422, Nodes_count: 4310, Cost Time: 51.93s\n",
      "Time: 2018-04-05 14:36:58.088000000~2018-04-05 14:51:59.209000000, Loss: 0.3818, Nodes_count: 4369, Cost Time: 54.96s\n",
      "Time: 2018-04-05 14:51:59.209000000~2018-04-05 15:06:59.385000000, Loss: 0.7047, Nodes_count: 4722, Cost Time: 57.59s\n",
      "Time: 2018-04-05 15:06:59.385000000~2018-04-05 15:22:31.261000000, Loss: 0.5929, Nodes_count: 4906, Cost Time: 62.37s\n",
      "Time: 2018-04-05 15:22:31.261000000~2018-04-05 15:37:54.348000000, Loss: 0.8369, Nodes_count: 5616, Cost Time: 66.86s\n",
      "Time: 2018-04-05 15:37:54.348000000~2018-04-05 15:53:34.793000000, Loss: 0.4746, Nodes_count: 5759, Cost Time: 68.81s\n",
      "Time: 2018-04-05 15:53:34.793000000~2018-04-05 16:09:23.048000000, Loss: 0.2789, Nodes_count: 5780, Cost Time: 69.71s\n",
      "Time: 2018-04-05 16:09:23.048000000~2018-04-05 16:25:55.195000000, Loss: 0.4448, Nodes_count: 5928, Cost Time: 71.33s\n",
      "Time: 2018-04-05 16:25:55.195000000~2018-04-05 16:41:03.167000000, Loss: 0.3649, Nodes_count: 5989, Cost Time: 73.83s\n",
      "Time: 2018-04-05 16:41:03.167000000~2018-04-05 16:59:07.920000000, Loss: 0.4514, Nodes_count: 6055, Cost Time: 74.87s\n",
      "Time: 2018-04-05 16:59:07.920000000~2018-04-05 17:15:19.588000000, Loss: 0.2542, Nodes_count: 6067, Cost Time: 75.10s\n",
      "Time: 2018-04-05 17:15:19.588000000~2018-04-05 17:35:14.078000000, Loss: 0.1997, Nodes_count: 6081, Cost Time: 75.29s\n",
      "Time: 2018-04-05 17:35:14.078000000~2018-04-05 17:55:16.875000000, Loss: 0.2065, Nodes_count: 6100, Cost Time: 75.49s\n",
      "Time: 2018-04-05 17:55:16.875000000~2018-04-05 18:15:42.341000000, Loss: 0.2075, Nodes_count: 6121, Cost Time: 75.67s\n",
      "Time: 2018-04-05 18:15:42.341000000~2018-04-05 18:37:14.681000000, Loss: 0.3020, Nodes_count: 6142, Cost Time: 75.94s\n",
      "Time: 2018-04-05 18:37:14.681000000~2018-04-05 18:58:00.402000000, Loss: 0.2031, Nodes_count: 6162, Cost Time: 76.13s\n",
      "Time: 2018-04-05 18:58:00.402000000~2018-04-05 19:17:05.754000000, Loss: 0.2515, Nodes_count: 6180, Cost Time: 76.32s\n",
      "Time: 2018-04-05 19:17:05.754000000~2018-04-05 19:37:53.928000000, Loss: 0.2105, Nodes_count: 6198, Cost Time: 76.51s\n",
      "Time: 2018-04-05 19:37:53.928000000~2018-04-05 19:59:18.826000000, Loss: 0.1991, Nodes_count: 6220, Cost Time: 76.69s\n",
      "Time: 2018-04-05 19:59:18.826000000~2018-04-05 20:19:00.511000000, Loss: 0.1702, Nodes_count: 6229, Cost Time: 76.88s\n",
      "Time: 2018-04-05 20:19:00.511000000~2018-04-05 20:39:00.399000000, Loss: 0.2149, Nodes_count: 6250, Cost Time: 77.07s\n",
      "Time: 2018-04-05 20:39:00.399000000~2018-04-05 21:00:00.321000000, Loss: 0.1905, Nodes_count: 6277, Cost Time: 77.25s\n",
      "Time: 2018-04-05 21:00:00.321000000~2018-04-05 21:19:46.983000000, Loss: 0.3580, Nodes_count: 6293, Cost Time: 77.53s\n",
      "Time: 2018-04-05 21:19:46.983000000~2018-04-05 21:38:46.760000000, Loss: 0.3677, Nodes_count: 6309, Cost Time: 77.80s\n",
      "Time: 2018-04-05 21:38:46.760000000~2018-04-05 22:00:00.344000000, Loss: 0.2072, Nodes_count: 6331, Cost Time: 78.12s\n",
      "Time: 2018-04-05 22:00:00.344000000~2018-04-05 22:19:18.475000000, Loss: 0.3734, Nodes_count: 6343, Cost Time: 78.39s\n",
      "Time: 2018-04-05 22:19:18.475000000~2018-04-05 22:39:21.047000000, Loss: 0.2185, Nodes_count: 6357, Cost Time: 78.58s\n",
      "Time: 2018-04-05 22:39:21.047000000~2018-04-05 22:57:00.416000000, Loss: 0.4181, Nodes_count: 6387, Cost Time: 78.86s\n",
      "Time: 2018-04-05 22:57:00.416000000~2018-04-05 23:15:02.754000000, Loss: 0.2526, Nodes_count: 6403, Cost Time: 79.06s\n",
      "Time: 2018-04-05 23:15:02.754000000~2018-04-05 23:35:48.736000000, Loss: 0.2045, Nodes_count: 6418, Cost Time: 79.25s\n",
      "Time: 2018-04-05 23:35:48.736000000~2018-04-05 23:58:11.523000000, Loss: 0.1650, Nodes_count: 6439, Cost Time: 79.43s\n"
     ]
    }
   ],
   "source": [
    "ans_4_5=test_day_new(graph_4_5,\"graph_4_5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after merge: TemporalData(dst=[1134670], msg=[1134670, 40], src=[1134670], t=[1134670])\n",
      "Time: 2018-04-06 00:00:00.050000000~2018-04-06 00:19:16.992000000, Loss: 1.1910, Nodes_count: 43, Cost Time: 0.16s\n",
      "Time: 2018-04-06 00:19:16.992000000~2018-04-06 00:45:05.853000000, Loss: 0.4263, Nodes_count: 87, Cost Time: 0.52s\n",
      "Time: 2018-04-06 00:45:05.853000000~2018-04-06 01:00:11.671000000, Loss: 0.3184, Nodes_count: 114, Cost Time: 0.72s\n",
      "Time: 2018-04-06 01:00:11.671000000~2018-04-06 01:18:57.427000000, Loss: 0.4092, Nodes_count: 126, Cost Time: 1.00s\n",
      "Time: 2018-04-06 01:18:57.427000000~2018-04-06 01:42:40.184000000, Loss: 0.1701, Nodes_count: 141, Cost Time: 1.18s\n",
      "Time: 2018-04-06 01:42:40.184000000~2018-04-06 02:05:25.221000000, Loss: 0.4424, Nodes_count: 176, Cost Time: 1.56s\n",
      "Time: 2018-04-06 02:05:25.221000000~2018-04-06 02:22:54.336000000, Loss: 0.3198, Nodes_count: 195, Cost Time: 1.85s\n",
      "Time: 2018-04-06 02:22:54.336000000~2018-04-06 02:44:45.281000000, Loss: 0.2107, Nodes_count: 225, Cost Time: 2.04s\n",
      "Time: 2018-04-06 02:44:45.281000000~2018-04-06 03:03:00.573000000, Loss: 0.2556, Nodes_count: 248, Cost Time: 2.26s\n",
      "Time: 2018-04-06 03:03:00.573000000~2018-04-06 03:25:24.762000000, Loss: 0.1772, Nodes_count: 266, Cost Time: 2.44s\n",
      "Time: 2018-04-06 03:25:24.762000000~2018-04-06 03:45:28.902000000, Loss: 0.1801, Nodes_count: 287, Cost Time: 2.63s\n",
      "Time: 2018-04-06 03:45:28.902000000~2018-04-06 04:05:31.388000000, Loss: 0.2223, Nodes_count: 310, Cost Time: 2.82s\n",
      "Time: 2018-04-06 04:05:31.388000000~2018-04-06 04:27:35.079000000, Loss: 0.2101, Nodes_count: 331, Cost Time: 3.00s\n",
      "Time: 2018-04-06 04:27:35.079000000~2018-04-06 04:45:32.422000000, Loss: 0.3187, Nodes_count: 353, Cost Time: 3.29s\n",
      "Time: 2018-04-06 04:45:32.422000000~2018-04-06 05:07:27.473000000, Loss: 0.2172, Nodes_count: 367, Cost Time: 3.49s\n",
      "Time: 2018-04-06 05:07:27.473000000~2018-04-06 05:27:24.984000000, Loss: 0.2129, Nodes_count: 391, Cost Time: 3.68s\n",
      "Time: 2018-04-06 05:27:24.984000000~2018-04-06 05:47:11.800000000, Loss: 0.2039, Nodes_count: 411, Cost Time: 3.86s\n",
      "Time: 2018-04-06 05:47:11.800000000~2018-04-06 06:05:34.537000000, Loss: 0.2771, Nodes_count: 430, Cost Time: 4.15s\n",
      "Time: 2018-04-06 06:05:34.537000000~2018-04-06 06:28:00.121000000, Loss: 0.1886, Nodes_count: 450, Cost Time: 4.34s\n",
      "Time: 2018-04-06 06:28:00.121000000~2018-04-06 06:45:02.339000000, Loss: 0.3124, Nodes_count: 469, Cost Time: 4.64s\n",
      "Time: 2018-04-06 06:45:02.339000000~2018-04-06 07:05:33.094000000, Loss: 0.2341, Nodes_count: 487, Cost Time: 4.84s\n",
      "Time: 2018-04-06 07:05:33.094000000~2018-04-06 07:27:38.006000000, Loss: 0.1741, Nodes_count: 511, Cost Time: 5.02s\n",
      "Time: 2018-04-06 07:27:38.006000000~2018-04-06 07:47:00.431000000, Loss: 0.1955, Nodes_count: 526, Cost Time: 5.21s\n",
      "Time: 2018-04-06 07:47:00.431000000~2018-04-06 08:07:33.934000000, Loss: 0.2089, Nodes_count: 546, Cost Time: 5.40s\n",
      "Time: 2018-04-06 08:07:33.934000000~2018-04-06 08:29:46.554000000, Loss: 0.2060, Nodes_count: 573, Cost Time: 5.72s\n",
      "Time: 2018-04-06 08:29:46.554000000~2018-04-06 08:45:16.620000000, Loss: 0.3229, Nodes_count: 594, Cost Time: 5.99s\n",
      "Time: 2018-04-06 08:45:16.620000000~2018-04-06 09:04:00.469000000, Loss: 0.1934, Nodes_count: 611, Cost Time: 6.21s\n",
      "Time: 2018-04-06 09:04:00.469000000~2018-04-06 09:29:00.411000000, Loss: 0.1881, Nodes_count: 640, Cost Time: 6.39s\n",
      "Time: 2018-04-06 09:29:00.411000000~2018-04-06 09:49:36.361000000, Loss: 0.2942, Nodes_count: 656, Cost Time: 6.69s\n",
      "Time: 2018-04-06 09:49:36.361000000~2018-04-06 10:11:25.827000000, Loss: 0.2343, Nodes_count: 679, Cost Time: 6.89s\n",
      "Time: 2018-04-06 10:11:25.827000000~2018-04-06 10:33:37.548000000, Loss: 0.2291, Nodes_count: 707, Cost Time: 7.08s\n",
      "Time: 2018-04-06 10:33:37.548000000~2018-04-06 10:51:59.967000000, Loss: 0.2836, Nodes_count: 725, Cost Time: 7.36s\n",
      "Time: 2018-04-06 10:51:59.967000000~2018-04-06 11:14:09.465000000, Loss: 0.1813, Nodes_count: 742, Cost Time: 7.56s\n",
      "Time: 2018-04-06 11:14:09.465000000~2018-04-06 11:30:58.081000000, Loss: 0.3254, Nodes_count: 772, Cost Time: 7.86s\n",
      "Time: 2018-04-06 11:30:58.081000000~2018-04-06 11:51:20.194000000, Loss: 0.1995, Nodes_count: 787, Cost Time: 8.06s\n",
      "Time: 2018-04-06 11:51:20.194000000~2018-04-06 12:09:43.911000000, Loss: 0.2235, Nodes_count: 806, Cost Time: 8.24s\n",
      "Time: 2018-04-06 12:09:43.911000000~2018-04-06 12:25:54.685000000, Loss: 0.2597, Nodes_count: 863, Cost Time: 8.59s\n",
      "Time: 2018-04-06 12:25:54.685000000~2018-04-06 12:41:20.717000000, Loss: 0.2401, Nodes_count: 919, Cost Time: 9.02s\n",
      "Time: 2018-04-06 12:41:20.717000000~2018-04-06 12:56:55.757000000, Loss: 0.2746, Nodes_count: 991, Cost Time: 9.46s\n",
      "Time: 2018-04-06 12:56:55.757000000~2018-04-06 13:12:04.238000000, Loss: 0.2650, Nodes_count: 1045, Cost Time: 9.90s\n",
      "Time: 2018-04-06 13:12:04.238000000~2018-04-06 13:30:04.328000000, Loss: 0.2555, Nodes_count: 1083, Cost Time: 10.19s\n",
      "Time: 2018-04-06 13:30:04.328000000~2018-04-06 13:45:17.542000000, Loss: 0.4062, Nodes_count: 1184, Cost Time: 10.79s\n",
      "Time: 2018-04-06 13:45:17.542000000~2018-04-06 14:00:35.220000000, Loss: 1.0075, Nodes_count: 1964, Cost Time: 12.62s\n",
      "Time: 2018-04-06 14:00:35.220000000~2018-04-06 14:19:19.282000000, Loss: 0.8048, Nodes_count: 2212, Cost Time: 13.65s\n",
      "Time: 2018-04-06 14:19:19.282000000~2018-04-06 14:36:21.009000000, Loss: 0.7542, Nodes_count: 2284, Cost Time: 14.41s\n",
      "Time: 2018-04-06 14:36:21.009000000~2018-04-06 14:52:35.840000000, Loss: 0.9400, Nodes_count: 2632, Cost Time: 15.82s\n",
      "Time: 2018-04-06 14:52:35.840000000~2018-04-06 15:08:30.414000000, Loss: 0.9752, Nodes_count: 2883, Cost Time: 17.05s\n",
      "Time: 2018-04-06 15:08:30.414000000~2018-04-06 15:29:50.869000000, Loss: 0.3855, Nodes_count: 2923, Cost Time: 17.49s\n",
      "Time: 2018-04-06 15:29:50.869000000~2018-04-06 15:49:26.015000000, Loss: 0.8519, Nodes_count: 3042, Cost Time: 18.38s\n",
      "Time: 2018-04-06 15:49:26.015000000~2018-04-06 16:05:10.837000000, Loss: 0.6076, Nodes_count: 3231, Cost Time: 24.94s\n",
      "Time: 2018-04-06 16:05:10.837000000~2018-04-06 16:20:11.626000000, Loss: 0.6353, Nodes_count: 3254, Cost Time: 25.80s\n",
      "Time: 2018-04-06 16:20:11.626000000~2018-04-06 16:35:25.587000000, Loss: 0.9411, Nodes_count: 4573, Cost Time: 32.78s\n",
      "Time: 2018-04-06 16:35:25.587000000~2018-04-06 16:50:32.945000000, Loss: 0.7948, Nodes_count: 4943, Cost Time: 37.20s\n",
      "Time: 2018-04-06 16:50:32.945000000~2018-04-06 17:05:39.288000000, Loss: 0.9304, Nodes_count: 5490, Cost Time: 39.40s\n",
      "Time: 2018-04-06 17:05:39.288000000~2018-04-06 17:20:54.977000000, Loss: 0.8430, Nodes_count: 6203, Cost Time: 45.98s\n",
      "Time: 2018-04-06 17:20:54.977000000~2018-04-06 17:36:52.502000000, Loss: 0.6619, Nodes_count: 7037, Cost Time: 52.48s\n",
      "Time: 2018-04-06 17:36:52.502000000~2018-04-06 17:52:05.798000000, Loss: 0.8545, Nodes_count: 7783, Cost Time: 55.89s\n",
      "Time: 2018-04-06 17:52:05.798000000~2018-04-06 18:12:57.421000000, Loss: 0.5942, Nodes_count: 7880, Cost Time: 60.65s\n",
      "Time: 2018-04-06 18:12:57.421000000~2018-04-06 18:30:22.483000000, Loss: 1.0096, Nodes_count: 8569, Cost Time: 63.27s\n",
      "Time: 2018-04-06 18:30:22.483000000~2018-04-06 18:45:23.209000000, Loss: 1.0073, Nodes_count: 10489, Cost Time: 70.04s\n",
      "Time: 2018-04-06 18:45:23.209000000~2018-04-06 19:04:54.592000000, Loss: 0.8522, Nodes_count: 11175, Cost Time: 80.47s\n",
      "Time: 2018-04-06 19:04:54.592000000~2018-04-06 19:23:03.611000000, Loss: 0.9816, Nodes_count: 12119, Cost Time: 88.83s\n",
      "Time: 2018-04-06 19:23:03.611000000~2018-04-06 19:44:32.667000000, Loss: 1.0145, Nodes_count: 12557, Cost Time: 90.96s\n",
      "Time: 2018-04-06 19:44:32.667000000~2018-04-06 20:00:39.987000000, Loss: 0.9490, Nodes_count: 12716, Cost Time: 92.04s\n",
      "Time: 2018-04-06 20:00:39.987000000~2018-04-06 20:19:42.681000000, Loss: 0.4485, Nodes_count: 12727, Cost Time: 92.40s\n",
      "Time: 2018-04-06 20:19:42.681000000~2018-04-06 20:42:46.845000000, Loss: 0.2638, Nodes_count: 12750, Cost Time: 93.29s\n",
      "Time: 2018-04-06 20:42:46.845000000~2018-04-06 20:59:25.771000000, Loss: 1.0745, Nodes_count: 13402, Cost Time: 94.60s\n",
      "Time: 2018-04-06 20:59:25.771000000~2018-04-06 21:28:30.420000000, Loss: 0.6785, Nodes_count: 13464, Cost Time: 94.99s\n",
      "Time: 2018-04-06 21:28:30.420000000~2018-04-06 21:53:32.222000000, Loss: 0.3350, Nodes_count: 13496, Cost Time: 95.23s\n",
      "Time: 2018-04-06 21:53:32.222000000~2018-04-06 22:10:33.999000000, Loss: 0.7544, Nodes_count: 14116, Cost Time: 100.41s\n",
      "Time: 2018-04-06 22:10:33.999000000~2018-04-06 22:39:24.718000000, Loss: 0.5458, Nodes_count: 14158, Cost Time: 100.95s\n",
      "Time: 2018-04-06 22:39:24.718000000~2018-04-06 23:10:53.922000000, Loss: 0.9472, Nodes_count: 14572, Cost Time: 104.05s\n",
      "Time: 2018-04-06 23:10:53.922000000~2018-04-06 23:31:29.855000000, Loss: 0.3560, Nodes_count: 14605, Cost Time: 104.32s\n",
      "Time: 2018-04-06 23:31:29.855000000~2018-04-06 23:58:03.573000000, Loss: 0.1894, Nodes_count: 14628, Cost Time: 104.41s\n"
     ]
    }
   ],
   "source": [
    "ans_4_6=test_day_new(graph_4_6,\"graph_4_6\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after merge: TemporalData(dst=[1847921], msg=[1847921, 40], src=[1847921], t=[1847921])\n",
      "Time: 2018-04-07 00:00:00.040000000~2018-04-07 00:30:11.169000000, Loss: 1.5013, Nodes_count: 75, Cost Time: 0.15s\n",
      "Time: 2018-04-07 00:30:11.169000000~2018-04-07 00:55:07.465000000, Loss: 0.6256, Nodes_count: 99, Cost Time: 0.32s\n",
      "Time: 2018-04-07 00:55:07.465000000~2018-04-07 01:18:15.361000000, Loss: 0.3260, Nodes_count: 119, Cost Time: 0.42s\n",
      "Time: 2018-04-07 01:18:15.361000000~2018-04-07 01:44:15.385000000, Loss: 0.2115, Nodes_count: 142, Cost Time: 0.50s\n",
      "Time: 2018-04-07 01:44:15.385000000~2018-04-07 02:15:00.653000000, Loss: 0.5588, Nodes_count: 176, Cost Time: 0.68s\n",
      "Time: 2018-04-07 02:15:00.653000000~2018-04-07 02:30:22.992000000, Loss: 0.8336, Nodes_count: 201, Cost Time: 0.87s\n",
      "Time: 2018-04-07 02:30:22.992000000~2018-04-07 02:45:49.212000000, Loss: 0.6254, Nodes_count: 210, Cost Time: 0.98s\n",
      "Time: 2018-04-07 02:45:49.212000000~2018-04-07 03:11:00.395000000, Loss: 0.2747, Nodes_count: 227, Cost Time: 1.07s\n",
      "Time: 2018-04-07 03:11:00.395000000~2018-04-07 03:32:00.300000000, Loss: 0.3726, Nodes_count: 256, Cost Time: 1.17s\n",
      "Time: 2018-04-07 03:32:00.300000000~2018-04-07 03:58:51.900000000, Loss: 0.2112, Nodes_count: 277, Cost Time: 1.26s\n",
      "Time: 2018-04-07 03:58:51.900000000~2018-04-07 04:23:34.116000000, Loss: 0.2425, Nodes_count: 300, Cost Time: 1.35s\n",
      "Time: 2018-04-07 04:23:34.116000000~2018-04-07 04:46:00.626000000, Loss: 0.2551, Nodes_count: 319, Cost Time: 1.44s\n",
      "Time: 2018-04-07 04:46:00.626000000~2018-04-07 05:10:42.317000000, Loss: 0.2314, Nodes_count: 333, Cost Time: 1.54s\n",
      "Time: 2018-04-07 05:10:42.317000000~2018-04-07 05:30:07.852000000, Loss: 0.4252, Nodes_count: 364, Cost Time: 1.63s\n",
      "Time: 2018-04-07 05:30:07.852000000~2018-04-07 05:57:05.858000000, Loss: 0.6526, Nodes_count: 378, Cost Time: 1.81s\n",
      "Time: 2018-04-07 05:57:05.858000000~2018-04-07 06:20:15.841000000, Loss: 0.3718, Nodes_count: 398, Cost Time: 1.91s\n",
      "Time: 2018-04-07 06:20:15.841000000~2018-04-07 06:44:17.080000000, Loss: 0.3015, Nodes_count: 419, Cost Time: 2.01s\n",
      "Time: 2018-04-07 06:44:17.080000000~2018-04-07 07:08:21.521000000, Loss: 0.2780, Nodes_count: 440, Cost Time: 2.10s\n",
      "Time: 2018-04-07 07:08:21.521000000~2018-04-07 07:30:13.991000000, Loss: 0.3201, Nodes_count: 468, Cost Time: 2.20s\n",
      "Time: 2018-04-07 07:30:13.991000000~2018-04-07 07:54:56.858000000, Loss: 0.2750, Nodes_count: 492, Cost Time: 2.29s\n",
      "Time: 2018-04-07 07:54:56.858000000~2018-04-07 08:17:34.639000000, Loss: 0.2948, Nodes_count: 514, Cost Time: 2.38s\n",
      "Time: 2018-04-07 08:17:34.639000000~2018-04-07 08:34:49.687000000, Loss: 0.8390, Nodes_count: 537, Cost Time: 2.57s\n",
      "Time: 2018-04-07 08:34:49.687000000~2018-04-07 08:58:17.722000000, Loss: 0.3449, Nodes_count: 561, Cost Time: 2.79s\n",
      "Time: 2018-04-07 08:58:17.722000000~2018-04-07 09:13:29.714000000, Loss: 0.7025, Nodes_count: 700, Cost Time: 8.01s\n",
      "Time: 2018-04-07 09:13:29.714000000~2018-04-07 09:28:31.696000000, Loss: 0.6914, Nodes_count: 772, Cost Time: 15.32s\n",
      "Time: 2018-04-07 09:28:31.696000000~2018-04-07 09:45:27.476000000, Loss: 0.7215, Nodes_count: 816, Cost Time: 23.23s\n",
      "Time: 2018-04-07 09:45:27.476000000~2018-04-07 10:00:33.713000000, Loss: 0.9531, Nodes_count: 835, Cost Time: 24.04s\n",
      "Time: 2018-04-07 10:00:33.713000000~2018-04-07 10:22:14.290000000, Loss: 0.8401, Nodes_count: 1542, Cost Time: 27.74s\n",
      "Time: 2018-04-07 10:22:14.290000000~2018-04-07 10:39:58.094000000, Loss: 0.9660, Nodes_count: 2469, Cost Time: 31.45s\n",
      "Time: 2018-04-07 10:39:58.094000000~2018-04-07 11:00:08.471000000, Loss: 0.7739, Nodes_count: 3246, Cost Time: 38.87s\n",
      "Time: 2018-04-07 11:00:08.471000000~2018-04-07 11:15:32.309000000, Loss: 0.7814, Nodes_count: 3821, Cost Time: 43.47s\n",
      "Time: 2018-04-07 11:15:32.309000000~2018-04-07 11:35:46.576000000, Loss: 0.7281, Nodes_count: 4297, Cost Time: 48.41s\n",
      "Time: 2018-04-07 11:35:46.576000000~2018-04-07 11:51:38.305000000, Loss: 0.5048, Nodes_count: 4381, Cost Time: 49.95s\n",
      "Time: 2018-04-07 11:51:38.305000000~2018-04-07 12:06:57.429000000, Loss: 0.6190, Nodes_count: 4472, Cost Time: 57.15s\n",
      "Time: 2018-04-07 12:06:57.429000000~2018-04-07 12:21:58.368000000, Loss: 0.7548, Nodes_count: 4623, Cost Time: 58.90s\n",
      "Time: 2018-04-07 12:21:58.368000000~2018-04-07 12:37:22.947000000, Loss: 0.9787, Nodes_count: 5239, Cost Time: 61.31s\n",
      "Time: 2018-04-07 12:37:22.947000000~2018-04-07 12:52:26.920000000, Loss: 0.7999, Nodes_count: 6067, Cost Time: 69.28s\n",
      "Time: 2018-04-07 12:52:26.920000000~2018-04-07 13:14:42.953000000, Loss: 0.9752, Nodes_count: 6329, Cost Time: 71.62s\n",
      "Time: 2018-04-07 13:14:42.953000000~2018-04-07 13:41:49.780000000, Loss: 0.3982, Nodes_count: 6352, Cost Time: 71.96s\n",
      "Time: 2018-04-07 13:41:49.780000000~2018-04-07 14:00:00.583000000, Loss: 0.7420, Nodes_count: 6413, Cost Time: 72.43s\n",
      "Time: 2018-04-07 14:00:00.583000000~2018-04-07 14:15:33.098000000, Loss: 0.7767, Nodes_count: 7312, Cost Time: 78.90s\n",
      "Time: 2018-04-07 14:15:33.098000000~2018-04-07 14:37:39.333000000, Loss: 0.8136, Nodes_count: 7844, Cost Time: 83.90s\n",
      "Time: 2018-04-07 14:37:39.333000000~2018-04-07 14:53:20.642000000, Loss: 0.7891, Nodes_count: 7871, Cost Time: 84.59s\n",
      "Time: 2018-04-07 14:53:20.642000000~2018-04-07 15:08:21.119000000, Loss: 0.6458, Nodes_count: 7938, Cost Time: 86.43s\n",
      "Time: 2018-04-07 15:08:21.119000000~2018-04-07 15:24:08.091000000, Loss: 0.7082, Nodes_count: 7950, Cost Time: 90.98s\n",
      "Time: 2018-04-07 15:24:08.091000000~2018-04-07 15:39:13.623000000, Loss: 0.6726, Nodes_count: 8026, Cost Time: 94.51s\n",
      "Time: 2018-04-07 15:39:13.623000000~2018-04-07 16:00:04.364000000, Loss: 0.6920, Nodes_count: 8070, Cost Time: 96.00s\n",
      "Time: 2018-04-07 16:00:04.364000000~2018-04-07 16:25:54.546000000, Loss: 2.2363, Nodes_count: 8424, Cost Time: 97.56s\n",
      "Time: 2018-04-07 16:25:54.546000000~2018-04-07 16:42:27.814000000, Loss: 0.2793, Nodes_count: 8432, Cost Time: 97.76s\n",
      "Time: 2018-04-07 16:42:27.814000000~2018-04-07 16:57:38.063000000, Loss: 0.6581, Nodes_count: 8527, Cost Time: 101.16s\n",
      "Time: 2018-04-07 16:57:38.063000000~2018-04-07 17:12:59.921000000, Loss: 1.2859, Nodes_count: 9175, Cost Time: 105.25s\n",
      "Time: 2018-04-07 17:12:59.921000000~2018-04-07 17:30:04.373000000, Loss: 0.9945, Nodes_count: 9731, Cost Time: 109.42s\n",
      "Time: 2018-04-07 17:30:04.373000000~2018-04-07 17:45:12.921000000, Loss: 0.6007, Nodes_count: 9826, Cost Time: 111.81s\n",
      "Time: 2018-04-07 17:45:12.921000000~2018-04-07 18:00:33.420000000, Loss: 1.1913, Nodes_count: 10730, Cost Time: 117.76s\n",
      "Time: 2018-04-07 18:00:33.420000000~2018-04-07 18:24:44.896000000, Loss: 0.2861, Nodes_count: 10749, Cost Time: 118.87s\n",
      "Time: 2018-04-07 18:24:44.896000000~2018-04-07 18:39:52.248000000, Loss: 1.2106, Nodes_count: 11190, Cost Time: 121.08s\n",
      "Time: 2018-04-07 18:39:52.248000000~2018-04-07 18:56:09.290000000, Loss: 0.7189, Nodes_count: 11300, Cost Time: 125.25s\n",
      "Time: 2018-04-07 18:56:09.290000000~2018-04-07 19:21:04.391000000, Loss: 0.5791, Nodes_count: 11330, Cost Time: 125.78s\n",
      "Time: 2018-04-07 19:21:04.391000000~2018-04-07 19:45:02.042000000, Loss: 0.3079, Nodes_count: 11352, Cost Time: 125.89s\n",
      "Time: 2018-04-07 19:45:02.042000000~2018-04-07 20:00:59.751000000, Loss: 0.8637, Nodes_count: 11389, Cost Time: 126.11s\n",
      "Time: 2018-04-07 20:00:59.751000000~2018-04-07 20:21:50.560000000, Loss: 1.7018, Nodes_count: 11503, Cost Time: 126.66s\n",
      "Time: 2018-04-07 20:21:50.560000000~2018-04-07 20:37:54.893000000, Loss: 1.7994, Nodes_count: 12079, Cost Time: 128.86s\n",
      "Time: 2018-04-07 20:37:54.893000000~2018-04-07 20:52:58.485000000, Loss: 0.9474, Nodes_count: 12858, Cost Time: 137.29s\n",
      "Time: 2018-04-07 20:52:58.485000000~2018-04-07 21:08:28.188000000, Loss: 0.8028, Nodes_count: 12892, Cost Time: 146.80s\n",
      "Time: 2018-04-07 21:08:28.188000000~2018-04-07 21:24:36.756000000, Loss: 0.8194, Nodes_count: 13376, Cost Time: 152.82s\n",
      "Time: 2018-04-07 21:24:36.756000000~2018-04-07 21:42:26.573000000, Loss: 0.2698, Nodes_count: 13379, Cost Time: 153.27s\n",
      "Time: 2018-04-07 21:42:26.573000000~2018-04-07 22:05:45.793000000, Loss: 0.6654, Nodes_count: 13426, Cost Time: 153.65s\n",
      "Time: 2018-04-07 22:05:45.793000000~2018-04-07 22:30:10.178000000, Loss: 0.7585, Nodes_count: 13468, Cost Time: 153.88s\n",
      "Time: 2018-04-07 22:30:10.178000000~2018-04-07 22:47:23.900000000, Loss: 0.8444, Nodes_count: 13481, Cost Time: 154.08s\n",
      "Time: 2018-04-07 22:47:23.900000000~2018-04-07 23:22:01.561000000, Loss: 0.5109, Nodes_count: 13519, Cost Time: 154.41s\n",
      "Time: 2018-04-07 23:22:01.561000000~2018-04-07 23:44:24.554000000, Loss: 0.2641, Nodes_count: 13543, Cost Time: 154.52s\n",
      "Time: 2018-04-07 23:44:24.554000000~2018-04-07 23:59:58.135000000, Loss: 0.2333, Nodes_count: 13557, Cost Time: 154.59s\n"
     ]
    }
   ],
   "source": [
    "ans_4_7=test_day_new(graph_4_7,\"graph_4_7\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after merge: TemporalData(dst=[2554245], msg=[2554245, 40], src=[2554245], t=[2554245])\n",
      "Time: 2018-04-10 00:00:00.041000000~2018-04-10 01:14:00.502000000, Loss: 2.4347, Nodes_count: 81, Cost Time: 0.07s\n",
      "Time: 2018-04-10 01:14:00.502000000~2018-04-10 02:24:00.583000000, Loss: 0.8558, Nodes_count: 156, Cost Time: 0.16s\n",
      "Time: 2018-04-10 02:24:00.583000000~2018-04-10 03:30:37.185000000, Loss: 1.1191, Nodes_count: 226, Cost Time: 0.24s\n",
      "Time: 2018-04-10 03:30:37.185000000~2018-04-10 04:51:00.454000000, Loss: 0.9735, Nodes_count: 300, Cost Time: 0.34s\n",
      "Time: 2018-04-10 04:51:00.454000000~2018-04-10 06:00:07.009000000, Loss: 0.8161, Nodes_count: 367, Cost Time: 0.43s\n",
      "Time: 2018-04-10 06:00:07.009000000~2018-04-10 07:04:52.138000000, Loss: 0.9079, Nodes_count: 442, Cost Time: 0.51s\n",
      "Time: 2018-04-10 07:04:52.138000000~2018-04-10 07:19:59.157000000, Loss: 0.8149, Nodes_count: 1100, Cost Time: 3.13s\n",
      "Time: 2018-04-10 07:19:59.157000000~2018-04-10 07:52:43.143000000, Loss: 0.5599, Nodes_count: 1151, Cost Time: 4.26s\n",
      "Time: 2018-04-10 07:52:43.143000000~2018-04-10 08:11:35.864000000, Loss: 0.9760, Nodes_count: 2173, Cost Time: 8.25s\n",
      "Time: 2018-04-10 08:11:35.864000000~2018-04-10 08:33:00.367000000, Loss: 0.7288, Nodes_count: 2462, Cost Time: 16.42s\n",
      "Time: 2018-04-10 08:33:00.367000000~2018-04-10 08:48:00.599000000, Loss: 0.6957, Nodes_count: 2505, Cost Time: 25.44s\n",
      "Time: 2018-04-10 08:48:00.599000000~2018-04-10 09:03:00.861000000, Loss: 0.5746, Nodes_count: 2560, Cost Time: 27.54s\n",
      "Time: 2018-04-10 09:03:00.861000000~2018-04-10 09:19:34.766000000, Loss: 0.8833, Nodes_count: 2925, Cost Time: 29.97s\n",
      "Time: 2018-04-10 09:19:34.766000000~2018-04-10 09:45:04.552000000, Loss: 0.8460, Nodes_count: 3671, Cost Time: 36.50s\n",
      "Time: 2018-04-10 09:45:04.552000000~2018-04-10 10:06:29.544000000, Loss: 1.0862, Nodes_count: 4097, Cost Time: 39.69s\n",
      "Time: 2018-04-10 10:06:29.544000000~2018-04-10 10:21:32.315000000, Loss: 0.4392, Nodes_count: 4210, Cost Time: 43.46s\n",
      "Time: 2018-04-10 10:21:32.315000000~2018-04-10 10:36:35.907000000, Loss: 0.6298, Nodes_count: 4385, Cost Time: 49.88s\n",
      "Time: 2018-04-10 10:36:35.907000000~2018-04-10 10:53:14.920000000, Loss: 0.6862, Nodes_count: 4417, Cost Time: 51.17s\n",
      "Time: 2018-04-10 10:53:14.920000000~2018-04-10 11:12:35.572000000, Loss: 0.5722, Nodes_count: 4512, Cost Time: 54.81s\n",
      "Time: 2018-04-10 11:12:35.572000000~2018-04-10 11:28:38.819000000, Loss: 0.5063, Nodes_count: 4737, Cost Time: 55.98s\n",
      "Time: 2018-04-10 11:28:38.819000000~2018-04-10 11:43:48.286000000, Loss: 0.6609, Nodes_count: 4848, Cost Time: 58.96s\n",
      "Time: 2018-04-10 11:43:48.286000000~2018-04-10 12:05:28.835000000, Loss: 0.8169, Nodes_count: 4939, Cost Time: 60.46s\n",
      "Time: 2018-04-10 12:05:28.835000000~2018-04-10 12:21:06.782000000, Loss: 0.7715, Nodes_count: 5614, Cost Time: 64.40s\n",
      "Time: 2018-04-10 12:21:06.782000000~2018-04-10 12:45:20.100000000, Loss: 0.8857, Nodes_count: 5769, Cost Time: 65.85s\n",
      "Time: 2018-04-10 12:45:20.100000000~2018-04-10 13:02:56.955000000, Loss: 0.9859, Nodes_count: 6670, Cost Time: 71.75s\n",
      "Time: 2018-04-10 13:02:56.955000000~2018-04-10 13:18:18.179000000, Loss: 0.0428, Nodes_count: 6749, Cost Time: 105.78s\n",
      "Time: 2018-04-10 13:18:18.179000000~2018-04-10 13:34:14.316000000, Loss: 0.5419, Nodes_count: 6832, Cost Time: 113.67s\n",
      "Time: 2018-04-10 13:34:14.316000000~2018-04-10 13:49:28.078000000, Loss: 0.7853, Nodes_count: 7389, Cost Time: 122.32s\n",
      "Time: 2018-04-10 13:49:28.078000000~2018-04-10 14:05:08.084000000, Loss: 1.0441, Nodes_count: 8060, Cost Time: 127.34s\n",
      "Time: 2018-04-10 14:05:08.084000000~2018-04-10 14:21:43.187000000, Loss: 0.6031, Nodes_count: 8142, Cost Time: 129.53s\n",
      "Time: 2018-04-10 14:21:43.187000000~2018-04-10 14:36:57.738000000, Loss: 0.6307, Nodes_count: 8256, Cost Time: 136.57s\n",
      "Time: 2018-04-10 14:36:57.738000000~2018-04-10 14:54:56.906000000, Loss: 0.9309, Nodes_count: 8459, Cost Time: 138.84s\n",
      "Time: 2018-04-10 14:54:56.906000000~2018-04-10 15:28:27.687000000, Loss: 0.7034, Nodes_count: 8595, Cost Time: 146.45s\n",
      "Time: 2018-04-10 15:28:27.687000000~2018-04-10 15:51:57.466000000, Loss: 1.1739, Nodes_count: 9434, Cost Time: 150.22s\n",
      "Time: 2018-04-10 15:51:57.466000000~2018-04-10 16:10:37.208000000, Loss: 0.9148, Nodes_count: 9567, Cost Time: 151.30s\n",
      "Time: 2018-04-10 16:10:37.208000000~2018-04-10 16:25:54.701000000, Loss: 0.7704, Nodes_count: 9787, Cost Time: 154.24s\n",
      "Time: 2018-04-10 16:25:54.701000000~2018-04-10 16:41:04.776000000, Loss: 0.7465, Nodes_count: 10062, Cost Time: 158.08s\n",
      "Time: 2018-04-10 16:41:04.776000000~2018-04-10 16:56:25.853000000, Loss: 0.8948, Nodes_count: 10664, Cost Time: 164.78s\n",
      "Time: 2018-04-10 16:56:25.853000000~2018-04-10 17:11:31.301000000, Loss: 0.6029, Nodes_count: 10929, Cost Time: 169.76s\n",
      "Time: 2018-04-10 17:11:31.301000000~2018-04-10 17:27:31.099000000, Loss: 0.7788, Nodes_count: 11314, Cost Time: 175.57s\n",
      "Time: 2018-04-10 17:27:31.099000000~2018-04-10 17:47:55.016000000, Loss: 0.5591, Nodes_count: 11373, Cost Time: 176.76s\n",
      "Time: 2018-04-10 17:47:55.016000000~2018-04-10 18:02:59.013000000, Loss: 0.9720, Nodes_count: 11877, Cost Time: 180.45s\n",
      "Time: 2018-04-10 18:02:59.013000000~2018-04-10 18:17:59.904000000, Loss: 0.5941, Nodes_count: 12019, Cost Time: 189.01s\n",
      "Time: 2018-04-10 18:17:59.904000000~2018-04-10 18:38:58.139000000, Loss: 0.7664, Nodes_count: 12080, Cost Time: 192.43s\n",
      "Time: 2018-04-10 18:38:58.139000000~2018-04-10 19:00:34.975000000, Loss: 1.1110, Nodes_count: 12515, Cost Time: 194.81s\n",
      "Time: 2018-04-10 19:00:34.975000000~2018-04-10 19:20:44.840000000, Loss: 1.1219, Nodes_count: 12685, Cost Time: 195.70s\n",
      "Time: 2018-04-10 19:20:44.840000000~2018-04-10 19:40:54.882000000, Loss: 1.4510, Nodes_count: 12812, Cost Time: 196.25s\n",
      "Time: 2018-04-10 19:40:54.882000000~2018-04-10 20:01:07.769000000, Loss: 1.3897, Nodes_count: 12983, Cost Time: 196.92s\n",
      "Time: 2018-04-10 20:01:07.769000000~2018-04-10 20:26:09.203000000, Loss: 0.9917, Nodes_count: 13943, Cost Time: 204.45s\n",
      "Time: 2018-04-10 20:26:09.203000000~2018-04-10 20:52:04.007000000, Loss: 1.0191, Nodes_count: 13975, Cost Time: 205.06s\n",
      "Time: 2018-04-10 20:52:04.007000000~2018-04-10 21:12:55.647000000, Loss: 0.9758, Nodes_count: 14407, Cost Time: 207.55s\n",
      "Time: 2018-04-10 21:12:55.647000000~2018-04-10 21:38:13.511000000, Loss: 0.8567, Nodes_count: 14450, Cost Time: 208.16s\n",
      "Time: 2018-04-10 21:38:13.511000000~2018-04-10 21:57:28.644000000, Loss: 0.4178, Nodes_count: 14478, Cost Time: 208.37s\n",
      "Time: 2018-04-10 21:57:28.644000000~2018-04-10 22:17:10.628000000, Loss: 0.2387, Nodes_count: 14503, Cost Time: 209.28s\n",
      "Time: 2018-04-10 22:17:10.628000000~2018-04-10 22:44:13.982000000, Loss: 0.6093, Nodes_count: 14520, Cost Time: 209.46s\n",
      "Time: 2018-04-10 22:44:13.982000000~2018-04-10 23:09:32.277000000, Loss: 0.3464, Nodes_count: 14550, Cost Time: 209.73s\n",
      "Time: 2018-04-10 23:09:32.277000000~2018-04-10 23:59:32.865000000, Loss: 1.4009, Nodes_count: 14606, Cost Time: 209.80s\n"
     ]
    }
   ],
   "source": [
    "ans_4_10=test_day_new(graph_4_10,\"graph_4_10\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after merge: TemporalData(dst=[1976440], msg=[1976440, 40], src=[1976440], t=[1976440])\n",
      "Time: 2018-04-11 00:00:00.063000000~2018-04-11 02:00:00.161000000, Loss: 2.4756, Nodes_count: 127, Cost Time: 0.07s\n",
      "Time: 2018-04-11 02:00:00.161000000~2018-04-11 03:45:00.251000000, Loss: 1.3634, Nodes_count: 230, Cost Time: 0.15s\n",
      "Time: 2018-04-11 03:45:00.251000000~2018-04-11 05:30:00.544000000, Loss: 1.4002, Nodes_count: 333, Cost Time: 0.27s\n",
      "Time: 2018-04-11 05:30:00.544000000~2018-04-11 07:00:52.974000000, Loss: 1.3375, Nodes_count: 439, Cost Time: 0.36s\n",
      "Time: 2018-04-11 07:00:52.974000000~2018-04-11 07:25:16.408000000, Loss: 0.7098, Nodes_count: 482, Cost Time: 0.67s\n",
      "Time: 2018-04-11 07:25:16.408000000~2018-04-11 07:46:44.962000000, Loss: 0.9261, Nodes_count: 514, Cost Time: 0.87s\n",
      "Time: 2018-04-11 07:46:44.962000000~2018-04-11 08:05:53.555000000, Loss: 0.8334, Nodes_count: 548, Cost Time: 0.97s\n",
      "Time: 2018-04-11 08:05:53.555000000~2018-04-11 08:21:10.246000000, Loss: 0.8749, Nodes_count: 744, Cost Time: 2.74s\n",
      "Time: 2018-04-11 08:21:10.246000000~2018-04-11 08:43:29.852000000, Loss: 0.8975, Nodes_count: 786, Cost Time: 3.06s\n",
      "Time: 2018-04-11 08:43:29.852000000~2018-04-11 08:58:36.759000000, Loss: 0.6198, Nodes_count: 891, Cost Time: 3.86s\n",
      "Time: 2018-04-11 08:58:36.759000000~2018-04-11 09:15:23.460000000, Loss: 0.8597, Nodes_count: 2594, Cost Time: 10.50s\n",
      "Time: 2018-04-11 09:15:23.460000000~2018-04-11 09:30:40.459000000, Loss: 0.5881, Nodes_count: 2661, Cost Time: 11.49s\n",
      "Time: 2018-04-11 09:30:40.459000000~2018-04-11 09:45:41.202000000, Loss: 0.7521, Nodes_count: 3334, Cost Time: 15.61s\n",
      "Time: 2018-04-11 09:45:41.202000000~2018-04-11 10:02:14.946000000, Loss: 0.5431, Nodes_count: 3515, Cost Time: 18.36s\n",
      "Time: 2018-04-11 10:02:14.946000000~2018-04-11 10:21:29.924000000, Loss: 0.6307, Nodes_count: 3752, Cost Time: 22.07s\n",
      "Time: 2018-04-11 10:21:29.924000000~2018-04-11 10:36:41.077000000, Loss: 0.5847, Nodes_count: 4436, Cost Time: 35.29s\n",
      "Time: 2018-04-11 10:36:41.077000000~2018-04-11 10:53:51.436000000, Loss: 0.3489, Nodes_count: 4597, Cost Time: 40.68s\n",
      "Time: 2018-04-11 10:53:51.436000000~2018-04-11 11:11:03.735000000, Loss: 0.8737, Nodes_count: 4831, Cost Time: 44.23s\n",
      "Time: 2018-04-11 11:11:03.735000000~2018-04-11 11:27:06.742000000, Loss: 1.0703, Nodes_count: 5587, Cost Time: 49.06s\n",
      "Time: 2018-04-11 11:27:06.742000000~2018-04-11 11:42:13.848000000, Loss: 0.4689, Nodes_count: 5777, Cost Time: 51.87s\n",
      "Time: 2018-04-11 11:42:13.848000000~2018-04-11 11:59:50.272000000, Loss: 0.6817, Nodes_count: 6163, Cost Time: 57.13s\n",
      "Time: 2018-04-11 11:59:50.272000000~2018-04-11 12:15:10.640000000, Loss: 0.8979, Nodes_count: 6619, Cost Time: 59.95s\n",
      "Time: 2018-04-11 12:15:10.640000000~2018-04-11 12:30:15.769000000, Loss: 1.2094, Nodes_count: 6680, Cost Time: 60.35s\n",
      "Time: 2018-04-11 12:30:15.769000000~2018-04-11 12:45:30.888000000, Loss: 1.3887, Nodes_count: 6832, Cost Time: 64.96s\n",
      "Time: 2018-04-11 12:45:30.888000000~2018-04-11 13:01:09.797000000, Loss: 1.6300, Nodes_count: 7500, Cost Time: 69.29s\n",
      "Time: 2018-04-11 13:01:09.797000000~2018-04-11 13:16:29.838000000, Loss: 1.6562, Nodes_count: 8579, Cost Time: 73.14s\n",
      "Time: 2018-04-11 13:16:29.838000000~2018-04-11 13:31:30.828000000, Loss: 1.8565, Nodes_count: 9592, Cost Time: 78.37s\n",
      "Time: 2018-04-11 13:31:30.828000000~2018-04-11 13:46:38.658000000, Loss: 1.8955, Nodes_count: 10337, Cost Time: 83.76s\n",
      "Time: 2018-04-11 13:46:38.658000000~2018-04-11 14:02:21.103000000, Loss: 1.8515, Nodes_count: 11153, Cost Time: 90.07s\n",
      "Time: 2018-04-11 14:02:21.103000000~2018-04-11 14:18:19.001000000, Loss: 1.9034, Nodes_count: 12116, Cost Time: 95.74s\n",
      "Time: 2018-04-11 14:18:19.001000000~2018-04-11 14:33:38.600000000, Loss: 1.8688, Nodes_count: 13192, Cost Time: 101.52s\n",
      "Time: 2018-04-11 14:33:38.600000000~2018-04-11 14:49:05.326000000, Loss: 1.0229, Nodes_count: 13440, Cost Time: 105.25s\n",
      "Time: 2018-04-11 14:49:05.326000000~2018-04-11 15:04:48.749000000, Loss: 1.1918, Nodes_count: 13870, Cost Time: 109.46s\n",
      "Time: 2018-04-11 15:04:48.749000000~2018-04-11 15:23:04.703000000, Loss: 1.6271, Nodes_count: 14026, Cost Time: 111.38s\n",
      "Time: 2018-04-11 15:23:04.703000000~2018-04-11 15:39:01.167000000, Loss: 2.0996, Nodes_count: 14174, Cost Time: 113.76s\n",
      "Time: 2018-04-11 15:39:01.167000000~2018-04-11 15:54:01.828000000, Loss: 1.6483, Nodes_count: 14762, Cost Time: 118.46s\n",
      "Time: 2018-04-11 15:54:01.828000000~2018-04-11 16:09:02.909000000, Loss: 1.7815, Nodes_count: 15025, Cost Time: 121.32s\n",
      "Time: 2018-04-11 16:09:02.909000000~2018-04-11 16:24:25.756000000, Loss: 1.7404, Nodes_count: 15746, Cost Time: 125.27s\n",
      "Time: 2018-04-11 16:24:25.756000000~2018-04-11 16:44:13.639000000, Loss: 1.5962, Nodes_count: 16251, Cost Time: 130.02s\n",
      "Time: 2018-04-11 16:44:13.639000000~2018-04-11 16:59:14.685000000, Loss: 1.3011, Nodes_count: 16933, Cost Time: 133.71s\n",
      "Time: 2018-04-11 16:59:14.685000000~2018-04-11 17:14:26.172000000, Loss: 1.3064, Nodes_count: 17438, Cost Time: 137.97s\n",
      "Time: 2018-04-11 17:14:26.172000000~2018-04-11 17:29:29.020000000, Loss: 1.6853, Nodes_count: 17966, Cost Time: 143.12s\n",
      "Time: 2018-04-11 17:29:29.020000000~2018-04-11 17:44:48.533000000, Loss: 1.6825, Nodes_count: 18466, Cost Time: 148.46s\n",
      "Time: 2018-04-11 17:44:48.533000000~2018-04-11 17:59:53.338000000, Loss: 1.7343, Nodes_count: 19017, Cost Time: 153.95s\n",
      "Time: 2018-04-11 17:59:53.338000000~2018-04-11 18:16:39.616000000, Loss: 0.3952, Nodes_count: 19268, Cost Time: 161.29s\n",
      "Time: 2018-04-11 18:16:39.616000000~2018-04-11 18:32:06.970000000, Loss: 1.7261, Nodes_count: 19822, Cost Time: 167.01s\n",
      "Time: 2018-04-11 18:32:06.970000000~2018-04-11 18:47:24.055000000, Loss: 1.6128, Nodes_count: 20424, Cost Time: 171.11s\n",
      "Time: 2018-04-11 18:47:24.055000000~2018-04-11 19:02:37.294000000, Loss: 1.5630, Nodes_count: 20887, Cost Time: 177.41s\n",
      "Time: 2018-04-11 19:02:37.294000000~2018-04-11 19:18:03.851000000, Loss: 0.9243, Nodes_count: 21288, Cost Time: 182.99s\n",
      "Time: 2018-04-11 19:18:03.851000000~2018-04-11 19:33:17.804000000, Loss: 1.6562, Nodes_count: 21834, Cost Time: 188.32s\n",
      "Time: 2018-04-11 19:33:17.804000000~2018-04-11 19:48:43.675000000, Loss: 1.5706, Nodes_count: 22117, Cost Time: 193.49s\n",
      "Time: 2018-04-11 19:48:43.675000000~2018-04-11 20:04:35.343000000, Loss: 0.3504, Nodes_count: 22349, Cost Time: 200.64s\n",
      "Time: 2018-04-11 20:04:35.343000000~2018-04-11 20:27:04.183000000, Loss: 1.6261, Nodes_count: 22587, Cost Time: 203.33s\n",
      "Time: 2018-04-11 20:27:04.183000000~2018-04-11 20:46:59.749000000, Loss: 1.5740, Nodes_count: 23551, Cost Time: 207.78s\n",
      "Time: 2018-04-11 20:46:59.749000000~2018-04-11 21:04:21.264000000, Loss: 1.6506, Nodes_count: 23854, Cost Time: 210.16s\n",
      "Time: 2018-04-11 21:04:21.264000000~2018-04-11 21:29:14.176000000, Loss: 1.6161, Nodes_count: 24180, Cost Time: 212.49s\n",
      "Time: 2018-04-11 21:29:14.176000000~2018-04-11 21:44:15.839000000, Loss: 0.7245, Nodes_count: 24644, Cost Time: 217.12s\n",
      "Time: 2018-04-11 21:44:15.839000000~2018-04-11 21:59:40.869000000, Loss: 1.5367, Nodes_count: 25119, Cost Time: 221.22s\n",
      "Time: 2018-04-11 21:59:40.869000000~2018-04-11 22:18:09.134000000, Loss: 0.4728, Nodes_count: 25398, Cost Time: 226.41s\n",
      "Time: 2018-04-11 22:18:09.134000000~2018-04-11 22:33:22.263000000, Loss: 1.6764, Nodes_count: 26030, Cost Time: 232.26s\n",
      "Time: 2018-04-11 22:33:22.263000000~2018-04-11 22:49:00.957000000, Loss: 1.4053, Nodes_count: 26346, Cost Time: 235.72s\n",
      "Time: 2018-04-11 22:49:00.957000000~2018-04-11 23:46:00.517000000, Loss: 1.4668, Nodes_count: 26396, Cost Time: 235.97s\n"
     ]
    }
   ],
   "source": [
    "ans_4_11=test_day_new(graph_4_11,\"graph_4_11\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize the node IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 293/293 [02:58<00:00,  1.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IDF weight calculate complete!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "node_set=set()\n",
    "\n",
    "file_list=[]\n",
    "\n",
    "file_path=\"graph_4_4/\"\n",
    "file_l=os.listdir(\"graph_4_4/\")\n",
    "for i in file_l:\n",
    "    file_list.append(file_path+i)\n",
    "\n",
    "file_path=\"graph_4_5/\"\n",
    "file_l=os.listdir(\"graph_4_5/\")\n",
    "for i in file_l:\n",
    "    file_list.append(file_path+i)\n",
    "\n",
    "file_path=\"graph_4_6/\"\n",
    "file_l=os.listdir(\"graph_4_6/\")\n",
    "for i in file_l:\n",
    "    file_list.append(file_path+i)\n",
    "\n",
    "\n",
    "file_path=\"graph_4_7/\"\n",
    "file_l=os.listdir(\"graph_4_7/\")\n",
    "for i in file_l:\n",
    "    file_list.append(file_path+i)\n",
    "\n",
    "node_IDF={}\n",
    "node_set = {}\n",
    "for f_path in tqdm(file_list):\n",
    "    f=open(f_path)\n",
    "    for line in f:\n",
    "        l=line.strip()\n",
    "        jdata=eval(l)\n",
    "        jdata=eval(l)\n",
    "        if jdata['loss']>0:\n",
    "            if 'netflow' not in str(jdata['srcmsg']):\n",
    "                if str(jdata['srcmsg']) not in node_set.keys():\n",
    "                    node_set[str(jdata['srcmsg'])] = set([f_path])\n",
    "                else:\n",
    "                    node_set[str(jdata['srcmsg'])].add(f_path)\n",
    "            if 'netflow' not in str(jdata['dstmsg']):\n",
    "                if str(jdata['dstmsg']) not in node_set.keys():\n",
    "                    node_set[str(jdata['dstmsg'])] = set([f_path])\n",
    "                else:\n",
    "                    node_set[str(jdata['dstmsg'])].add(f_path)\n",
    "for n in node_set:\n",
    "    include_count = len(node_set[n])   \n",
    "    IDF=math.log(len(file_list)/(include_count+1))\n",
    "    node_IDF[n] = IDF    \n",
    "\n",
    "\n",
    "torch.save(node_IDF,\"node_IDF\")\n",
    "print(\"IDF weight calculate complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_train_IDF(find_str,file_list):\n",
    "    include_count=0\n",
    "    for f_path in (file_list):\n",
    "        f=open(f_path)\n",
    "        if find_str in f.read():\n",
    "            include_count+=1             \n",
    "    IDF=math.log(len(file_list)/(include_count+1))\n",
    "    return IDF\n",
    "\n",
    "\n",
    "def cal_IDF(find_str,file_path,file_list):\n",
    "    file_list=os.listdir(file_path)\n",
    "    include_count=0\n",
    "    different_neighbor=set()\n",
    "    for f_path in (file_list):\n",
    "        f=open(file_path+f_path)\n",
    "        if find_str in f.read():\n",
    "            include_count+=1                \n",
    "                \n",
    "    IDF=math.log(len(file_list)/(include_count+1))\n",
    "    \n",
    "    return IDF,1\n",
    "\n",
    "def cal_redundant(find_str,edge_list):\n",
    "    \n",
    "    different_neighbor=set()\n",
    "    for e in edge_list:\n",
    "        if find_str in str(e):\n",
    "            different_neighbor.add(e[0])\n",
    "            different_neighbor.add(e[1])\n",
    "    return len(different_neighbor)-2\n",
    "\n",
    "def cal_anomaly_loss(loss_list,edge_list,file_path):\n",
    "    \n",
    "    if len(loss_list)!=len(edge_list):\n",
    "        print(\"error!\")\n",
    "        return 0\n",
    "    count=0\n",
    "    loss_sum=0\n",
    "    loss_std=std(loss_list)\n",
    "    loss_mean=mean(loss_list)\n",
    "    edge_set=set()\n",
    "    node_set=set()\n",
    "    node2redundant={}\n",
    "    \n",
    "    thr=loss_mean+1.5*loss_std\n",
    "\n",
    "    print(\"thr:\",thr)\n",
    "\n",
    "    for i in range(len(loss_list)):\n",
    "        if loss_list[i]>thr:\n",
    "            count+=1\n",
    "            src_node=edge_list[i][0]\n",
    "            dst_node=edge_list[i][1]\n",
    "            \n",
    "            loss_sum+=loss_list[i]\n",
    "    \n",
    "            node_set.add(src_node)\n",
    "            node_set.add(dst_node)\n",
    "            edge_set.add(edge_list[i][0]+edge_list[i][1])\n",
    "    return count, loss_sum/(count + 0.00001) ,node_set,edge_set\n",
    "#     return count, count/len(loss_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construct the relations between time windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def is_include_key_word(s):\n",
    "    keywords=[\n",
    "         'netflow',\n",
    "\n",
    "        'glx_alsa_675',\n",
    "        '/data/system/',\n",
    "         '/storage/emulated/',\n",
    "        '/data/data/com.android',\n",
    "        '/proc/',\n",
    "        'nz9885vc.default',\n",
    "      \n",
    "      ]\n",
    "    flag=False\n",
    "    for i in keywords:\n",
    "        if i in s:\n",
    "            flag=True\n",
    "    return flag\n",
    "\n",
    "\n",
    "\n",
    "def cal_set_rel(s1,s2,node_IDF, file_list):\n",
    "    new_s=s1 & s2\n",
    "    count=0\n",
    "    for i in new_s:\n",
    "#     jdata=json.loads(i)\n",
    "        if is_include_key_word(i) is False :\n",
    "            if i in node_IDF.keys():\n",
    "                IDF=node_IDF[i]\n",
    "            else:\n",
    "                IDF=math.log(len(file_list)/(1))\n",
    "            if IDF>6:\n",
    "                print(\"node:\",i,\" IDF:\",IDF)\n",
    "                count+=1\n",
    "    return count\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# def cal_set_rel(s1,s2,node_IDF, file_list, node_IDF_4_4_7, file_list_4_4_7):\n",
    "#     new_s=s1 & s2\n",
    "#     count=0\n",
    "#     for i in new_s:\n",
    "# #     jdata=json.loads(i)\n",
    "#         if 'netflow' not in i and 'glx_alsa_675' not in i and '/data/system/' not in i and '/storage/emulated/' not in i and  '/data/data/com.android' not in i and  '/proc/' not in i and 'nz9885vc.default' not in i :\n",
    "\n",
    "# #         'netflow' not in i\n",
    "# #         and 'usr' not in i and 'var' not in i\n",
    "#             if i in node_IDF.keys():\n",
    "#                 IDF=node_IDF[i]\n",
    "#             else:\n",
    "#                 IDF=math.log(len(file_list)/(1))\n",
    "                \n",
    "#             if i in node_IDF_4_4_7.keys():\n",
    "#                 IDF4=node_IDF_4_4_7[i]\n",
    "#             else:\n",
    "#                 IDF4=math.log(len(file_list_4_4_7)/(1))    \n",
    "            \n",
    "# #             print(IDF)\n",
    "#             if (IDF+IDF4)>9:\n",
    "#                 print(\"node:\",i,\" IDF:\",IDF+IDF4)\n",
    "#                 count+=1\n",
    "#     return count\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# label generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels={}\n",
    "    \n",
    "    \n",
    "filelist = os.listdir(\"graph_4_10\")\n",
    "for f in filelist:\n",
    "    labels[\"graph_4_10/\"+f]=0\n",
    "\n",
    "filelist = os.listdir(\"graph_4_11\")\n",
    "for f in filelist:\n",
    "    labels[\"graph_4_11/\"+f]=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "attack_list=[\n",
    "    'graph_4_11/2018-04-11 13:46:38.658000000~2018-04-11 14:02:21.103000000.txt',\n",
    "    'graph_4_11/2018-04-11 14:02:21.103000000~2018-04-11 14:18:19.001000000.txt',\n",
    "    'graph_4_11/2018-04-11 14:18:19.001000000~2018-04-11 14:33:38.600000000.txt',\n",
    "    'graph_4_11/2018-04-11 14:33:38.600000000~2018-04-11 14:49:05.326000000.txt',\n",
    "    'graph_4_11/2018-04-11 14:49:05.326000000~2018-04-11 15:04:48.749000000.txt',\n",
    "]\n",
    "for i in attack_list:\n",
    "    labels[i]=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_label={}\n",
    "\n",
    "filelist = os.listdir(\"graph_4_10/\")\n",
    "for f in filelist:\n",
    "    pred_label[\"graph_4_10/\"+f]=0\n",
    "    \n",
    "filelist = os.listdir(\"graph_4_11/\")\n",
    "for f in filelist:\n",
    "    pred_label[\"graph_4_11/\"+f]=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_list=[]\n",
    "\n",
    "file_path=\"graph_4_4/\"\n",
    "file_l=os.listdir(\"graph_4_4/\")\n",
    "for i in file_l:\n",
    "    file_list.append(file_path+i)\n",
    "\n",
    "file_path=\"graph_4_5/\"\n",
    "file_l=os.listdir(\"graph_4_5/\")\n",
    "for i in file_l:\n",
    "    file_list.append(file_path+i)\n",
    "\n",
    "file_path=\"graph_4_6/\"\n",
    "file_l=os.listdir(\"graph_4_6/\")\n",
    "for i in file_l:\n",
    "    file_list.append(file_path+i)\n",
    "\n",
    "\n",
    "file_path=\"graph_4_7/\"\n",
    "file_l=os.listdir(\"graph_4_7/\")\n",
    "for i in file_l:\n",
    "    file_list.append(file_path+i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index_count: 0\n",
      "thr: 2.9131093692414938\n",
      "2018-04-10 00:00:00.041000000~2018-04-10 01:14:00.502000000.txt    0.0  count: 0  percentage: 0.0  node count: 0  edge count: 0\n",
      "index_count: 1\n",
      "thr: 2.4505172825883186\n",
      "2018-04-10 01:14:00.502000000~2018-04-10 02:24:00.583000000.txt    2.836467621259631  count: 143  percentage: 0.1396484375  node count: 72  edge count: 70\n",
      "index_count: 2\n",
      "thr: 3.0353913896982863\n",
      "2018-04-10 02:24:00.583000000~2018-04-10 03:30:37.185000000.txt    4.160153197968703  count: 78  percentage: 0.076171875  node count: 8  edge count: 6\n",
      "index_count: 3\n",
      "thr: 2.7444641739268594\n",
      "2018-04-10 03:30:37.185000000~2018-04-10 04:51:00.454000000.txt    3.8535016423213033  count: 72  percentage: 0.0703125  node count: 12  edge count: 10\n",
      "index_count: 4\n",
      "thr: 2.2709182118825995\n",
      "2018-04-10 04:51:00.454000000~2018-04-10 06:00:07.009000000.txt    2.557459614218246  count: 153  percentage: 0.1494140625  node count: 68  edge count: 66\n",
      "index_count: 5\n",
      "thr: 2.4438397403567667\n",
      "2018-04-10 06:00:07.009000000~2018-04-10 07:04:52.138000000.txt    2.795820858660771  count: 124  percentage: 0.12109375  node count: 65  edge count: 61\n",
      "index_count: 6\n",
      "thr: 2.4288654798264737\n",
      "2018-04-10 07:04:52.138000000~2018-04-10 07:19:59.157000000.txt    3.4940670799296702  count: 3611  percentage: 0.10371668198529412  node count: 412  edge count: 411\n",
      "index_count: 7\n",
      "thr: 1.326934450057888\n",
      "2018-04-10 07:19:59.157000000~2018-04-10 07:52:43.143000000.txt    1.958473862589858  count: 887  percentage: 0.06663161057692307  node count: 79  edge count: 74\n",
      "index_count: 8\n",
      "thr: 2.476086504617435\n",
      "2018-04-10 07:52:43.143000000~2018-04-10 08:11:35.864000000.txt    3.6366517285218642  count: 4901  percentage: 0.092041015625  node count: 694  edge count: 692\n",
      "index_count: 9\n",
      "thr: 1.463175792744252\n",
      "2018-04-10 08:11:35.864000000~2018-04-10 08:33:00.367000000.txt    3.183053287133338  count: 2834  percentage: 0.023454051906779662  node count: 303  edge count: 312\n",
      "index_count: 10\n",
      "thr: 1.1602760916303243\n",
      "2018-04-10 08:33:00.367000000~2018-04-10 08:48:00.599000000.txt    3.2100632236533118  count: 591  percentage: 0.00450897216796875  node count: 89  edge count: 93\n",
      "index_count: 11\n",
      "thr: 1.6244311782515548\n",
      "2018-04-10 08:48:00.599000000~2018-04-10 09:03:00.861000000.txt    3.3220125700037153  count: 563  percentage: 0.030544704861111112  node count: 103  edge count: 101\n",
      "index_count: 12\n",
      "thr: 2.7375220868961785\n",
      "2018-04-10 09:03:00.861000000~2018-04-10 09:19:34.766000000.txt    3.939753348629066  count: 2940  percentage: 0.10633680555555555  node count: 236  edge count: 238\n",
      "index_count: 13\n",
      "thr: 2.2858196982225945\n",
      "2018-04-10 09:19:34.766000000~2018-04-10 09:45:04.552000000.txt    3.5247247416448526  count: 6634  percentage: 0.08524362664473684  node count: 420  edge count: 426\n",
      "index_count: 14\n",
      "thr: 2.9042117181818803\n",
      "2018-04-10 09:45:04.552000000~2018-04-10 10:06:29.544000000.txt    3.812857096406302  count: 3487  percentage: 0.12161690848214286  node count: 352  edge count: 349\n",
      "index_count: 15\n",
      "thr: 1.6895301686471367\n",
      "2018-04-10 10:06:29.544000000~2018-04-10 10:21:32.315000000.txt    3.6684199003967093  count: 1858  percentage: 0.03944463315217391  node count: 187  edge count: 195\n",
      "index_count: 16\n",
      "thr: 1.5476384955296725\n",
      "2018-04-10 10:21:32.315000000~2018-04-10 10:36:35.907000000.txt    3.2894466065417682  count: 2148  percentage: 0.024972098214285716  node count: 246  edge count: 281\n",
      "index_count: 17\n",
      "thr: 1.5893183549196888\n",
      "2018-04-10 10:36:35.907000000~2018-04-10 10:53:14.920000000.txt    3.0496106197209047  count: 366  percentage: 0.03249289772727273  node count: 84  edge count: 87\n",
      "index_count: 18\n",
      "thr: 1.6072551471831227\n",
      "2018-04-10 10:53:14.920000000~2018-04-10 11:12:35.572000000.txt    3.612093162678681  count: 1339  percentage: 0.02615234375  node count: 190  edge count: 198\n",
      "index_count: 19\n",
      "thr: 2.2717833766159474\n",
      "2018-04-10 11:12:35.572000000~2018-04-10 11:28:38.819000000.txt    3.716219151627054  count: 935  percentage: 0.09130859375  node count: 288  edge count: 299\n",
      "index_count: 20\n",
      "thr: 1.5554750430335118\n",
      "2018-04-10 11:28:38.819000000~2018-04-10 11:43:48.286000000.txt    2.957137996587118  count: 1410  percentage: 0.034423828125  node count: 174  edge count: 183\n",
      "index_count: 21\n",
      "thr: 2.242435832335874\n",
      "2018-04-10 11:43:48.286000000~2018-04-10 12:05:28.835000000.txt    3.6579977901469465  count: 990  percentage: 0.06905691964285714  node count: 297  edge count: 335\n",
      "index_count: 22\n",
      "thr: 2.4994734300548336\n",
      "2018-04-10 12:05:28.835000000~2018-04-10 12:21:06.782000000.txt    3.942077415992421  count: 4119  percentage: 0.08744480298913043  node count: 688  edge count: 744\n",
      "index_count: 23\n",
      "thr: 2.4173283975481668\n",
      "2018-04-10 12:21:06.782000000~2018-04-10 12:45:20.100000000.txt    3.323772348224558  count: 1437  percentage: 0.10023716517857142  node count: 116  edge count: 113\n",
      "index_count: 24\n",
      "thr: 2.5053787572639266\n",
      "2018-04-10 12:45:20.100000000~2018-04-10 13:02:56.955000000.txt    3.744941650443401  count: 6459  percentage: 0.09556995738636363  node count: 620  edge count: 618\n",
      "index_count: 25\n",
      "thr: 0.3239301706302193\n",
      "2018-04-10 13:02:56.955000000~2018-04-10 13:18:18.179000000.txt    0.849935660663719  count: 14149  percentage: 0.027801575075452716  node count: 169  edge count: 189\n",
      "index_count: 26\n",
      "thr: 1.2152857940572463\n",
      "2018-04-10 13:18:18.179000000~2018-04-10 13:34:14.316000000.txt    2.473564958522111  count: 1180  percentage: 0.016004774305555556  node count: 156  edge count: 183\n",
      "index_count: 27\n",
      "thr: 1.8146322476912564\n",
      "2018-04-10 13:34:14.316000000~2018-04-10 13:49:28.078000000.txt    3.3003552180869486  count: 5495  percentage: 0.04923129300458716  node count: 601  edge count: 632\n",
      "index_count: 28\n",
      "thr: 2.785681533084512\n",
      "2018-04-10 13:49:28.078000000~2018-04-10 14:05:08.084000000.txt    3.851959415610312  count: 4936  percentage: 0.11210029069767442  node count: 323  edge count: 354\n",
      "index_count: 29\n",
      "thr: 2.2304650363030643\n",
      "2018-04-10 14:05:08.084000000~2018-04-10 14:21:43.187000000.txt    3.3093526708988468  count: 2333  percentage: 0.10356001420454546  node count: 124  edge count: 126\n",
      "index_count: 30\n",
      "thr: 1.453521078454027\n",
      "2018-04-10 14:21:43.187000000~2018-04-10 14:36:57.738000000.txt    3.4351443080514557  count: 1725  percentage: 0.01736670425257732  node count: 222  edge count: 260\n",
      "index_count: 31\n",
      "thr: 2.661695121381733\n",
      "2018-04-10 14:36:57.738000000~2018-04-10 14:54:56.906000000.txt    3.779133943242233  count: 1696  percentage: 0.103515625  node count: 324  edge count: 333\n",
      "index_count: 32\n",
      "thr: 1.3694758609034663\n",
      "2018-04-10 14:54:56.906000000~2018-04-10 15:28:27.687000000.txt    2.7195296776176683  count: 2532  percentage: 0.02310893691588785  node count: 377  edge count: 414\n",
      "index_count: 33\n",
      "thr: 2.983942083055098\n",
      "2018-04-10 15:28:27.687000000~2018-04-10 15:51:57.466000000.txt    3.7600399453492312  count: 4109  percentage: 0.13375651041666667  node count: 465  edge count: 461\n",
      "index_count: 34\n",
      "thr: 2.590281080358933\n",
      "2018-04-10 15:51:57.466000000~2018-04-10 16:10:37.208000000.txt    3.5514826616081083  count: 962  percentage: 0.117431640625  node count: 140  edge count: 141\n",
      "index_count: 35\n",
      "thr: 2.501599125068999\n",
      "2018-04-10 16:10:37.208000000~2018-04-10 16:25:54.701000000.txt    3.6483752806132945  count: 2971  percentage: 0.10004714439655173  node count: 200  edge count: 216\n",
      "index_count: 36\n",
      "thr: 2.0969397712422135\n",
      "2018-04-10 16:25:54.701000000~2018-04-10 16:41:04.776000000.txt    3.0917766192165987  count: 3756  percentage: 0.08530159883720931  node count: 330  edge count: 338\n",
      "index_count: 37\n",
      "thr: 2.347592672732049\n",
      "2018-04-10 16:41:04.776000000~2018-04-10 16:56:25.853000000.txt    3.596696598041636  count: 6230  percentage: 0.0881736865942029  node count: 588  edge count: 603\n",
      "index_count: 38\n",
      "thr: 2.0332053398116576\n",
      "2018-04-10 16:56:25.853000000~2018-04-10 17:11:31.301000000.txt    3.779074208695391  count: 3172  percentage: 0.056321022727272727  node count: 407  edge count: 437\n",
      "index_count: 39\n",
      "thr: 2.1690619850699733\n",
      "2018-04-10 17:11:31.301000000~2018-04-10 17:27:31.099000000.txt    3.6549188477870573  count: 4545  percentage: 0.07158833165322581  node count: 649  edge count: 662\n",
      "index_count: 40\n",
      "thr: 2.132519441417899\n",
      "2018-04-10 17:27:31.099000000~2018-04-10 17:47:55.016000000.txt    3.555733491081897  count: 625  percentage: 0.0762939453125  node count: 133  edge count: 164\n",
      "index_count: 41\n",
      "thr: 2.67068144057454\n",
      "2018-04-10 17:47:55.016000000~2018-04-10 18:02:59.013000000.txt    3.8453790155239003  count: 3327  percentage: 0.09845525568181818  node count: 377  edge count: 380\n",
      "index_count: 42\n",
      "thr: 1.3227664891713018\n",
      "2018-04-10 18:02:59.013000000~2018-04-10 18:17:59.904000000.txt    2.739479282706438  count: 2146  percentage: 0.018546045353982302  node count: 235  edge count: 280\n",
      "index_count: 43\n",
      "thr: 1.5687483253578451\n",
      "2018-04-10 18:17:59.904000000~2018-04-10 18:38:58.139000000.txt    2.9727381826037167  count: 1270  percentage: 0.03445095486111111  node count: 293  edge count: 322\n",
      "index_count: 44\n",
      "thr: 2.807134922732744\n",
      "2018-04-10 18:38:58.139000000~2018-04-10 19:00:34.975000000.txt    3.869334477838221  count: 2389  percentage: 0.11109561011904762  node count: 375  edge count: 375\n",
      "index_count: 45\n",
      "thr: 2.8724111896380107\n",
      "2018-04-10 19:00:34.975000000~2018-04-10 19:20:44.840000000.txt    3.706040132213943  count: 861  percentage: 0.1201171875  node count: 156  edge count: 155\n",
      "index_count: 46\n",
      "thr: 3.3499595482487035\n",
      "2018-04-10 19:20:44.840000000~2018-04-10 19:40:54.882000000.txt    3.9917873266120205  count: 451  percentage: 0.110107421875  node count: 107  edge count: 104\n",
      "index_count: 47\n",
      "thr: 3.174310882171968\n",
      "2018-04-10 19:40:54.882000000~2018-04-10 20:01:07.769000000.txt    3.678286467188493  count: 728  percentage: 0.1421875  node count: 120  edge count: 119\n",
      "index_count: 48\n",
      "thr: 2.4912544173252646\n",
      "2018-04-10 20:01:07.769000000~2018-04-10 20:26:09.203000000.txt    3.6361002484615224  count: 8309  percentage: 0.10271212420886076  node count: 905  edge count: 905\n",
      "index_count: 49\n",
      "thr: 2.856270642108494\n",
      "2018-04-10 20:26:09.203000000~2018-04-10 20:52:04.007000000.txt    3.8894438922175887  count: 105  percentage: 0.1025390625  node count: 30  edge count: 27\n",
      "index_count: 50\n",
      "thr: 2.613891760659783\n",
      "2018-04-10 20:52:04.007000000~2018-04-10 21:12:55.647000000.txt    3.8392795817077467  count: 2632  percentage: 0.09885817307692307  node count: 337  edge count: 336\n",
      "index_count: 51\n",
      "thr: 2.5954580949796062\n",
      "2018-04-10 21:12:55.647000000~2018-04-10 21:38:13.511000000.txt    3.8299892568335214  count: 390  percentage: 0.09521484375  node count: 60  edge count: 78\n",
      "index_count: 52\n",
      "thr: 1.7191375894536347\n",
      "2018-04-10 21:38:13.511000000~2018-04-10 21:57:28.644000000.txt    3.3536558973945576  count: 114  percentage: 0.0556640625  node count: 67  edge count: 65\n",
      "index_count: 53\n",
      "thr: 1.787119634676093\n",
      "2018-04-10 21:57:28.644000000~2018-04-10 22:17:10.628000000.txt    4.8344435811753295  count: 432  percentage: 0.03835227272727273  node count: 66  edge count: 63\n",
      "index_count: 54\n",
      "thr: 2.0342096682984616\n",
      "2018-04-10 22:17:10.628000000~2018-04-10 22:44:13.982000000.txt    2.983297576079696  count: 95  percentage: 0.0927734375  node count: 37  edge count: 33\n",
      "index_count: 55\n",
      "thr: 1.7585046272622513\n",
      "2018-04-10 22:44:13.982000000~2018-04-10 23:09:32.277000000.txt    3.2163231873272  count: 222  percentage: 0.072265625  node count: 63  edge count: 59\n",
      "index_count: 56\n",
      "thr: 2.8706359353059616\n",
      "2018-04-10 23:09:32.277000000~2018-04-10 23:59:32.865000000.txt    3.2106769552574166  count: 17  percentage: 0.043701799485861184  node count: 11  edge count: 9\n"
     ]
    }
   ],
   "source": [
    "# node_IDF=torch.load(\"node_IDF_4_10\")\n",
    "# node_IDF_4_7=torch.load(\"node_IDF_4_4-7\")\n",
    "node_IDF_4_4_7=torch.load(\"node_IDF\")\n",
    "y_data_4_10=[]\n",
    "df_list_4_10=[]\n",
    "# node_set_list=[]\n",
    "history_list=[]\n",
    "tw_que=[]\n",
    "his_tw={}\n",
    "current_tw={}\n",
    "loss_list_4_10=[]\n",
    "\n",
    "\n",
    "file_l=os.listdir(\"graph_4_10\")\n",
    "index_count=0\n",
    "for f_path in sorted(file_l):\n",
    "    f=open(\"graph_4_10/\"+f_path)\n",
    "    edge_loss_list=[]\n",
    "    edge_list=[]\n",
    "    print('index_count:',index_count)\n",
    "#     print(f_path)\n",
    "    for line in f:\n",
    "        l=line.strip()\n",
    "        jdata=eval(l)\n",
    "        edge_loss_list.append(jdata['loss'])\n",
    "        edge_list.append([str(jdata['srcmsg']),str(jdata['dstmsg'])])\n",
    "    df_list_4_10.append(pd.DataFrame(edge_loss_list))\n",
    "    count,loss_avg,node_set,edge_set=cal_anomaly_loss(edge_loss_list,edge_list,\"graph_4_10/\")\n",
    "    current_tw['name']=f_path\n",
    "    current_tw['loss']=loss_avg\n",
    "    current_tw['index']=index_count\n",
    "    current_tw['nodeset']=node_set\n",
    "\n",
    "    added_que_flag=False\n",
    "    for hq in history_list:\n",
    "        for his_tw in hq:\n",
    "            if cal_set_rel(current_tw['nodeset'],his_tw['nodeset'],node_IDF_4_4_7, file_list)!=0 and current_tw['name']!=his_tw['name']:\n",
    "#                 print(\"history queue:\",his_tw['name'])\n",
    "                hq.append(copy.deepcopy(current_tw))\n",
    "                added_que_flag=True\n",
    "                break\n",
    "            if added_que_flag:\n",
    "                break\n",
    "    if added_que_flag is False:\n",
    "        temp_hq=[copy.deepcopy(current_tw)]\n",
    "        history_list.append(temp_hq)\n",
    "    index_count+=1\n",
    "    loss_list_4_10.append(loss_avg)\n",
    "    print( f_path,\"  \",loss_avg,\" count:\",count,\" percentage:\",count/len(edge_list),\" node count:\",len(node_set),\" edge count:\",len(edge_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_list=[]\n",
    "for hl in history_list:\n",
    "    loss_count=0\n",
    "    for hq in hl:\n",
    "        if loss_count==0:\n",
    "            loss_count=(loss_count+1)*(hq['loss']+1)\n",
    "        else:\n",
    "            loss_count=(loss_count)*(hq['loss']+1)\n",
    "#     name_list=[]\n",
    "    if loss_count>9:\n",
    "        name_list=[]\n",
    "        for i in hl:\n",
    "            name_list.append(i['name']) \n",
    "        print(name_list)\n",
    "        for i in name_list:\n",
    "            pred_label[i]=1\n",
    "        print(loss_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4-11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index_count: 0\n",
      "thr: 2.985977518173932\n",
      "graph_4_11/2018-04-11 00:00:00.063000000~2018-04-11 02:00:00.161000000.txt    0.0  count: 0  percentage: 0.0  node count: 0  edge count: 0\n",
      "index_count: 1\n",
      "thr: 2.8283381276176907\n",
      "graph_4_11/2018-04-11 02:00:00.161000000~2018-04-11 03:45:00.251000000.txt    3.3625901651538337  count: 42  percentage: 0.041015625  node count: 8  edge count: 6\n",
      "index_count: 2\n",
      "thr: 2.8787989609271603\n",
      "graph_4_11/2018-04-11 03:45:00.251000000~2018-04-11 05:30:00.544000000.txt    3.134660661828412  count: 107  percentage: 0.1044921875  node count: 11  edge count: 9\n",
      "index_count: 3\n",
      "thr: 2.799850149739225\n",
      "graph_4_11/2018-04-11 05:30:00.544000000~2018-04-11 07:00:52.974000000.txt    3.242761895426892  count: 74  percentage: 0.072265625  node count: 10  edge count: 7\n",
      "index_count: 4\n",
      "thr: 1.7366477389575752\n",
      "graph_4_11/2018-04-11 07:00:52.974000000~2018-04-11 07:25:16.408000000.txt    2.356366250211236  count: 340  percentage: 0.0830078125  node count: 56  edge count: 53\n",
      "index_count: 5\n",
      "thr: 2.287270930172202\n",
      "graph_4_11/2018-04-11 07:25:16.408000000~2018-04-11 07:46:44.962000000.txt    2.6681349665823664  count: 240  percentage: 0.1171875  node count: 43  edge count: 40\n",
      "index_count: 6\n",
      "thr: 2.2990374800289866\n",
      "graph_4_11/2018-04-11 07:46:44.962000000~2018-04-11 08:05:53.555000000.txt    3.0136422230919853  count: 123  percentage: 0.1201171875  node count: 45  edge count: 42\n",
      "index_count: 7\n",
      "thr: 2.4066174604336243\n",
      "graph_4_11/2018-04-11 08:05:53.555000000~2018-04-11 08:21:10.246000000.txt    3.203833054429025  count: 2408  percentage: 0.11197916666666667  node count: 114  edge count: 111\n",
      "index_count: 8\n",
      "thr: 2.478748599170528\n",
      "graph_4_11/2018-04-11 08:21:10.246000000~2018-04-11 08:43:29.852000000.txt    2.9930833311200624  count: 179  percentage: 0.08740234375  node count: 59  edge count: 56\n",
      "index_count: 9\n",
      "thr: 2.391228029359451\n",
      "graph_4_11/2018-04-11 08:43:29.852000000~2018-04-11 08:58:36.759000000.txt    4.170694092985853  count: 734  percentage: 0.0716796875  node count: 88  edge count: 86\n",
      "index_count: 10\n",
      "thr: 2.4944307097696026\n",
      "graph_4_11/2018-04-11 08:58:36.759000000~2018-04-11 09:15:23.460000000.txt    3.5748895496949533  count: 8497  percentage: 0.10372314453125  node count: 890  edge count: 921\n",
      "index_count: 11\n",
      "thr: 2.1354129441054424\n",
      "graph_4_11/2018-04-11 09:15:23.460000000~2018-04-11 09:30:40.459000000.txt    3.14298969942505  count: 612  percentage: 0.099609375  node count: 111  edge count: 123\n",
      "index_count: 12\n",
      "thr: 2.2990958529513836\n",
      "graph_4_11/2018-04-11 09:30:40.459000000~2018-04-11 09:45:41.202000000.txt    3.672203620844096  count: 4039  percentage: 0.08049665178571429  node count: 665  edge count: 722\n",
      "index_count: 13\n",
      "thr: 2.001357649675396\n",
      "graph_4_11/2018-04-11 09:45:41.202000000~2018-04-11 10:02:14.946000000.txt    3.6390658359359627  count: 1838  percentage: 0.059830729166666666  node count: 249  edge count: 297\n",
      "index_count: 14\n",
      "thr: 2.289163068900735\n",
      "graph_4_11/2018-04-11 10:02:14.946000000~2018-04-11 10:21:29.924000000.txt    3.7007244220999067  count: 3431  percentage: 0.0797758556547619  node count: 299  edge count: 347\n",
      "index_count: 15\n",
      "thr: 1.7394060764028023\n",
      "graph_4_11/2018-04-11 10:21:29.924000000~2018-04-11 10:36:41.077000000.txt    3.001465915348983  count: 9445  percentage: 0.0555640530873494  node count: 778  edge count: 871\n",
      "index_count: 16\n",
      "thr: 1.62399221721415\n",
      "graph_4_11/2018-04-11 10:36:41.077000000~2018-04-11 10:53:51.436000000.txt    3.0561442480317154  count: 3817  percentage: 0.07033092570754718  node count: 428  edge count: 500\n",
      "index_count: 17\n",
      "thr: 2.3418322223240313\n",
      "graph_4_11/2018-04-11 10:53:51.436000000~2018-04-11 11:11:03.735000000.txt    3.3571332380352605  count: 3086  percentage: 0.09417724609375  node count: 318  edge count: 362\n",
      "index_count: 18\n",
      "thr: 2.7732809676211643\n",
      "graph_4_11/2018-04-11 11:11:03.735000000~2018-04-11 11:27:06.742000000.txt    3.8535694290996774  count: 4743  percentage: 0.10526899857954546  node count: 452  edge count: 484\n",
      "index_count: 19\n",
      "thr: 1.9277959975671186\n",
      "graph_4_11/2018-04-11 11:27:06.742000000~2018-04-11 11:42:13.848000000.txt    3.3322039542486053  count: 2147  percentage: 0.07765480324074074  node count: 438  edge count: 475\n",
      "index_count: 20\n",
      "thr: 2.3449261041637426\n",
      "graph_4_11/2018-04-11 11:42:13.848000000~2018-04-11 11:59:50.272000000.txt    3.5274353320086997  count: 5058  percentage: 0.09685202205882353  node count: 556  edge count: 740\n",
      "index_count: 21\n",
      "thr: 2.4783852563288233\n",
      "graph_4_11/2018-04-11 11:59:50.272000000~2018-04-11 12:15:10.640000000.txt    3.401108569175676  count: 3123  percentage: 0.11730018028846154  node count: 441  edge count: 454\n",
      "index_count: 22\n",
      "thr: 3.082122061489468\n",
      "graph_4_11/2018-04-11 12:15:10.640000000~2018-04-11 12:30:15.769000000.txt    3.808389344696971  count: 230  percentage: 0.1123046875  node count: 49  edge count: 53\n",
      "index_count: 23\n",
      "thr: 3.894340296926036\n",
      "graph_4_11/2018-04-11 12:30:15.769000000~2018-04-11 12:45:30.888000000.txt    4.76164211178864  count: 8320  percentage: 0.15625  node count: 88  edge count: 112\n",
      "index_count: 24\n",
      "thr: 3.6395508671516996\n",
      "graph_4_11/2018-04-11 12:45:30.888000000~2018-04-11 13:01:09.797000000.txt    4.057743348463694  count: 4769  percentage: 0.11088634672619048  node count: 195  edge count: 218\n",
      "index_count: 25\n",
      "thr: 4.119008619944882\n",
      "graph_4_11/2018-04-11 13:01:09.797000000~2018-04-11 13:16:29.838000000.txt    5.377089229992724  count: 2521  percentage: 0.09118200231481481  node count: 480  edge count: 500\n",
      "index_count: 26\n",
      "thr: 4.606655989589732\n",
      "graph_4_11/2018-04-11 13:16:29.838000000~2018-04-11 13:31:30.828000000.txt    5.840051953769052  count: 3746  percentage: 0.10759420955882353  node count: 576  edge count: 579\n",
      "index_count: 27\n",
      "thr: 4.580333953520143\n",
      "graph_4_11/2018-04-11 13:31:30.828000000~2018-04-11 13:46:38.658000000.txt    5.620454100320597  count: 3513  percentage: 0.09801897321428571  node count: 407  edge count: 420\n",
      "index_count: 28\n",
      "thr: 4.307406135381195\n",
      "graph_4_11/2018-04-11 13:46:38.658000000~2018-04-11 14:02:21.103000000.txt    5.2896977643799685  count: 4940  percentage: 0.08933738425925926  node count: 500  edge count: 518\n",
      "index_count: 29\n",
      "thr: 4.45258247829387\n",
      "graph_4_11/2018-04-11 14:02:21.103000000~2018-04-11 14:18:19.001000000.txt    5.676230428406007  count: 2830  percentage: 0.069091796875  node count: 440  edge count: 447\n",
      "index_count: 30\n",
      "thr: 4.449790981634569\n",
      "graph_4_11/2018-04-11 14:18:19.001000000~2018-04-11 14:33:38.600000000.txt    5.7061647309058054  count: 3814  percentage: 0.09084413109756098  node count: 549  edge count: 562\n",
      "index_count: 31\n",
      "thr: 2.584479430504095\n",
      "graph_4_11/2018-04-11 14:33:38.600000000~2018-04-11 14:49:05.326000000.txt    3.8041010653829423  count: 3256  percentage: 0.08832465277777778  node count: 447  edge count: 488\n",
      "index_count: 32\n",
      "thr: 3.5264872124778734\n",
      "graph_4_11/2018-04-11 14:49:05.326000000~2018-04-11 15:04:48.749000000.txt    4.951425077794432  count: 3510  percentage: 0.09793526785714286  node count: 420  edge count: 443\n",
      "index_count: 33\n",
      "thr: 4.355243894787037\n",
      "graph_4_11/2018-04-11 15:04:48.749000000~2018-04-11 15:23:04.703000000.txt    5.819574529735627  count: 1079  percentage: 0.09579190340909091  node count: 326  edge count: 327\n",
      "index_count: 34\n",
      "thr: 4.70622799998077\n",
      "graph_4_11/2018-04-11 15:23:04.703000000~2018-04-11 15:39:01.167000000.txt    6.513971307505646  count: 1249  percentage: 0.05808221726190476  node count: 273  edge count: 271\n",
      "index_count: 35\n",
      "thr: 4.309715574952529\n",
      "graph_4_11/2018-04-11 15:39:01.167000000~2018-04-11 15:54:01.828000000.txt    5.517568069384271  count: 2844  percentage: 0.09919084821428571  node count: 384  edge count: 396\n",
      "index_count: 36\n",
      "thr: 4.505955237461103\n",
      "graph_4_11/2018-04-11 15:54:01.828000000~2018-04-11 16:09:02.909000000.txt    5.642285394919566  count: 1658  percentage: 0.08995225694444445  node count: 375  edge count: 379\n",
      "index_count: 37\n",
      "thr: 4.401156760746295\n",
      "graph_4_11/2018-04-11 16:09:02.909000000~2018-04-11 16:24:25.756000000.txt    5.7927854996552695  count: 2661  percentage: 0.09280831473214286  node count: 380  edge count: 387\n",
      "index_count: 38\n",
      "thr: 4.205884153971461\n",
      "graph_4_11/2018-04-11 16:24:25.756000000~2018-04-11 16:44:13.639000000.txt    5.509889950144843  count: 2811  percentage: 0.09803989955357142  node count: 368  edge count: 396\n",
      "index_count: 39\n",
      "thr: 3.429781301284487\n",
      "graph_4_11/2018-04-11 16:44:13.639000000~2018-04-11 16:59:14.685000000.txt    4.5578171092099815  count: 2616  percentage: 0.09123883928571429  node count: 459  edge count: 486\n",
      "index_count: 40\n",
      "thr: 3.520202079867907\n",
      "graph_4_11/2018-04-11 16:59:14.685000000~2018-04-11 17:14:26.172000000.txt    4.806975761275608  count: 2964  percentage: 0.09337197580645161  node count: 494  edge count: 518\n",
      "index_count: 41\n",
      "thr: 4.355490926984618\n",
      "graph_4_11/2018-04-11 17:14:26.172000000~2018-04-11 17:29:29.020000000.txt    5.595572886525393  count: 3121  percentage: 0.10159505208333333  node count: 421  edge count: 431\n",
      "index_count: 42\n",
      "thr: 4.330327759113471\n",
      "graph_4_11/2018-04-11 17:29:29.020000000~2018-04-11 17:44:48.533000000.txt    5.544160456645181  count: 3099  percentage: 0.10087890625  node count: 414  edge count: 426\n",
      "index_count: 43\n",
      "thr: 4.4446084768553\n",
      "graph_4_11/2018-04-11 17:44:48.533000000~2018-04-11 17:59:53.338000000.txt    5.768465763204117  count: 3108  percentage: 0.09790826612903226  node count: 381  edge count: 395\n",
      "index_count: 44\n",
      "thr: 1.7817003912591538\n",
      "graph_4_11/2018-04-11 17:59:53.338000000~2018-04-11 18:16:39.616000000.txt    3.2353659748333468  count: 5472  percentage: 0.07320205479452055  node count: 623  edge count: 796\n",
      "index_count: 45\n",
      "thr: 4.493423342594225\n",
      "graph_4_11/2018-04-11 18:16:39.616000000~2018-04-11 18:32:06.970000000.txt    5.87356209029428  count: 3265  percentage: 0.099639892578125  node count: 367  edge count: 372\n",
      "index_count: 46\n",
      "thr: 4.341614024905121\n",
      "graph_4_11/2018-04-11 18:32:06.970000000~2018-04-11 18:47:24.055000000.txt    5.774734299614414  count: 2746  percentage: 0.09932002314814815  node count: 388  edge count: 395\n",
      "index_count: 47\n",
      "thr: 3.99020328736726\n",
      "graph_4_11/2018-04-11 18:47:24.055000000~2018-04-11 19:02:37.294000000.txt    5.085611217805598  count: 3287  percentage: 0.09171316964285714  node count: 325  edge count: 354\n",
      "index_count: 48\n",
      "thr: 3.1008326113701505\n",
      "graph_4_11/2018-04-11 19:02:37.294000000~2018-04-11 19:18:03.851000000.txt    4.421531703182335  count: 4206  percentage: 0.09779575892857142  node count: 457  edge count: 503\n",
      "index_count: 49\n",
      "thr: 4.3383315300139325\n",
      "graph_4_11/2018-04-11 19:18:03.851000000~2018-04-11 19:33:17.804000000.txt    5.677119268164647  count: 3347  percentage: 0.102142333984375  node count: 379  edge count: 389\n",
      "index_count: 50\n",
      "thr: 4.1954200507742625\n",
      "graph_4_11/2018-04-11 19:33:17.804000000~2018-04-11 19:48:43.675000000.txt    5.523878478258537  count: 2971  percentage: 0.09671223958333333  node count: 392  edge count: 406\n",
      "index_count: 51\n",
      "thr: 1.6347949765370275\n",
      "graph_4_11/2018-04-11 19:48:43.675000000~2018-04-11 20:04:35.343000000.txt    3.0804932728002448  count: 5189  percentage: 0.06847814611486487  node count: 627  edge count: 811\n",
      "index_count: 52\n",
      "thr: 4.42202492813695\n",
      "graph_4_11/2018-04-11 20:04:35.343000000~2018-04-11 20:27:04.183000000.txt    5.725893282607113  count: 1567  percentage: 0.10930524553571429  node count: 414  edge count: 422\n",
      "index_count: 53\n",
      "thr: 3.9815843399829625\n",
      "graph_4_11/2018-04-11 20:27:04.183000000~2018-04-11 20:46:59.749000000.txt    5.198256951288804  count: 2967  percentage: 0.0934664818548387  node count: 583  edge count: 604\n",
      "index_count: 54\n",
      "thr: 4.098079758934977\n",
      "graph_4_11/2018-04-11 20:46:59.749000000~2018-04-11 21:04:21.264000000.txt    5.063056900159351  count: 1569  percentage: 0.08512369791666667  node count: 308  edge count: 316\n",
      "index_count: 55\n",
      "thr: 4.077783836446888\n",
      "graph_4_11/2018-04-11 21:04:21.264000000~2018-04-11 21:29:14.176000000.txt    5.29315760340736  count: 1510  percentage: 0.0921630859375  node count: 361  edge count: 374\n",
      "index_count: 56\n",
      "thr: 2.4415236544883103\n",
      "graph_4_11/2018-04-11 21:29:14.176000000~2018-04-11 21:44:15.839000000.txt    3.710221388252267  count: 4234  percentage: 0.0984468005952381  node count: 585  edge count: 694\n",
      "index_count: 57\n",
      "thr: 4.153764259231819\n",
      "graph_4_11/2018-04-11 21:44:15.839000000~2018-04-11 21:59:40.869000000.txt    5.477024908812526  count: 2681  percentage: 0.10069861778846154  node count: 497  edge count: 510\n",
      "index_count: 58\n",
      "thr: 2.079046897621584\n",
      "graph_4_11/2018-04-11 21:59:40.869000000~2018-04-11 22:18:09.134000000.txt    3.4997781275393423  count: 4354  percentage: 0.0850390625  node count: 618  edge count: 684\n",
      "index_count: 59\n",
      "thr: 4.340888574476097\n",
      "graph_4_11/2018-04-11 22:18:09.134000000~2018-04-11 22:33:22.263000000.txt    5.674318482738462  count: 3555  percentage: 0.09919084821428571  node count: 332  edge count: 340\n",
      "index_count: 60\n",
      "thr: 3.4967908977510307\n",
      "graph_4_11/2018-04-11 22:33:22.263000000~2018-04-11 22:49:00.957000000.txt    4.3243882953231445  count: 2019  percentage: 0.098583984375  node count: 284  edge count: 334\n",
      "index_count: 61\n",
      "thr: 3.5206848233530152\n",
      "graph_4_11/2018-04-11 22:49:00.957000000~2018-04-11 23:46:00.517000000.txt    4.270625690907469  count: 105  percentage: 0.1025390625  node count: 35  edge count: 33\n"
     ]
    }
   ],
   "source": [
    "# node_IDF=torch.load(\"node_IDF_4_11\")\n",
    "# node_IDF_4_4_7=torch.load(\"node_IDF_4_4-7\")\n",
    "node_IDF_4_4_7=torch.load(\"node_IDF\")\n",
    "y_data_4_11=[]\n",
    "df_list_4_11=[]\n",
    "# node_set_list=[]\n",
    "history_list=[]\n",
    "tw_que=[]\n",
    "his_tw={}\n",
    "current_tw={}\n",
    "\n",
    "loss_list_4_11=[]\n",
    "\n",
    "file_path_list=[]\n",
    "\n",
    "\n",
    "file_path=\"graph_4_11/\"\n",
    "file_l=os.listdir(\"graph_4_11/\")\n",
    "for i in file_l:\n",
    "    file_path_list.append(file_path+i)\n",
    "\n",
    "index_count=0\n",
    "for f_path in sorted(file_path_list):\n",
    "    f=open(f_path)\n",
    "    edge_loss_list=[]\n",
    "    edge_list=[]\n",
    "    print('index_count:',index_count)\n",
    "    \n",
    "    for line in f:\n",
    "        l=line.strip()\n",
    "        jdata=eval(l)\n",
    "        edge_loss_list.append(jdata['loss'])\n",
    "        edge_list.append([str(jdata['srcmsg']),str(jdata['dstmsg'])])\n",
    "    df_list_4_11.append(pd.DataFrame(edge_loss_list))\n",
    "    count,loss_avg,node_set,edge_set=cal_anomaly_loss(edge_loss_list,edge_list,\"graph_4_11/\")\n",
    "\n",
    "    current_tw['name']=f_path\n",
    "    current_tw['loss']=loss_avg\n",
    "    current_tw['index']=index_count\n",
    "    current_tw['nodeset']=node_set\n",
    "\n",
    "    added_que_flag=False\n",
    "    for hq in history_list:\n",
    "        for his_tw in hq:\n",
    "            if cal_set_rel(current_tw['nodeset'],his_tw['nodeset'],node_IDF_4_4_7, file_list)!=0 and current_tw['name']!=his_tw['name']:\n",
    "#                 print(\"history queue:\",his_tw['name'])\n",
    "                hq.append(copy.deepcopy(current_tw))\n",
    "                added_que_flag=True\n",
    "                break\n",
    "            if added_que_flag:\n",
    "                break\n",
    "    if added_que_flag is False:\n",
    "        temp_hq=[copy.deepcopy(current_tw)]\n",
    "        history_list.append(temp_hq)\n",
    "    index_count+=1\n",
    "    loss_list_4_11.append(loss_avg)\n",
    "    print( f_path,\"  \",loss_avg,\" count:\",count,\" percentage:\",count/len(edge_list),\" node count:\",len(node_set),\" edge count:\",len(edge_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_list=[]\n",
    "for hl in history_list:\n",
    "    loss_count=0\n",
    "    for hq in hl:\n",
    "        if loss_count==0:\n",
    "            loss_count=(loss_count+1)*(hq['loss']+1)\n",
    "        else:\n",
    "            loss_count=(loss_count)*(hq['loss']+1)\n",
    "#     name_list=[]\n",
    "    if loss_count>9:\n",
    "        name_list=[]\n",
    "        for i in hl:\n",
    "            name_list.append(i['name']) \n",
    "        print(name_list)\n",
    "        for i in name_list:\n",
    "            pred_label[i]=1\n",
    "        print(loss_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import average_precision_score, roc_auc_score\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import torch\n",
    "from sklearn import preprocessing\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def plot_thr():\n",
    "    np.seterr(invalid='ignore')\n",
    "    step=0.01\n",
    "    thr_list=torch.arange(-5,5,step)\n",
    "    \n",
    "    \n",
    "\n",
    "    precision_list=[]\n",
    "    recall_list=[]\n",
    "    fscore_list=[]\n",
    "    accuracy_list=[]\n",
    "    auc_val_list=[]\n",
    "    for thr in thr_list:\n",
    "        threshold=thr\n",
    "        y_prediction=[]\n",
    "        for i in y_test_scores:\n",
    "            if i >threshold:\n",
    "                y_prediction.append(1)\n",
    "            else:\n",
    "                y_prediction.append(0)\n",
    "        precision,recall,fscore,accuracy,auc_val=classifier_evaluation(y_test, y_prediction)   \n",
    "        precision_list.append(float(precision))\n",
    "        recall_list.append(float(recall))\n",
    "        fscore_list.append(float(fscore))\n",
    "        accuracy_list.append(float(accuracy))\n",
    "        auc_val_list.append(float(auc_val))\n",
    "\n",
    "    max_fscore=max(fscore_list)\n",
    "    max_fscore_index=fscore_list.index(max_fscore)\n",
    "    print(max_fscore_index)\n",
    "    print(\"max threshold:\",thr_list[max_fscore_index])\n",
    "    print('precision:',precision_list[max_fscore_index])\n",
    "    print('recall:',recall_list[max_fscore_index])\n",
    "    print('fscore:',fscore_list[max_fscore_index])\n",
    "    print('accuracy:',accuracy_list[max_fscore_index])    \n",
    "    print('auc:',auc_val_list[max_fscore_index])\n",
    "    \n",
    "        \n",
    "     # list tensor\n",
    "#     precision_list=torch.tensor(precision_list)   \n",
    "#     recall_list=torch.tensor(recall_list)   \n",
    "#     fscore_list=torch.tensor(fscore_list)   \n",
    "#     accuracy_list=torch.tensor(accuracy_list)   \n",
    "#     auc_val_list=torch.tensor(auc_val_list)   \n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "    # plt.scatter(attack_x, attack_y, s=20, c='r', label='Attack graph',marker='*')\n",
    "    # plt.scatter(bengin_x, bengin_y, s=20, c='g', label='Bengin graph',marker='1')\n",
    "    # plt.scatter(bengin_x, bengin_y, s=20, c='g', label='Bengin graph',marker='1')\n",
    "\n",
    "    plt.plot(thr_list,precision_list,color='red',label='precision',linewidth=2.0,linestyle='-')\n",
    "    plt.plot(thr_list,recall_list,color='orange',label='recall',linewidth=2.0,linestyle='solid')\n",
    "    plt.plot(thr_list,fscore_list,color='y',label='F-score',linewidth=2.0,linestyle='dashed')\n",
    "    plt.plot(thr_list,accuracy_list,color='g',label='accuracy',linewidth=2.0,linestyle='dashdot')\n",
    "    plt.plot(thr_list,auc_val_list,color='b',label='auc_val',linewidth=2.0,linestyle='dotted')\n",
    "    # '-', '--', '-.', ':', 'None', ' ', '', 'solid', 'dashed', 'dashdot', 'dotted'\n",
    "\n",
    "\n",
    "    # plt.scatter(turnovers, graph_loss, c=color)\n",
    "    plt.xlabel(\"Threshold\", fontdict={'size': 16})\n",
    "    plt.ylabel(\"Rate\", fontdict={'size': 16})\n",
    "    plt.title(\"Different evaluation Indicators by varying threshold value\", fontdict={'size': 12})\n",
    "    plt.legend(loc='best', fontsize=12, markerscale=0.5)\n",
    "    plt.show()\n",
    "\n",
    "def classifier_evaluation(y_test, y_test_pred):\n",
    "    # groundtruth, pred_value\n",
    "    tn, fp, fn, tp =confusion_matrix(y_test, y_test_pred).ravel()\n",
    "#     tn+=100\n",
    "#     print(clf_name,\" : \")\n",
    "    print('tn:',tn)\n",
    "    print('fp:',fp)\n",
    "    print('fn:',fn)\n",
    "    print('tp:',tp)\n",
    "    precision=tp/(tp+fp)\n",
    "    recall=tp/(tp+fn)\n",
    "    accuracy=(tp+tn)/(tp+tn+fp+fn)\n",
    "    fscore=2*(precision*recall)/(precision+recall)    \n",
    "    auc_val=roc_auc_score(y_test, y_test_pred)\n",
    "    print(\"precision:\",precision)\n",
    "    print(\"recall:\",recall)\n",
    "    print(\"fscore:\",fscore)\n",
    "    print(\"accuracy:\",accuracy)\n",
    "    print(\"auc_val:\",auc_val)\n",
    "    return precision,recall,fscore,accuracy,auc_val\n",
    "\n",
    "def minmax(data):\n",
    "    min_val=min(data)\n",
    "    max_val=max(data)\n",
    "    ans=[]\n",
    "    for i in data:\n",
    "        ans.append((i-min_val)/(max_val-min_val))\n",
    "    return ans\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=[]\n",
    "y_pred=[]\n",
    "for i in labels:\n",
    "    y.append(labels[i])\n",
    "    y_pred.append(pred_label[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tn: 114\n",
      "fp: 0\n",
      "fn: 5\n",
      "tp: 0\n",
      "precision: nan\n",
      "recall: 0.0\n",
      "fscore: nan\n",
      "accuracy: 0.957983193277311\n",
      "auc_val: 0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2897588/3255821976.py:88: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  precision=tp/(tp+fp)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(nan, 0.0, nan, 0.957983193277311, 0.5)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier_evaluation(y,y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Count attack edge numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keyword_hit(line):\n",
    "    attack_nodes=[\n",
    "            'shared_files',\n",
    "        'csb.tracee.27331.27355',\n",
    "        'netrecon',\n",
    "#         '/data/data/org.mozilla.fennec_firefox_dev/',\n",
    "     \n",
    "#             'firefox',\n",
    "        '153.178.46.202',\n",
    "       '111.82.111.27',\n",
    "        '166.199.230.185',\n",
    "        '140.57.183.17',\n",
    "      \n",
    "        \n",
    "        ]\n",
    "    flag=False\n",
    "    for i in attack_nodes:\n",
    "        if i in line:\n",
    "            flag=True\n",
    "            break\n",
    "    return flag\n",
    "\n",
    "\n",
    "\n",
    "files=[\n",
    "    \n",
    "        'graph_4_11/2018-04-11 13:46:38.658000000~2018-04-11 14:02:21.103000000.txt',\n",
    "    'graph_4_11/2018-04-11 14:02:21.103000000~2018-04-11 14:18:19.001000000.txt',\n",
    "    'graph_4_11/2018-04-11 14:18:19.001000000~2018-04-11 14:33:38.600000000.txt',\n",
    "    'graph_4_11/2018-04-11 14:33:38.600000000~2018-04-11 14:49:05.326000000.txt',\n",
    "    'graph_4_11/2018-04-11 14:49:05.326000000~2018-04-11 15:04:48.749000000.txt',\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:00<00:00, 27.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "647\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "attack_edge_count=0\n",
    "for fpath in tqdm(files):\n",
    "    f=open(fpath)\n",
    "    for line in f:\n",
    "        if keyword_hit(line):\n",
    "            attack_edge_count+=1\n",
    "print(attack_edge_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 1/5 [00:01<00:04,  1.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.8515160594261832\n",
      "1.6372600506366748\n",
      "thr: 4.307406135381195\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 2/5 [00:01<00:02,  1.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.9033884368034761\n",
      "1.6994626943269289\n",
      "thr: 4.45258247829387\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 3/5 [00:02<00:01,  1.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.8688417302356721\n",
      "1.7206328342659312\n",
      "thr: 4.449790981634569\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 4/5 [00:03<00:00,  1.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0228312342770651\n",
      "1.0410987974846866\n",
      "thr: 2.584479430504095\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:04<00:00,  1.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1917353118531602\n",
      "1.5565012670831422\n",
      "thr: 3.5264872124778734\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from graphviz import Digraph\n",
    "import networkx as nx\n",
    "import datetime\n",
    "import community.community_louvain as community_louvain\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "\n",
    "# Some common path abstraction for visualization\n",
    "replace_dic={\n",
    " '/data/data/org.mozilla.fennec_firefox_dev/cache/':'/data/data/org.mozilla.fennec_firefox_dev/cache/*',\n",
    "     '/data/data/org.mozilla.fennec_firefox_dev/files/':'/data/data/org.mozilla.fennec_firefox_dev/files/*',\n",
    "    '/system/fonts/':'/system/fonts/*',\n",
    "    '/data/data/com.android.email/cache/':'/data/data/com.android.email/cache/*',\n",
    "    '/data/data/com.android.email/files/':'/data/data/com.android.email/files/*',\n",
    "    'UNNAMED':'UNNAMED:*',\n",
    "    \n",
    "}\n",
    "\n",
    "\n",
    "def replace_path_name(path_name):\n",
    "    for i in replace_dic:\n",
    "        if i in path_name:\n",
    "            return replace_dic[i]\n",
    "    return path_name\n",
    "\n",
    "\n",
    "# Users should manually put the detected anomalous time windows here\n",
    "attack_list = [\n",
    "        'graph_4_11/2018-04-11 13:46:38.658000000~2018-04-11 14:02:21.103000000.txt',\n",
    "    'graph_4_11/2018-04-11 14:02:21.103000000~2018-04-11 14:18:19.001000000.txt',\n",
    "    'graph_4_11/2018-04-11 14:18:19.001000000~2018-04-11 14:33:38.600000000.txt',\n",
    "    'graph_4_11/2018-04-11 14:33:38.600000000~2018-04-11 14:49:05.326000000.txt',\n",
    "    'graph_4_11/2018-04-11 14:49:05.326000000~2018-04-11 15:04:48.749000000.txt',\n",
    "]\n",
    "\n",
    "original_edges_count = 0\n",
    "graphs = []\n",
    "gg = nx.DiGraph()\n",
    "count = 0\n",
    "for path in tqdm(attack_list):\n",
    "    if \".txt\" in path:\n",
    "        line_count = 0\n",
    "        node_set = set()\n",
    "        tempg = nx.DiGraph()\n",
    "        f = open(path, \"r\")\n",
    "        edge_list = []\n",
    "        for line in f:\n",
    "            count += 1\n",
    "            l = line.strip()\n",
    "            jdata = eval(l)\n",
    "            edge_list.append(jdata)\n",
    "\n",
    "        edge_list = sorted(edge_list, key=lambda x: x['loss'], reverse=True)\n",
    "        original_edges_count += len(edge_list)\n",
    "\n",
    "        loss_list = []\n",
    "        for i in edge_list:\n",
    "            loss_list.append(i['loss'])\n",
    "        loss_mean = mean(loss_list)\n",
    "        loss_std = std(loss_list)\n",
    "        print(loss_mean)\n",
    "        print(loss_std)\n",
    "        thr = loss_mean + 1.5 * loss_std\n",
    "        print(\"thr:\", thr)\n",
    "        for e in edge_list:\n",
    "            if e['loss'] > thr:\n",
    "                tempg.add_edge(str(hashgen(replace_path_name(e['srcmsg']))),\n",
    "                               str(hashgen(replace_path_name(e['dstmsg']))))\n",
    "                gg.add_edge(str(hashgen(replace_path_name(e['srcmsg']))), str(hashgen(replace_path_name(e['dstmsg']))),\n",
    "                            loss=e['loss'], srcmsg=e['srcmsg'], dstmsg=e['dstmsg'], edge_type=e['edge_type'],\n",
    "                            time=e['time'])\n",
    "\n",
    "\n",
    "partition = community_louvain.best_partition(gg.to_undirected())\n",
    "\n",
    "# Generate the candidate subgraphs based on community discovery results\n",
    "communities = {}\n",
    "max_partition = 0\n",
    "for i in partition:\n",
    "    if partition[i] > max_partition:\n",
    "        max_partition = partition[i]\n",
    "for i in range(max_partition + 1):\n",
    "    communities[i] = nx.DiGraph()\n",
    "for e in gg.edges:\n",
    "    communities[partition[e[0]]].add_edge(e[0], e[1])\n",
    "    communities[partition[e[1]]].add_edge(e[0], e[1])\n",
    "\n",
    "\n",
    "# Define the attack nodes. They are **only be used to plot the colors of attack nodes and edges**.\n",
    "# They won't change the detection results.\n",
    "# Didn't add too much nodes for coloring. Most of the results are compared with the ground truth documentations manually\n",
    "def attack_edge_flag(msg):\n",
    "    attack_nodes = [\n",
    "        '/data/data/org.mozilla.fennec_firefox_dev/',\n",
    "        '/data/data/org.mozilla.fennec_firefox_dev/shared_files',\n",
    "        '/data/local/tmp',\n",
    "        'csb.tracee.27331.27355',\n",
    "        '/data/data/org.mozilla.fennec_firefox_dev/csb.tracee.27331.27355',\n",
    "        '111.82.111.27',\n",
    "        '166.199.230.185',\n",
    "        'glx_alsa_675',\n",
    "    ]\n",
    "    flag = False\n",
    "    for i in attack_nodes:\n",
    "        if i in str(msg):\n",
    "            flag = True\n",
    "    return flag\n",
    "\n",
    "\n",
    "# Plot and render candidate subgraph\n",
    "os.system(f\"mkdir -p ./graph_visual/\")\n",
    "graph_index = 0\n",
    "for c in communities:\n",
    "    dot = Digraph(name=\"MyPicture\", comment=\"the test\", format=\"pdf\")\n",
    "    dot.graph_attr['rankdir'] = 'LR'\n",
    "\n",
    "    for e in communities[c].edges:\n",
    "        try:\n",
    "            temp_edge = gg.edges[e]\n",
    "            srcnode = e['srcnode']\n",
    "            dstnode = e['dstnode']\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        if True:\n",
    "            # source node\n",
    "            if \"'subject': '\" in temp_edge['srcmsg']:\n",
    "                src_shape = 'box'\n",
    "            elif \"'file': '\" in temp_edge['srcmsg']:\n",
    "                src_shape = 'oval'\n",
    "            elif \"'netflow': '\" in temp_edge['srcmsg']:\n",
    "                src_shape = 'diamond'\n",
    "            if attack_edge_flag(temp_edge['srcmsg']):\n",
    "                src_node_color = 'red'\n",
    "            else:\n",
    "                src_node_color = 'blue'\n",
    "            dot.node(name=str(hashgen(replace_path_name(temp_edge['srcmsg']))), label=str(\n",
    "                replace_path_name(temp_edge['srcmsg']) + str(\n",
    "                    partition[str(hashgen(replace_path_name(temp_edge['srcmsg'])))])), color=src_node_color,\n",
    "                     shape=src_shape)\n",
    "\n",
    "            # destination node\n",
    "            if \"'subject': '\" in temp_edge['dstmsg']:\n",
    "                dst_shape = 'box'\n",
    "            elif \"'file': '\" in temp_edge['dstmsg']:\n",
    "                dst_shape = 'oval'\n",
    "            elif \"'netflow': '\" in temp_edge['dstmsg']:\n",
    "                dst_shape = 'diamond'\n",
    "            if attack_edge_flag(temp_edge['dstmsg']):\n",
    "                dst_node_color = 'red'\n",
    "            else:\n",
    "                dst_node_color = 'blue'\n",
    "            dot.node(name=str(hashgen(replace_path_name(temp_edge['dstmsg']))), label=str(\n",
    "                replace_path_name(temp_edge['dstmsg']) + str(\n",
    "                    partition[str(hashgen(replace_path_name(temp_edge['dstmsg'])))])), color=dst_node_color,\n",
    "                     shape=dst_shape)\n",
    "\n",
    "            if attack_edge_flag(temp_edge['srcmsg']) and attack_edge_flag(temp_edge['dstmsg']):\n",
    "                edge_color = 'red'\n",
    "            else:\n",
    "                edge_color = 'blue'\n",
    "            dot.edge(str(hashgen(replace_path_name(temp_edge['srcmsg']))),\n",
    "                     str(hashgen(replace_path_name(temp_edge['dstmsg']))), label=temp_edge['edge_type'],\n",
    "                     color=edge_color)\n",
    "\n",
    "    dot.render(f'./graph_visual/subgraph_' + str(graph_index), view=False)\n",
    "    graph_index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#END"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
